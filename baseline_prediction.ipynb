{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Something about data.\n",
    "import os\n",
    "\n",
    "index_dir = os.path.join(\"index\")\n",
    "index_test_path = os.path.join(index_dir, \"gene_index.01_test.csv\")\n",
    "index_training_validation_path = os.path.join(index_dir, \"gene_index.01_train_validation.csv\")\n",
    "gene_dir = os.path.join(\"data\", \"gene_dir\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Lib\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import keras.utils\n",
    "from keras.layers import Embedding, Dense, Flatten, Dropout, SpatialDropout1D, TimeDistributed, LSTM, GRU, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from keras import Input, Model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "baseline_dir = os.path.join(\"data\", \"baseline\")\n",
    "model_dir = os.path.join(baseline_dir, \"model\")\n",
    "data_dir = os.path.join(baseline_dir, \"kmer\")\n",
    "num_classes = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64)\n",
      "{'AAA': 0, 'AAC': 1, 'AAG': 2, 'AAT': 3, 'ACA': 4, 'ACC': 5, 'ACG': 6, 'ACT': 7, 'AGA': 8, 'AGC': 9, 'AGG': 10, 'AGT': 11, 'ATA': 12, 'ATC': 13, 'ATG': 14, 'ATT': 15, 'CAA': 16, 'CAC': 17, 'CAG': 18, 'CAT': 19, 'CCA': 20, 'CCC': 21, 'CCG': 22, 'CCT': 23, 'CGA': 24, 'CGC': 25, 'CGG': 26, 'CGT': 27, 'CTA': 28, 'CTC': 29, 'CTG': 30, 'CTT': 31, 'GAA': 32, 'GAC': 33, 'GAG': 34, 'GAT': 35, 'GCA': 36, 'GCC': 37, 'GCG': 38, 'GCT': 39, 'GGA': 40, 'GGC': 41, 'GGG': 42, 'GGT': 43, 'GTA': 44, 'GTC': 45, 'GTG': 46, 'GTT': 47, 'TAA': 48, 'TAC': 49, 'TAG': 50, 'TAT': 51, 'TCA': 52, 'TCC': 53, 'TCG': 54, 'TCT': 55, 'TGA': 56, 'TGC': 57, 'TGG': 58, 'TGT': 59, 'TTA': 60, 'TTC': 61, 'TTG': 62, 'TTT': 63}\n",
      "data exists at data\\baseline\\kmer\\gene_index.01_train_validation_ss_all_pos_train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 71058/71058 [00:14<00:00, 4878.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data exists at data\\baseline\\kmer\\gene_index.01_train_validation_ss_all_pos_validation.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 17765/17765 [00:03<00:00, 4801.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data exists at data\\baseline\\kmer\\gene_index.01_test_ss_all_pos.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 6977/6977 [00:01<00:00, 5044.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data (71058, 512), (71058, 512, 8)\n",
      "Validation data (17765, 512), (17765, 512, 8)\n",
      "Test data (6977, 512), (6977, 512, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "characters = [\"A\", \"C\", \"G\", \"T\"]\n",
    "\n",
    "kmer_dict = {}\n",
    "count = 0\n",
    "embedding_vectors = []\n",
    "for a in characters:\n",
    "  for b in characters:\n",
    "    for c in characters:\n",
    "      v = [0 for i in range(64)]\n",
    "      kmer_dict[f\"{a}{b}{c}\"] = count\n",
    "      v[count] = 1\n",
    "      embedding_vectors.append(v)\n",
    "      count += 1\n",
    "\n",
    "embedding_vectors = np.array(embedding_vectors)\n",
    "print(embedding_vectors.shape)\n",
    "print(kmer_dict)\n",
    "embedding_matrix = embedding_vectors\n",
    "\n",
    "Label_Dictionary = {\n",
    "    '[CLS]': -100, #_create_one_hot_encoding(0, 10),\n",
    "    '[SEP]': -100,\n",
    "    # '[PAD]': 2, #_create_one_hot_encoding(9, 10)\n",
    "    'III': -100,   # Created III instead of iii for PAD special token. \n",
    "                # This is for enabling reading contigs if token is predicted as padding. \n",
    "                # #_create_one_hot_encoding(9, 10)\n",
    "    'iii': 0,   #_create_one_hot_encoding(1, 10),\n",
    "    'iiE': 1,   #_create_one_hot_encoding(2, 10),\n",
    "    'iEi': 2,   #_create_one_hot_encoding(3, 10),\n",
    "    'Eii': 3,   #_create_one_hot_encoding(4, 10),\n",
    "    'iEE': 4,   #_create_one_hot_encoding(5, 10),\n",
    "    'EEi': 5,   #_create_one_hot_encoding(6, 10),\n",
    "    'EiE': 6,   #_create_one_hot_encoding(7, 10),\n",
    "    'EEE': 7,  #_create_one_hot_encoding(8, 10),\n",
    "}\n",
    "\n",
    "label_dict = Label_Dictionary\n",
    "\n",
    "Index_Dictionary = {\n",
    "    #0: \"[CLS]\",\n",
    "    #1: \"[SEP]\",\n",
    "    #2: \"III\",       # Use `III` as padding symbol.\n",
    "    0: \"iii\",\n",
    "    1: \"iiE\",\n",
    "    2: \"iEi\",\n",
    "    3: \"Eii\",\n",
    "    4: \"iEE\",\n",
    "    5: \"EEi\",\n",
    "    6: \"EiE\",\n",
    "    7: \"EEE\",\n",
    "    -100: \"[CLS]/[SEP]/[III]\"\n",
    "}\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "training_data_path = os.path.join(data_dir, \"gene_index.01_train_validation_ss_all_pos_train.csv\")\n",
    "validation_data_path = os.path.join(data_dir, \"gene_index.01_train_validation_ss_all_pos_validation.csv\")\n",
    "test_data_path = os.path.join(data_dir, \"gene_index.01_test_ss_all_pos.csv\")\n",
    "\n",
    "def preprocessing(data_path):\n",
    "  if os.path.exists(data_path):\n",
    "    print(f\"data exists at {data_path}\")\n",
    "  else:\n",
    "    raise FileNotFoundError(f\"data not found ata {data_path}\")\n",
    "  encoded_sequences = []\n",
    "  encoded_labels = []\n",
    "  df = pd.read_csv(data_path)\n",
    "  for i, r in tqdm(df.iterrows(), total=df.shape[0], desc=\"Preprocessing\"):\n",
    "    sequence = r[\"sequence\"]\n",
    "    label = r[\"label\"]\n",
    "\n",
    "    encoded_sequence = [kmer_dict[a] for a in sequence.split(' ')]\n",
    "    encoded_label = [label_dict[a] for a in label.split(' ')]\n",
    "\n",
    "    if len(encoded_sequence) != 512 or len(encoded_label) != 512:\n",
    "      raise ValueError(f\"input size not correct. expected 512 and 512, found {len(encoded_sequence)} {len(encoded_label)}\")\n",
    "\n",
    "    encoded_sequences.append(\n",
    "        encoded_sequence\n",
    "    )\n",
    "    encoded_labels.append(\n",
    "        encoded_label\n",
    "    )\n",
    "  return encoded_sequences, encoded_labels\n",
    "\n",
    "\n",
    "X_train, Y_train = preprocessing(training_data_path)\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array([tf.keras.utils.to_categorical(y, num_classes=num_classes) for y in Y_train])\n",
    "X_val, Y_val = preprocessing(validation_data_path)\n",
    "X_val = np.array(X_val)\n",
    "Y_val = np.array([tf.keras.utils.to_categorical(y, num_classes=num_classes) for y in Y_val])\n",
    "X_test, Y_test = preprocessing(test_data_path)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array([tf.keras.utils.to_categorical(y, num_classes=num_classes) for y in Y_test])\n",
    "\n",
    "print(f\"Training data {np.array(X_train).shape}, {np.array(Y_train).shape}\")\n",
    "print(f\"Validation data {np.array(X_val).shape}, {np.array(Y_val).shape}\")\n",
    "print(f\"Test data {np.array(X_test).shape}, {np.array(Y_test).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6977, 512) (6977, 512, 8)\n",
      "219/219 [==============================] - 94s 421ms/step\n",
      "(6977, 512, 8)\n",
      "(6977, 512, 8) (6977, 512, 8)\n",
      "(6977, 512) (6977, 512)\n",
      "(3572224,) (3572224,)\n",
      "[7 7 7 ... 7 7 7] True\n",
      "[7 7 7 ... 7 7 7] True\n",
      "8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         iii       0.82      0.87      0.85   2697709\n",
      "         iiE       0.81      0.30      0.44      5857\n",
      "         iEi       0.00      0.00      0.00         0\n",
      "         Eii       0.61      0.36      0.45      5876\n",
      "         iEE       0.75      0.26      0.38      5856\n",
      "         EEi       0.58      0.36      0.45      5875\n",
      "         EiE       0.00      0.00      0.00         0\n",
      "         EEE       0.50      0.41      0.45    851051\n",
      "\n",
      "   micro avg       0.76      0.76      0.76   3572224\n",
      "   macro avg       0.51      0.32      0.38   3572224\n",
      "weighted avg       0.74      0.76      0.75   3572224\n",
      "\n",
      "(6977, 512) (6977, 512, 8)\n",
      "219/219 [==============================] - 95s 427ms/step\n",
      "(6977, 512, 8)\n",
      "(6977, 512, 8) (6977, 512, 8)\n",
      "(6977, 512) (6977, 512)\n",
      "(3572224,) (3572224,)\n",
      "[0 0 0 ... 7 7 7] True\n",
      "[7 7 7 ... 7 7 7] True\n",
      "8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         iii       0.86      0.81      0.84   2697709\n",
      "         iiE       0.85      0.33      0.48      5857\n",
      "         iEi       0.00      0.00      0.00         0\n",
      "         Eii       0.66      0.67      0.67      5876\n",
      "         iEE       0.81      0.39      0.52      5856\n",
      "         EEi       0.67      0.70      0.68      5875\n",
      "         EiE       0.00      0.00      0.00         0\n",
      "         EEE       0.50      0.61      0.55    851051\n",
      "\n",
      "   micro avg       0.76      0.76      0.76   3572224\n",
      "   macro avg       0.54      0.44      0.47   3572224\n",
      "weighted avg       0.78      0.76      0.76   3572224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bilstm_model = tf.keras.models.load_model(\n",
    "    os.path.join(model_dir, \"model_bilstm.h5\")\n",
    "    )\n",
    "\n",
    "bigru_model = tf.keras.models.load_model(\n",
    "    os.path.join(model_dir, \"model_bigru.h5\")\n",
    "    )\n",
    "\n",
    "for m in [bilstm_model, bigru_model]:\n",
    "\n",
    "    print(X_test.shape, Y_test.shape)\n",
    "    y_pred = m.predict(X_test)\n",
    "    print(y_pred.shape)\n",
    "\n",
    "    print(y_pred.shape, Y_test.shape)\n",
    "\n",
    "    y_pred_ids = np.argmax(y_pred, axis=-1)\n",
    "    y_test_ids = np.argmax(Y_test, axis=-1)\n",
    "\n",
    "    print(y_pred_ids.shape, y_test_ids.shape)\n",
    "\n",
    "    y_pred_ids_flatten = y_pred_ids.flatten()\n",
    "    y_test_ids_flatten = y_test_ids.flatten()\n",
    "\n",
    "    label_indices = [i for i in range(num_classes)]\n",
    "    print(y_pred_ids_flatten.shape, y_test_ids_flatten.shape)\n",
    "    print(y_pred_ids_flatten, all([a in label_indices for a in y_pred_ids_flatten]))\n",
    "    print(y_test_ids_flatten, all([a in label_indices for a in y_test_ids_flatten]))\n",
    "    print(num_classes)\n",
    "    print(classification_report(\n",
    "        y_test_ids_flatten, \n",
    "        y_pred_ids_flatten, \n",
    "        labels=[i for i in range(num_classes)], \n",
    "        target_names=[Index_Dictionary.get(i) for i in range(num_classes)], \n",
    "        zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       1.00      0.67      0.80         3\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.60      0.60      0.60         5\n",
      "   macro avg       0.30      0.33      0.29         5\n",
      "weighted avg       0.70      0.60      0.61         5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\.virtualenv\\sequence-processing-310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\.virtualenv\\sequence-processing-310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\.virtualenv\\sequence-processing-310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\.virtualenv\\sequence-processing-310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\.virtualenv\\sequence-processing-310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\.virtualenv\\sequence-processing-310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = [0, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 2, 2, 1]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "target_indices = [0, 1, 2, 3, 4]\n",
    "print(classification_report(y_true, y_pred, labels=target_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\.virtualenv\\sequence-processing-310\\lib\\site-packages\\numpy\\lib\\arraysetops.py:608: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [31], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m y_true \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m      5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_names\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2164\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2162\u001b[0m headers \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mprecision\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mf1-score\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msupport\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   2163\u001b[0m \u001b[39m# compute per-class results without averaging\u001b[39;00m\n\u001b[1;32m-> 2164\u001b[0m p, r, f1, s \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[0;32m   2165\u001b[0m     y_true,\n\u001b[0;32m   2166\u001b[0m     y_pred,\n\u001b[0;32m   2167\u001b[0m     labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   2168\u001b[0m     average\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   2169\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   2170\u001b[0m     zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[0;32m   2171\u001b[0m )\n\u001b[0;32m   2172\u001b[0m rows \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(target_names, p, r, f1, s)\n\u001b[0;32m   2174\u001b[0m \u001b[39mif\u001b[39;00m y_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1567\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1565\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1566\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1567\u001b[0m MCM \u001b[39m=\u001b[39m multilabel_confusion_matrix(\n\u001b[0;32m   1568\u001b[0m     y_true,\n\u001b[0;32m   1569\u001b[0m     y_pred,\n\u001b[0;32m   1570\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1571\u001b[0m     labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   1572\u001b[0m     samplewise\u001b[39m=\u001b[39;49msamplewise,\n\u001b[0;32m   1573\u001b[0m )\n\u001b[0;32m   1574\u001b[0m tp_sum \u001b[39m=\u001b[39m MCM[:, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m]\n\u001b[0;32m   1575\u001b[0m pred_sum \u001b[39m=\u001b[39m tp_sum \u001b[39m+\u001b[39m MCM[:, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:506\u001b[0m, in \u001b[0;36mmultilabel_confusion_matrix\u001b[1;34m(y_true, y_pred, sample_weight, labels, samplewise)\u001b[0m\n\u001b[0;32m    504\u001b[0m le \u001b[39m=\u001b[39m LabelEncoder()\n\u001b[0;32m    505\u001b[0m le\u001b[39m.\u001b[39mfit(labels)\n\u001b[1;32m--> 506\u001b[0m y_true \u001b[39m=\u001b[39m le\u001b[39m.\u001b[39;49mtransform(y_true)\n\u001b[0;32m    507\u001b[0m y_pred \u001b[39m=\u001b[39m le\u001b[39m.\u001b[39mtransform(y_pred)\n\u001b[0;32m    508\u001b[0m sorted_labels \u001b[39m=\u001b[39m le\u001b[39m.\u001b[39mclasses_\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:138\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39mif\u001b[39;00m _num_samples(y) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([])\n\u001b[1;32m--> 138\u001b[0m \u001b[39mreturn\u001b[39;00m _encode(y, uniques\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclasses_)\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-310\\lib\\site-packages\\sklearn\\utils\\_encode.py:229\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    228\u001b[0m     \u001b[39mif\u001b[39;00m check_unknown:\n\u001b[1;32m--> 229\u001b[0m         diff \u001b[39m=\u001b[39m _check_unknown(values, uniques)\n\u001b[0;32m    230\u001b[0m         \u001b[39mif\u001b[39;00m diff:\n\u001b[0;32m    231\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my contains previously unseen labels: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(diff)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-310\\lib\\site-packages\\sklearn\\utils\\_encode.py:303\u001b[0m, in \u001b[0;36m_check_unknown\u001b[1;34m(values, known_values, return_mask)\u001b[0m\n\u001b[0;32m    300\u001b[0m         valid_mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(\u001b[39mlen\u001b[39m(values), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[0;32m    302\u001b[0m \u001b[39m# check for nans in the known_values\u001b[39;00m\n\u001b[1;32m--> 303\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39;49misnan(known_values)\u001b[39m.\u001b[39many():\n\u001b[0;32m    304\u001b[0m     diff_is_nan \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39misnan(diff)\n\u001b[0;32m    305\u001b[0m     \u001b[39mif\u001b[39;00m diff_is_nan\u001b[39m.\u001b[39many():\n\u001b[0;32m    306\u001b[0m         \u001b[39m# removes nan from valid_mask\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "label_names = ['class A', 'class B', 'class C', 'class D', 'class E', 'class F']\n",
    "y_true = [1, 2, 3, 2, 5]\n",
    "y_pred = [0, 2, 3, 4, 5]\n",
    "print(classification_report(y_true, y_pred, labels=label_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('sequence-processing-310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95adb5e5f98c5e0ba07577f17d74d9d3cc9cb557759904522f47cd47fc4449cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
