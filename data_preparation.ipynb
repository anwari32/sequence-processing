{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from data_dir import genome_grch37, grch37_sample_dir, genome_grch37_dir\n",
    "from data_dir import genome_grch38, grch38_sample_dir, genome_grch38_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_desc(desc):\n",
    "    # desc_obj = {'gene': '', 'gene_id': '', 'genebank': '', 'ensembl': ''}\n",
    "    desc_obj = {}\n",
    "    arr = desc.split(';') # Split desc with semicolon as separator.\n",
    "    for e in arr:\n",
    "        det = e.split('=') # Split every parameter and its corresponding value.\n",
    "        param = det[0].lower()\n",
    "        val = det[1]\n",
    "        desc_obj[param] = val\n",
    "        if param == \"dbxref\":\n",
    "            # Parse value of dbxref.\n",
    "            # i.e. Dbxref=GeneID:653635,Genbank:NR_024540.1,HGNC:HGNC:38034\n",
    "            # param = dbxref (in lowercase)\n",
    "            # val = GeneID:653635,Genbank:NR_024540.1,HGNC:HGNC:38034\n",
    "            dbxref_vals = val.split(',')\n",
    "            for e in dbxref_vals:\n",
    "                arr = e.split(':')\n",
    "                dbxref_param = arr[0].lower()\n",
    "                dbxref_val = arr[1]\n",
    "                if dbxref_param == 'geneid':\n",
    "                    desc_obj['gene_id'] = dbxref_val\n",
    "                elif dbxref_param == 'genbank':\n",
    "                    desc_obj['genbank'] = dbxref_val\n",
    "                elif dbxref_param == 'ensembl':\n",
    "                    desc_obj['ensembl'] = dbxref_val\n",
    "                else:\n",
    "                    break\n",
    "    \n",
    "    return desc_obj\n",
    "\n",
    "def _gff_parseline(line, regions):\n",
    "    if line[0] == '#':\n",
    "        return False\n",
    "    else:\n",
    "        words = line.split('\\t')\n",
    "        sequence_id = words[0]\n",
    "        refseq = words[1]\n",
    "        region = words[2]\n",
    "        start = int(words[3]) # One-based numbering.\n",
    "        start_index = start-1 # Zero-based numbering.\n",
    "        end = int(words[4])\n",
    "        end_index = end-1\n",
    "        desc = words[8] # Description.\n",
    "        desc_obj = _parse_desc(desc)\n",
    "        gene = desc_obj['gene'] if 'gene' in desc_obj.keys() else '' # Gene name.\n",
    "        gene_id = desc_obj['gene_id'] if 'gene_id' in desc_obj.keys() else '' # Gene ID\n",
    "        genbank = desc_obj['genbank'] if 'genbank' in desc_obj.keys() else '' # GeneBank\n",
    "        ensembl = desc_obj['ensembl'] if 'ensembl' in desc_obj.keys() else '' # Ensembl\n",
    "        if regions is None:\n",
    "            return {'sequence_id': sequence_id, 'refseq': refseq, 'region': region, 'start': start, 'start_index': start_index, 'end': end, 'end_index': end_index, 'desc': desc_obj, 'gene': gene, 'gene_id': gene_id, 'genbank': genbank, 'ensembl': ensembl}\n",
    "        elif region in regions:\n",
    "            return {'sequence_id': sequence_id, 'refseq': refseq, 'region': region, 'start': start, 'start_index': start_index, 'end': end, 'end_index': end_index, 'desc': desc_obj, 'gene': gene, 'gene_id': gene_id, 'genbank': genbank, 'ensembl': ensembl}\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def gff_to_csv(file, csv_output, regions):\n",
    "    if os.path.exists(file):\n",
    "        # Prepare file and dataframe.\n",
    "        if os.path.exists(csv_output):\n",
    "            os.remove(csv_output)\n",
    "        colnames = ['sequence_id', 'refseq', 'region', 'start_index', 'end_index', 'start', 'end', 'gene', 'gene_id', 'genebank', 'ensembl']\n",
    "        header = \",\".join(colnames)\n",
    "        f = open(file, 'r')\n",
    "        out = open(csv_output, 'x')\n",
    "        out.write(\"{} \\n\".format(header))\n",
    "        \n",
    "        for line in f:\n",
    "            d = _gff_parseline(line, regions)\n",
    "            try:\n",
    "                if d != False:\n",
    "                    if d:\n",
    "                        output = \"{},{},{},{},{},{},{},{},{},{},{}\\n\".format(d['sequence_id'], d['refseq'], d['region'], d['start_index'], d['end_index'], d['start'], d['end'], d['gene'], d['gene_id'], d['genbank'], d['ensembl'])\n",
    "                        out.write(output)\n",
    "                    else:\n",
    "                        break\n",
    "            except:\n",
    "                out.close()\n",
    "                f.close()\n",
    "\n",
    "        out.close()\n",
    "        f.close()\n",
    "\n",
    "print(genome_grch37)\n",
    "print(genome_grch38)\n",
    "print(grch37_sample_dir)\n",
    "print(grch38_sample_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = \"NC_000001.11\tRefSeq\tregion\t1\t248956422\t.\t+\t.\tID=NC_000001.11:1..248956422;Dbxref=taxon:9606;Name=1;chromosome=1;gbkey=Src;genome=chromosome;mol_type=genomic DNA\"\n",
    "# s = \"NC_000001.11\tBestRefSeq\texon\t13221\t14409\t.\t+\t.\tID=exon-NR_046018.2-3;Parent=rna-NR_046018.2;Dbxref=GeneID:100287102,Genbank:NR_046018.2,HGNC:HGNC:37102;gbkey=misc_RNA;gene=DDX11L1;product=DEAD/H-box helicase 11 like 1 (pseudogene);pseudo=true;transcript_id=NR_046018.2\"\n",
    "s = \"NC_000001.11\tBestRefSeq\texon\t29321\t29370\t.\t-\t.\tID=exon-NR_024540.1-1;Parent=rna-NR_024540.1;Dbxref=GeneID:653635,Genbank:NR_024540.1,HGNC:HGNC:38034;gbkey=misc_RNA;gene=WASH7P;product=WASP family homolog 7%2C pseudogene;pseudo=true;transcript_id=NR_024540.1\"\n",
    "d = _gff_parseline(s, ['exon'])\n",
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gff_to_csv(genome_grch37, grch37_sample_dir + \"/grch37_all_07012022.csv\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gff_to_csv(genome_grch38, grch38_sample_dir + \"/grch38_all.csv\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gff_to_csv(genome_grch37, grch37_sample_dir + \"/grch37_exon_only_06012022.csv\", ['exon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gff_to_csv(genome_grch38, grch38_sample_dir + \"/grch38_exon_only_06012022.csv\", ['exon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['sequence_id', 'refseq', 'region', 'start_index', 'end_index', 'start', 'end', 'gene', 'gene_id', 'genbank', 'ensembl']\n",
    "header = \",\".join(colnames)\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gff_to_csvs(gff_file, target_folder, regions, header):\n",
    "    f = open(gff_file)\n",
    "    target_file = target_folder + '/'\n",
    "    cur_seq = \"\"\n",
    "    temp_seq = \"\"\n",
    "    output_file = \"\"\n",
    "    file_to_write = {}\n",
    "    for line in f:\n",
    "        d = _gff_parseline(line, regions)\n",
    "        if d:\n",
    "            output = \"{},{},{},{},{},{},{},{},{},{},{} \\n\".format(d['sequence_id'], d['refseq'], d['region'], d['start_index'], d['end_index'], d['start'], d['end'], d['gene'], d['gene_id'], d['genbank'], d['ensembl'])\n",
    "            temp_seq = d['sequence_id']\n",
    "            if cur_seq == \"\":\n",
    "                cur_seq = temp_seq\n",
    "\n",
    "            # Prepare desired file to write.\n",
    "            output_file = target_file + temp_seq + '.csv'\n",
    "\n",
    "            # Compare if this sequence_id is the as previous sequence_id.\n",
    "            if temp_seq == cur_seq:\n",
    "\n",
    "                # If it is then write to desired file.\n",
    "                # Check if file exists. If not then create file.\n",
    "                if os.path.exists(output_file):\n",
    "                    file_to_write.write(output)\n",
    "                else:\n",
    "                    file_to_write = open(output_file, 'x')\n",
    "\n",
    "                    # Write header first.\n",
    "                    file_to_write.write(\"{}\\n\".format(header))\n",
    "                    file_to_write.write(output)\n",
    "            \n",
    "            # If this sequence_id is not the same as previous sequence_id, close the existing file.\n",
    "            elif cur_seq != temp_seq:\n",
    "                file_to_write.close()\n",
    "                cur_seq = temp_seq\n",
    "\n",
    "    # Close any file related to this procedure.\n",
    "    file_to_write.close()\n",
    "    f.close()                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gff_to_csvs(genome_grch38, './sample/grch38/genes', None, header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gff_to_csvs(genome_grch37, './sample/grch37/genes', None, header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from data_dir import hs_nc1\n",
    "\n",
    "print(hs_nc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    complete_sequence_file  : path of complete sequence file in FASTA format.\n",
    "    label_location_file     : path of file containing exon region in CSV.\n",
    "\"\"\"\n",
    "def generate_labels(complete_sequence_file):\n",
    "    print(\"reading complete sequence at {} \\n\".format(complete_sequence_file))\n",
    "    seq = SeqIO.parse(complete_sequence_file, \"fasta\")\n",
    "    return seq\n",
    "\n",
    "seq = generate_labels(hs_nc1)\n",
    "complete_sequence = list(seq)[0].seq\n",
    "complete_labels = ['.' if a != 'N' else 'N' for a in complete_sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_dir import chr1\n",
    "print(chr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Char 'N' represents any base. It indicates that sequence has no information regarding the base at that position.\n",
    "Char '.' represents any feature other than Exon. Char 'E' is exon.\n",
    "\"\"\"\n",
    "# Try open the csv using pandas.\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./sample/grch38/genes/NC_000001.11.csv')\n",
    "df.head(3)\n",
    "df[df['region'] == 'region'].loc[0]['end_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all gene in dataframe.\n",
    "genes = df['gene'].unique()\n",
    "genes = list(genes)\n",
    "genes = [a for a in genes if str(a) != 'nan']\n",
    "print('how many gene? {}'.format(len(genes)))\n",
    "# print(genes)\n",
    "genes[0]\n",
    "ndf = df[df['gene'] == genes[0]]\n",
    "#ndf = ndf.loc[ndf['region'].isin(['gene', 'pseudogene'])]\n",
    "ndf.iloc[0]['region']\n",
    "ndf = ndf[ndf['region'] == 'exon']\n",
    "ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in genes:\n",
    "    # Filter dataframe to contain certain genbank.\n",
    "    ndf = df[df['gene'] == g]\n",
    "\n",
    "    g_region_df = ndf.loc[ndf['region'].isin(['gene', 'pseudogene'])]\n",
    "    g_start_index = g_region_df.iloc[0]['start_index']\n",
    "    g_end_index = g_region_df.iloc[0]['end_index']\n",
    "\n",
    "    # Prepare sequence and its label.\n",
    "    g_sequence = complete_sequence[g_start_index:g_end_index+1]\n",
    "    g_label = ['N' if a == 'N' else '.' for a in g_sequence]\n",
    "\n",
    "    # Generate labels from this dataframe.\n",
    "    try:\n",
    "        exons = ndf[ndf['region'] == 'exon']\n",
    "        for i, row in exons.iterrows():\n",
    "            s = row['start_index']\n",
    "            e = row['end_index']\n",
    "\n",
    "            for j in range(s, e+1):\n",
    "                rel_index = j-g_start_index\n",
    "                g_label[rel_index] = 'E' if g_label[rel_index] != 'N' else 'N'\n",
    "\n",
    "        fname = fname = './sample/grch38/labels/{}.txt'.format(g)\n",
    "        g_file = open(fname, 'x')\n",
    "        g_file.write('{}\\n{}\\n'.format(g_sequence, \"\".join(g_label)))\n",
    "        g_file.close()\n",
    "    except IndexError:\n",
    "        print('gene {}, length {}'.format(g, len(g_sequence)))\n",
    "        print('gene region {}-{}'.format(g_start_index, g_end_index))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_seq1 = [c for c in complete_labels]\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    start_index = row['start_index']\n",
    "    end_index = row['end_index']\n",
    "    # print(\"region {}-{}\".format(start_index, end_index))\n",
    "    for j in range(start_index, end_index+1):\n",
    "        label_seq1[j] = 'E'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string(s, length):\n",
    "    return (s[0+i:length+i] for i in range(0, len(s), length))\n",
    "\n",
    "arr_label = split_string(label_seq1, 50)\n",
    "f = open('chr1.label.txt', 'x')\n",
    "for label in arr_label:\n",
    "    f.write(\"{}\\n\".format(\"\".join(label)))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"ATCGATGCAGCAGACGACAGCATCAGCATCGACTCGACGATCGACTGACTGACTGACTGAC\"\n",
    "print('len s {}'.format(len(s)))\n",
    "def _get_kmer(sequence, k):\n",
    "    lenseq = len(sequence)\n",
    "    if (lenseq > 0 and k > 0):\n",
    "        arr = [sequence[i:i+k] for i in range(lenseq+1-k)]\n",
    "        return arr\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "kmers = _get_kmer(s, 3)\n",
    "kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Process sequence into kmers.\n",
    "Create read by window for certain window size.\n",
    "@param sequence : a sequence.\n",
    "@param k_size : size of kmer.\n",
    "@param t_size : size of substring to be read from sequence.\n",
    "@window_size : size of sliding.\n",
    "\"\"\"\n",
    "def _read_by_window(sequence, k_size, t_size, window_size):\n",
    "    kmers = _get_kmer(sequence, k_size)\n",
    "    len_kmers = len(kmers)\n",
    "    reads = [kmers[i:i+t_size] for i in range(0, len_kmers+1, window_size)]\n",
    "    return reads\n",
    "\n",
    "reads = _read_by_window(s, 3, 4, 2)\n",
    "reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, basename\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "dirpath = './sample/grch38/labels'\n",
    "files = [(dirpath + '/' + a) for a in listdir(dirpath) if isfile(dirpath + '/' + a)]\n",
    "files = [a for a in files if isfile(a)] # files containing path of files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer_size = 3\n",
    "t_size = 512\n",
    "window_size = 256\n",
    "window_sizes = [64, 128, 256]\n",
    "for window_size in window_sizes:\n",
    "    for fpath in files:\n",
    "        try:\n",
    "            print('opening file {}'.format(fpath))\n",
    "            f = open(fpath)\n",
    "            filename = basename(fpath)\n",
    "            filename = filename.split('.')\n",
    "            extension = filename[1]\n",
    "            filename = filename[0]\n",
    "            t = {}\n",
    "            lines = f.readlines()\n",
    "            sequence = lines[0]\n",
    "            labels = lines[1]\n",
    "            if len(sequence) == len(labels): # Make sure that sequence and labels are compatible. If their size is different then something wrong with labelling process above.\n",
    "                seq_reads = _read_by_window(sequence, kmer_size, t_size, window_size)\n",
    "                label_reads = _read_by_window(labels, kmer_size, t_size, window_size)\n",
    "\n",
    "                tpath =\"{}/{}.k{}.t{}.w{}.csv\".format(dirpath, filename, kmer_size, t_size, window_size)\n",
    "                if os.path.exists(tpath):\n",
    "                    os.remove(tpath)\n",
    "                t = open(tpath, 'x')\n",
    "                t_header = ','.join(['kmers', 'labels'])\n",
    "                t.write('{}\\n'.format(t_header))\n",
    "                for j in range(len(seq_reads)):\n",
    "                    seqread = ';'.join(seq_reads[j])\n",
    "                    labelread = ';'.join(label_reads[j])\n",
    "                    entry = ','.join([seqread, labelread])\n",
    "                    t.write('{}\\n'.format(entry))\n",
    "                t.close()\n",
    "                f.close()\n",
    "        except Exception as e:\n",
    "            logging.error(traceback.format_exc())\n",
    "            logging.error('closing {}'.format(f.name))\n",
    "            logging.error('closing {}'.format(t.name))\n",
    "            t.close()\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates promoter dataset.\n",
    "\"\"\"\n",
    "hs_promoter_TATA = './data/promoter/deepromoter/hs_pos_TATA.txt'\n",
    "mm_promoter_TATA = './data/promoter/deepromoter/mm_pos_TATA.txt'\n",
    "hs_promoter_non_TATA = './data/promoter/deepromoter/hs_pos_nonTATA.txt'\n",
    "mm_promoter_non_TATA = './data/promoter/deepromoter/mm_pos_nonTATA.txt'\n",
    "\n",
    "positive_promoter = [hs_promoter_TATA]\n",
    "negative_promoter = [hs_promoter_non_TATA]\n",
    "\n",
    "# Create new dataset file.\n",
    "pos_promoter_dataset_file = './dataset/promoter/pos_prom_dataset.csv'\n",
    "neg_promoter_dataset_file = './dataset/promoter/neg_prom_dataset.csv'\n",
    "if os.path.exists(pos_promoter_dataset_file):\n",
    "    os.remove(pos_promoter_dataset_file)\n",
    "\n",
    "header = ','.join(['sequence', 'label'])\n",
    "t = open(pos_promoter_dataset_file, 'x')\n",
    "t.write('{}\\n'.format(header))\n",
    "\n",
    "# Generating promoter positive dataset.\n",
    "for ps in positive_promoter:\n",
    "    f = {}\n",
    "    try:\n",
    "        f = open(ps, 'r')\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            t.write('{},{}\\n'.format(line, 1))\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        print('Error {}'.format(e))\n",
    "        t.close()\n",
    "        f.close()\n",
    "\n",
    "t.close()\n",
    "if os.path.exists(neg_promoter_dataset_file):\n",
    "    os.remove(neg_promoter_dataset_file)\n",
    "\n",
    "t = open(neg_promoter_dataset_file, 'x')\n",
    "t.write('{}\\n'.format(header))\n",
    "\n",
    "# Generating promoter negative dataset.\n",
    "for ns in negative_promoter:\n",
    "    f = {}\n",
    "    try:\n",
    "        f = open(ns, 'r')\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            t.write('{},{}\\n'.format(line, 0))\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        print('Error {}'.format(e))\n",
    "        t.close()\n",
    "        f.close()\n",
    "\n",
    "t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Rebalance promoter dataset by selecting smallest count between positive and negative dataset.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "pos_df = pd.read_csv(pos_promoter_dataset_file)\n",
    "neg_df = pd.read_csv(neg_promoter_dataset_file)\n",
    "\n",
    "pos_count = len(pos_df)\n",
    "neg_count = len(neg_df)\n",
    "\n",
    "print('positive count {}, negative count {}'.format(pos_count, neg_count))\n",
    "count = pos_count if pos_count < neg_count else neg_count\n",
    "print('select count = {}'.format(count))\n",
    "\n",
    "sample_prom_pos_df = pos_df.sample(n=count, random_state=1)\n",
    "sample_prom_neg_df = neg_df.sample(n=count, random_state=1)\n",
    "\n",
    "print('dataset size pos {} neg {}'.format(sample_prom_pos_df.size, sample_prom_neg_df.size))\n",
    "sample_prom_pos_df.head(10)\n",
    "\n",
    "sample_prom_pos_df.to_csv('./dataset/promoter/sample_pos_dataset.csv')\n",
    "sample_prom_neg_df.to_csv('./dataset/promoter/sample_neg_dataset.csv')\n",
    "\n",
    "sample_prom_df = sample_prom_pos_df.append(sample_prom_neg_df)\n",
    "sample_prom_df.to_csv('./dataset/promoter/sample_prom_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split promoter data to train, validation, and test set.\n",
    "Set fraction 0.8, 0.1, and 0.1 for train, validation, and test set.\n",
    "\"\"\"\n",
    "prom_dataset_path = './dataset/promoter'\n",
    "pos_prom_path = '{}/pos_prom_dataset.csv'.format(prom_dataset_path)\n",
    "neg_prom_path = '{}/neg_prom_dataset.csv'.format(prom_dataset_path)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pos_prom_df = pd.read_csv(pos_prom_path)\n",
    "neg_prom_df = pd.read_csv(neg_prom_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CTCCACTTTTTCTCACGTTTATCTGAGCGAAAACAAGCACGGTTCG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCAGCAGATGGAAAACAGGACAATGTAACACTGTTCTTATCATCAC...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCAGAGAAACTGGTCTCTTGATAATAGCCATAGATTACATACTGTG...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  label\n",
       "0  CTCCACTTTTTCTCACGTTTATCTGAGCGAAAACAAGCACGGTTCG...      1\n",
       "1  CCAGCAGATGGAAAACAGGACAATGTAACACTGTTCTTATCATCAC...      1\n",
       "2  TCAGAGAAACTGGTCTCTTGATAATAGCCATAGATTACATACTGTG...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_prom_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGCCACGTGAGTCGCTGGGCTATGGGTGGTGGTGGGGGTGAGGGAG...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TACTTCCGGTTTCCACGGAGCTCCGCCCCTTAGGGGGGTTCTCGCT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GGAGCCTGGTAGGGAGGACAAATCTCTCGAAATCTCAGTTGGCGCT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  label\n",
       "0  AGCCACGTGAGTCGCTGGGCTATGGGTGGTGGTGGGGGTGAGGGAG...      0\n",
       "1  TACTTCCGGTTTCCACGGAGCTCCGCCCCTTAGGGGGGTTCTCGCT...      0\n",
       "2  GGAGCCTGGTAGGGAGGACAAATCTCTCGAAATCTCAGTTGGCGCT...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_prom_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 20 test 10\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate promoter data sample for model training, validation, and testing.\n",
    "\"\"\"\n",
    "sample_pos_prom_df = pos_prom_df\n",
    "sample_neg_prom_df = neg_prom_df\n",
    "sample_pos_prom_train_df = sample_pos_prom_df.sample(n=10, random_state=1337)\n",
    "sample_neg_prom_train_df = sample_neg_prom_df.sample(n=10, random_state=1337)\n",
    "sample_pos_prom_df = sample_pos_prom_df.drop(sample_pos_prom_train_df.index)\n",
    "sample_neg_prom_df = sample_neg_prom_df.drop(sample_neg_prom_train_df.index)\n",
    "sample_pos_prom_test_df = sample_pos_prom_df.sample(n=5, random_state=1337)\n",
    "sample_neg_prom_test_df = sample_neg_prom_df.sample(n=5, random_state=1337)\n",
    "\n",
    "sample_prom_train_df = sample_pos_prom_train_df.append(sample_neg_prom_train_df)\n",
    "sample_prom_test_df = sample_pos_prom_test_df.append(sample_neg_prom_test_df)\n",
    "\n",
    "print('train {} test {}'.format(len(sample_prom_train_df), len(sample_prom_test_df)))\n",
    "\n",
    "sample_prom_train_df.to_csv('./dataset/promoter/sample_promoter_train.csv', index=False)\n",
    "sample_prom_test_df.to_csv('./dataset/promoter/sample_promoter_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate poly-A dataset from human data only.\n",
    "Poly-A data is concluded from DeeReCT-PolyA and the DeeReCT-PolyA model uses 5-fold cross validation.\n",
    "Three of them for training, one for validation, and one for testing.\n",
    "For this section, first, second, and third fold are used for training; fourth for validation; and fifth for testing.\n",
    "\n",
    "DeeReCT-PolyA uses multiple datasets: dragon human (Kalkatawi et. al., 2012) and Omni human (Magana-Mora et. al., 2017).\n",
    "Omni dataset is chosen because it's relatively new (2017 vs 2012) and contains more data (Xia et. al., 2018).\n",
    "\"\"\"\n",
    "dragon_human_pos_dir = './data/poly-a/deerectpolya/human/dragon_polyA_data/positive5fold'\n",
    "dragon_human_neg_dir = './data/poly-a/deerectpolya/human/dragon_polyA_data/negative5fold'\n",
    "omni_human_pos_dir = './data/poly-a/deerectpolya/human/omni_polyA_data/positive'\n",
    "omni_human_neg_dir = './data/poly-a/deerectpolya/human/omni_polyA_data/negative'\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, basename\n",
    "\n",
    "pos_dir = omni_human_pos_dir\n",
    "neg_dir = omni_human_neg_dir\n",
    "pos_files = listdir(pos_dir)\n",
    "pos_files = ['{}/{}'.format(pos_dir, a) for a in listdir(pos_dir) if isfile('{}/{}'.format(pos_dir, a))]\n",
    "neg_files = listdir(neg_dir)\n",
    "neg_files = ['{}/{}'.format(neg_dir, a) for a in listdir(neg_dir) if isfile('{}/{}'.format(neg_dir, a))]\n",
    "\n",
    "#print(len(pos_files))\n",
    "#print(len(neg_files))\n",
    "\n",
    "dataset_dir = './dataset/poly-a'\n",
    "pos_dataset_path = '{}/pos_polya.csv'.format(dataset_dir)\n",
    "neg_dataset_path = '{}/neg_polya.csv'.format(dataset_dir)\n",
    "\n",
    "files = [(pos_files, pos_dataset_path, '1'), (neg_files, neg_dataset_path, '0')]\n",
    "for p in files:\n",
    "    fs = p[0]\n",
    "    dataset_path = p[1]\n",
    "    label = p[2]\n",
    "\n",
    "    if os.path.exists(dataset_path):\n",
    "        os.remove(dataset_path)\n",
    "    t = open(dataset_path, 'x')\n",
    "    t.write('{}\\n'.format(','.join(['sequence', 'label'])))\n",
    "    for fpath in fs:\n",
    "        f = {}\n",
    "        try:\n",
    "            f = open(fpath, 'r')\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                t.write('{},{}\\n'.format(line, label))\n",
    "        except Exception as e:\n",
    "            print('Error {}'.format(e))\n",
    "            f.close()\n",
    "        finally:\n",
    "            f.close()\n",
    "\n",
    "    t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split positive and negative data into three parts for training, validation, and test set.\n",
    "Process both data using pandas.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "pos_df = pd.read_csv(pos_dataset_path)\n",
    "neg_df = pd.read_csv(neg_dataset_path)\n",
    "pos_df_size = len(pos_df)\n",
    "neg_df_size = len(neg_df)\n",
    "\n",
    "if pos_df_size == neg_df_size:\n",
    "    print('both are {}. data balance.'.format(pos_df_size))\n",
    "else:\n",
    "    count = pos_df_size if pos_df_size < neg_df_size else neg_df_size\n",
    "    print('data imbalance at pos = {} and neg = {}.\\nSelect count = {}.'.format(pos_df_size, neg_df_size, count))\n",
    "\n",
    "# Split positive data into three parts.\n",
    "pos_train_df = pos_df.sample(frac=0.8, replace=False, random_state=1)\n",
    "pos_val_df = pos_df.drop(pos_train_df.index)\n",
    "pos_test_df = pos_val_df.sample(frac=0.5, replace=False, random_state=1)\n",
    "pos_val_df = pos_val_df.drop(pos_test_df.index)\n",
    "\n",
    "pos_train_df.to_csv('{}/pos_polya_train.csv'.format(dataset_dir), index=False)\n",
    "pos_val_df.to_csv('{}/pos_polya_val.csv'.format(dataset_dir), index=False)\n",
    "pos_test_df.to_csv('{}/pos_polya_test.csv'.format(dataset_dir), index=False)\n",
    "\n",
    "print('pos train set {}, pos validation set {}, pos test set {}'.format(len(pos_train_df), len(pos_val_df), len(pos_test_df)))\n",
    "\n",
    "# Split negative data into three parts.\n",
    "neg_train_df = neg_df.sample(frac=0.8, replace=False, random_state=1)\n",
    "neg_val_df = neg_df.drop(neg_train_df.index)\n",
    "neg_test_df = neg_val_df.sample(frac=0.5, replace=False, random_state=1)\n",
    "neg_val_df = neg_val_df.drop(neg_test_df.index)\n",
    "\n",
    "neg_train_df.to_csv('{}/neg_polya_train.csv'.format(dataset_dir), index=False)\n",
    "neg_val_df.to_csv('{}/neg_polya_val.csv'.format(dataset_dir), index=False)\n",
    "neg_test_df.to_csv('{}/neg_polya_test.csv'.format(dataset_dir), index=False)\n",
    "\n",
    "print('neg train set {}, neg validation set {}, neg test set {}'.format(len(neg_train_df), len(neg_val_df), len(neg_test_df)))\n",
    "\n",
    "# Merge each of train, validation, and test set.\n",
    "train_df = pos_train_df.append(neg_train_df)\n",
    "val_df = pos_val_df.append(neg_val_df)\n",
    "test_df = pos_test_df.append(neg_test_df)\n",
    "\n",
    "train_df.to_csv('{}/polya_train.csv'.format(dataset_dir), index=False)\n",
    "val_df.to_csv('{}/polya_val.csv'.format(dataset_dir), index=False)\n",
    "test_df.to_csv('{}/polya_test.csv'.format(dataset_dir), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create sample of poly-a training and test data.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "pos_polya_train_df = pd.read_csv('./dataset/poly-a/pos_polya_train.csv')\n",
    "neg_polya_train_df = pd.read_csv('./dataset/poly-a/neg_polya_train.csv')\n",
    "sample_pos_polya_train_df = pos_polya_train_df.sample(n=10, random_state=1337)\n",
    "sample_neg_polya_train_df = neg_polya_train_df.sample(n=10, random_state=1337)\n",
    "sample_pos_polya_train_df.append(sample_neg_polya_train_df).to_csv('./dataset/poly-a/sample_polya_train.csv', index=False)\n",
    "\n",
    "pos_polya_test_df = pd.read_csv('./dataset/poly-a/pos_polya_test.csv')\n",
    "neg_polya_test_df = pd.read_csv('./dataset/poly-a/neg_polya_test.csv')\n",
    "sample_pos_polya_test_df = pos_polya_test_df.sample(n=5, random_state=1337)\n",
    "sample_neg_polya_test_df = neg_polya_test_df.sample(n=5, random_state=1337)\n",
    "sample_pos_polya_test_df.append(sample_neg_polya_test_df).to_csv('./dataset/poly-a/sample_polya_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create splice-site dataset.\n",
    "\"\"\"\n",
    "from os.path import basename\n",
    "\n",
    "ss_dir = './data/splice-sites/splice-deep/'\n",
    "pos_acc_ss_hs = '{}/positive_DNA_seqs_acceptor_hs.fa'.format(ss_dir)\n",
    "pos_don_ss_hs = '{}/positive_DNA_seqs_donor_hs.fa'.format(ss_dir)\n",
    "neg_acc_ss_hs = '{}/negative_DNA_seqs_acceptor_hs.fa'.format(ss_dir)\n",
    "neg_don_ss_hs = '{}/negative_DNA_seqs_donor_hs.fa'.format(ss_dir)\n",
    "\n",
    "ss_dataset_dir = './dataset/splice-sites'\n",
    "pos_ss_acc_dataset = '{}/pos_ss_acc_hs.csv'.format(ss_dataset_dir)\n",
    "pos_ss_don_dataset = '{}/pos_ss_don_hs.csv'.format(ss_dataset_dir)\n",
    "neg_ss_acc_dataset = '{}/neg_ss_acc_hs.csv'.format(ss_dataset_dir)\n",
    "neg_ss_don_dataset = '{}/neg_ss_don_hs.csv'.format(ss_dataset_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [(pos_acc_ss_hs, 1, 'acc', pos_ss_acc_dataset), \n",
    "            (pos_don_ss_hs, 1, 'don', pos_ss_don_dataset), \n",
    "            (neg_acc_ss_hs, 0, 'acc', neg_ss_acc_dataset), \n",
    "            (neg_don_ss_hs, 0, 'don', neg_ss_don_dataset)]\n",
    "for p in files:\n",
    "    fname = p[0]\n",
    "    label = p[1]\n",
    "    acc_don = p[2]\n",
    "    dataset_path = p[3]\n",
    "\n",
    "    f = {}\n",
    "    t = {}\n",
    "    if os.path.exists(dataset_path):\n",
    "        os.remove(dataset_path)\n",
    "    try:\n",
    "        f = open(fname, 'r')\n",
    "        t = open(dataset_path, 'x')\n",
    "        t.write('{}\\n'.format(','.join(['sequence', 'label'])))\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            t.write('{},{}\\n'.format(line, label))\n",
    "        t.close()\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        print('Error {}'.format(e))\n",
    "        t.close()\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create train, validation, and test set for splice site. To do that, the data need to be balance.\n",
    "If not the sampling based on smallest count is required. Processing is done using pandas.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "pos_ss_acc_df = pd.read_csv(pos_ss_acc_dataset)\n",
    "pos_ss_don_df = pd.read_csv(pos_ss_don_dataset)\n",
    "neg_ss_acc_df = pd.read_csv(neg_ss_acc_dataset)\n",
    "neg_ss_don_df = pd.read_csv(neg_ss_don_dataset)\n",
    "\n",
    "print('{}\\n{}\\n{}\\n{}'.format(pos_ss_acc_dataset, pos_ss_don_dataset, neg_ss_acc_dataset, neg_ss_don_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ss_acc_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ss_don_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_ss_acc_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_ss_don_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because loading the dataframe is time consuming, leave the loading at cell above and do later processing here.\n",
    "pos_ss_acc_size = len(pos_ss_acc_df)\n",
    "pos_ss_don_size = len(pos_ss_don_df)\n",
    "neg_ss_acc_size = len(neg_ss_acc_df)\n",
    "neg_ss_don_size = len(neg_ss_don_df)\n",
    "\n",
    "count = 0\n",
    "if pos_ss_acc_size == pos_ss_don_size == neg_ss_acc_size == neg_ss_don_size:\n",
    "    print('dataset balance')\n",
    "    count = pos_ss_acc_size\n",
    "else:\n",
    "    print('dataset imbalance')\n",
    "    print('pos acc {}\\npos don {}\\nneg acc {}\\nneg don {}'.format(pos_ss_acc_size, pos_ss_don_size, neg_ss_acc_size, neg_ss_don_size))\n",
    "    count = min([pos_ss_acc_size, pos_ss_don_size, neg_ss_acc_size, neg_ss_don_size])\n",
    "    print('count = {}'.format(count))\n",
    "\n",
    "pos_ss_acc_df_sample = pos_ss_acc_df.sample(n=count, replace=False, random_state=1337)\n",
    "pos_ss_don_df_sample = pos_ss_don_df.sample(n=count, replace=False, random_state=1337)\n",
    "neg_ss_acc_df_sample = neg_ss_acc_df.sample(n=count, replace=False, random_state=1337)\n",
    "neg_ss_don_df_sample = neg_ss_don_df.sample(n=count, replace=False, random_state=1337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all training, validation, and test data to single file for each dataset.\n",
    "# Take train, validation, and test at 8:1:1 ratio.\n",
    "dfs = [(pos_ss_acc_df_sample, 'pos_ss_acc'), (pos_ss_don_df_sample, 'pos_ss_don'), (neg_ss_acc_df_sample, 'neg_ss_acc'), (neg_ss_don_df_sample, 'neg_ss_don')]\n",
    "training_df = pd.DataFrame(columns = ['sequence', 'label'])\n",
    "validation_df = pd.DataFrame(columns = ['sequence', 'label'])\n",
    "testing_df = pd.DataFrame(columns = ['sequence', 'label'])\n",
    "for p in dfs:\n",
    "    df = p[0]\n",
    "    fname = p[1]\n",
    "    train_df = df.sample(frac=0.8, random_state=1337)\n",
    "    val_df = df.drop(train_df.index)\n",
    "    test_df = val_df.sample(frac=0.5, random_state=37)\n",
    "    val_df = val_df.drop(test_df.index)\n",
    "\n",
    "    # try:\n",
    "    train_df.to_csv('{}/{}_train.csv'.format(ss_dataset_dir, fname), index=False)\n",
    "    training_df = training_df.append(train_df)\n",
    "    val_df.to_csv('{}/{}_val.csv'.format(ss_dataset_dir, fname), index=False)\n",
    "    validation_df = validation_df.append(val_df)\n",
    "    test_df.to_csv('{}/{}_test.csv'.format(ss_dataset_dir, fname), index=False)\n",
    "    testing_df = testing_df.append(test_df)\n",
    "\n",
    "    #except Exception as e:\n",
    "    #    print('Error {}'.format(e))\n",
    "\n",
    "training_df.to_csv('{}/ss_train.csv'.format(ss_dataset_dir), index=False)\n",
    "validation_df.to_csv('{}/ss_val.csv'.format(ss_dataset_dir), index=False)\n",
    "testing_df.to_csv('{}/ss_test.csv'.format(ss_dataset_dir), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create sample splice sites training and test data.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "ss_train_path = './dataset/splice-sites/ss_train.csv'\n",
    "ss_test_path = './dataset/splice-sites/ss_test.csv'\n",
    "\n",
    "ss_train_df = pd.read_csv(ss_train_path)\n",
    "ss_test_df = pd.read_csv(ss_test_path)\n",
    "pos_sample_ss_train_df = ss_train_df[ss_train_df['label'] == 1].sample(n=10, random_state=1337)\n",
    "neg_sample_ss_train_df = ss_train_df[ss_train_df['label'] == 0].sample(n=10, random_state=1337)\n",
    "\n",
    "pos_sample_ss_test_df = ss_test_df[ss_test_df['label'] == 1].sample(n=5, random_state=1337)\n",
    "neg_sample_ss_test_df = ss_test_df[ss_test_df['label'] == 0].sample(n=5, random_state=1337)\n",
    "\n",
    "sample_ss_train_df = pos_sample_ss_train_df.append(neg_sample_ss_train_df)\n",
    "sample_ss_test_df = pos_sample_ss_test_df.append(neg_sample_ss_test_df)\n",
    "sample_ss_train_df.to_csv('./dataset/splice-sites/sample_ss_train.csv', index=False)\n",
    "sample_ss_test_df.to_csv('./dataset/splice-sites/sample_ss_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate sample training and testing data from promoter, splice-sites, and poly-a.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "cols = ['sequence', 'label_prom', 'label_ss', 'label_polya']\n",
    "header = ','.join(cols)\n",
    "\n",
    "prom_train_sample = './dataset/promoter/sample_promoter_train.csv'\n",
    "prom_train_sample_df = pd.read_csv(prom_train_sample)\n",
    "prom_test_sample = './dataset/promoter/sample_promoter_test.csv'\n",
    "prom_test_sample_df = pd.read_csv(prom_test_sample)\n",
    "ss_train_sample = './dataset/splice-sites/sample_ss_train.csv'\n",
    "ss_train_sample_df = pd.read_csv(ss_train_sample)\n",
    "ss_test_sample = './dataset/splice-sites/sample_ss_test.csv'\n",
    "ss_test_sample_df = pd.read_csv(ss_test_sample)\n",
    "polya_train_sample = './dataset/poly-a/sample_polya_train.csv'\n",
    "polya_train_sample_df = pd.read_csv(polya_train_sample)\n",
    "polya_test_sample = './dataset/poly-a/sample_polya_test.csv'\n",
    "polya_test_sample_df = pd.read_csv(polya_test_sample)\n",
    "\n",
    "training_sample = pd.DataFrame(columns=cols)\n",
    "testing_sample = pd.DataFrame(columns=cols)\n",
    "\n",
    "# Append prom_train_sample.\n",
    "for i, r in prom_train_sample_df.iterrows():\n",
    "    row = {\n",
    "        'sequence': r['sequence'],\n",
    "        'label_prom': r['label'],\n",
    "        'label_ss': 0,\n",
    "        'label_polya': 0\n",
    "    }\n",
    "    training_sample = training_sample.append(row, ignore_index=True)\n",
    "\n",
    "for i, r in ss_train_sample_df.iterrows():\n",
    "    row = {\n",
    "        'sequence': r['sequence'],\n",
    "        'label_prom': 0,\n",
    "        'label_ss': r['label'],\n",
    "        'label_polya': 0\n",
    "    }\n",
    "    training_sample = training_sample.append(row, ignore_index=True)\n",
    "\n",
    "for i, r in polya_train_sample_df.iterrows():\n",
    "    row = {\n",
    "        'sequence': r['sequence'],\n",
    "        'label_prom': 0,\n",
    "        'label_ss': 0,\n",
    "        'label_polya': r['label']\n",
    "    }\n",
    "    training_sample = training_sample.append(row, ignore_index=True)\n",
    "\n",
    "\n",
    "# Append prom_test_sample.\n",
    "for i, r in prom_test_sample_df.iterrows():\n",
    "    # print('appending {} {}'.format(r['sequence'], r['label']))\n",
    "    row = {\n",
    "        'sequence': r['sequence'],\n",
    "        'label_prom': r['label'],\n",
    "        'label_ss': 0,\n",
    "        'label_polya': 0,\n",
    "    }\n",
    "    testing_sample = testing_sample.append(row, ignore_index=True)\n",
    "\n",
    "for i, r in ss_test_sample_df.iterrows():\n",
    "    row = {\n",
    "        'sequence': r['sequence'],\n",
    "        'label_prom': 0,\n",
    "        'label_ss': r['label'],\n",
    "        'label_polya': 0\n",
    "    }\n",
    "    testing_sample = testing_sample.append(row, ignore_index=True)\n",
    "\n",
    "for i, r in polya_test_sample_df.iterrows():\n",
    "    row = {\n",
    "        'sequence': r['sequence'],\n",
    "        'label_prom': 0,\n",
    "        'label_ss': 0,\n",
    "        'label_polya': r['label'],\n",
    "    }\n",
    "    testing_sample = testing_sample.append(row, ignore_index=True)\n",
    "\n",
    "training_sample.to_csv('./sample/training_sample.csv', index=False)\n",
    "testing_sample.to_csv('./sample/testing_sample.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7170ee380771f2003055f3cee3e2e4dd0d81d1dac73a8c82197236b1572f37c2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('sequence-processing': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
