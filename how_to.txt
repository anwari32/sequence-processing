HOW TO RUN MULTITASK TRAINING WITH SAMPLE TRAINING DATA with N_SAMPLE = 5, on GPU, on WINDOWS
$ python run_train_mtl.py \
    -p pretrained/3-new-12w-0 \
    -t workspace/train.sample.csv \
    -e 5 \
    -b 2 \
    --device=cuda:0 \
    --learning_rate=4e-4 \
    --epsilon=1e-6 \
    --warm_up=10 \
    --log=logs/sample_log/2022-02-27.e5.b2.tsample.loss_sum.txt \
    --limit_train=5 \
    --loss_strategy=sum 
    --save_model_path=result/sample/ \
    --beta1=0.9 \
    --beta2=0.98

HOW TO RUN MULTITASK TRAINING WITH TRAINING DATA train.1300.kmer.all.expanded.csv, on GPU:1. on LINUX
$ python run_train_mtl.py \
    -p pretrained/3-new-12w-0 \
    -t workspace/train.1300.kmer.all.expanded.csv \
    -e 10 \
    -b 128 \
    --device=cuda:1 \
    --learning_rate=4e-4 \
    --epsilon=1e-6 \
    --warm_up=10000 \
    --log=logs/mtl/train_1300/2022-03-03/2022-02-03.e10.b128.t1300.loss_sum.txt \
    --limit_train=0 \
    --loss_strategy=sum \
    --save_model_path=result/mtl/train_1300/2022-02-03.e10.b128.t1300.loss_sum/ \
    --beta1=0.9 \
    --beta2=0.98

HOW TO RUN MULTITASK TRAINING WITH TRAINING DATA train.2000.kmer.all.expanded.csv, on GPU:2
$ python run_train_mtl.py \
    -p pretrained/3-new-12w-0 \
    -t workspace/train.2000.kmer.all.expanded.csv \
    -e 10 \
    -b 128 \
    --device=cuda:2 \
    --learning_rate=4e-4 \
    --epsilon=1e-6 \
    --warm_up=10000 \
    --log=logs/mtl/train_2000/2022-03-03/2022-02-03.e10.b128.t2000.loss_sum.txt \
    --limit_train=0 \
    --loss_strategy=sum \
    --save_model_path=result/mtl/train_2000/2022-03-03/2022-02-03.e10.b128.t2000.loss_sum/ \
    --beta1=0.9 \
    --beta2=0.98
