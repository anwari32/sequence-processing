{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEARNING TENSORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Randomly initialize weights.\n",
    "wa = np.random.randn()\n",
    "wb = np.random.randn()\n",
    "wc = np.random.randn()\n",
    "wd = np.random.randn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUMPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16] [2.19856608 2.18154583 2.16455966 ... 0.92088761 0.91899073 0.9170749 ] 99 4511.435470617761\n",
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16] [1.87005544 1.8548837  1.83974313 ... 0.75729895 0.75485477 0.75239216] 199 3189.276266209214\n",
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16] [1.59525215 1.58161865 1.5680139  ... 0.61667588 0.61375163 0.61080932] 299 2255.622525334223\n",
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16] [1.36594971 1.35359535 1.34126777 ... 0.49654177 0.49319886 0.48983816] 399 1596.2261503727118\n",
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16] [1.1745747  1.16328431 1.15201907 ... 0.3939871  0.39027996 0.38655525] 499 1130.466173586347\n",
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16] [1.01482124 1.00441608 0.99403471 ... 0.3065     0.30247667 0.29843592] 599 801.4408944849591\n",
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16] [0.88143773 0.87176925 0.86212345 ... 0.23191521 0.22761794 0.22330335] 699 568.9828148237627\n",
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16] [0.77004949 0.76099424 0.75196075 ... 0.16836886 0.16383467 0.15928322] 799 404.73284958058514\n",
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16] [0.67701155 0.66846688 0.65994321 ... 0.11425848 0.10951972 0.10476375] 899 288.6659592555111\n",
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16] [0.59928618 0.59116672 0.58306765 ... 0.068208   0.06329288 0.05836056] 999 206.640028899102\n",
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16] [0.53434114 0.5265759  0.51883052 ... 0.02903701 0.02397004 0.01888589] 1099 148.66630113877497\n",
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16] [ 0.48006515  0.47259504  0.46514437 ... -0.00426604 -0.00946358\n",
      " -0.01467829] 1199 107.68873319970353\n",
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16] [ 0.43469745  0.42747331  0.42026824 ... -0.03256717 -0.03787684\n",
      " -0.0432037 ] 1299 78.72238080003984\n",
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16] [ 0.39676935  0.38975024  0.38274991 ... -0.05660716 -0.06201304\n",
      " -0.06743611] 1399 58.24511142357574\n",
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16] [ 0.36505546  0.3582073   0.35137768 ... -0.07701916 -0.0825075\n",
      " -0.08801303] 1499 43.7680995285871\n",
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16] [ 0.33853322  0.33182763  0.32514038 ... -0.09434391 -0.09990284\n",
      " -0.10547898] 1599 33.532515084658826\n",
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16] [ 0.31634913  0.30976247  0.30319398 ... -0.10904286 -0.11466218\n",
      " -0.12029873] 1699 26.295300125828625\n",
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16] [ 0.29779067  0.29130322  0.28483382 ... -0.12150958 -0.12718053\n",
      " -0.13286872] 1799 21.17784782260771\n",
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16] [ 0.28226286  0.27585822  0.26947149 ... -0.13207952 -0.13779457\n",
      " -0.14352686] 1899 17.559101558258714\n",
      "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
      "  3.14159265] [-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
      "  3.14315906e-03  1.22464680e-16] [ 0.26926882  0.26293329  0.25661558 ... -0.1410384  -0.14679108\n",
      " -0.15256102] 1999 15.000026362613621\n",
      "result: y = -0.08290237785225357 + 0.8635189977948355x + 0.014302039681035723x^2 + -0.09429452252497775x^3\n"
     ]
    }
   ],
   "source": [
    "# Create random input and output data.\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Randomly initialize weights.\n",
    "a = wa\n",
    "b = wb\n",
    "c = wc\n",
    "d = wd\n",
    "\n",
    "learning_rate = 1e-06\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y (y_hat)\n",
    "    y_hat = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = np.square(y_hat - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(x, y, y_hat, t, loss)\n",
    "\n",
    "    # Backpropagation to compute gradients.\n",
    "    grad_y_hat = 2.0 * (y_hat - y)\n",
    "    grad_a = grad_y_hat.sum()\n",
    "    grad_b = (grad_y_hat * x).sum()\n",
    "    grad_c = (grad_y_hat * x ** 2).sum()\n",
    "    grad_d = (grad_y_hat * x ** 3).sum()\n",
    "\n",
    "    # Update weights\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print('result: y = {} + {}x + {}x^2 + {}x^3'.format(a,b,c,d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: TENSORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run on gpu GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "if (torch.cuda.is_available()):\n",
    "    device = torch.device('cuda')\n",
    "    print('run on gpu {}'.format(torch.cuda.get_device_name()))\n",
    "\n",
    "wta = torch.randn((), device=device, dtype=dtype)\n",
    "wtb = torch.randn((), device=device, dtype=dtype)\n",
    "wtc = torch.randn((), device=device, dtype=dtype)\n",
    "wtd = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "torch_learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1282767.25\n",
      "199 1282767.25\n",
      "299 1282767.25\n",
      "399 1282767.25\n",
      "499 1282767.25\n",
      "599 1282767.25\n",
      "699 1282767.25\n",
      "799 1282767.25\n",
      "899 1282767.25\n",
      "999 1282767.25\n",
      "1099 1282767.25\n",
      "1199 1282767.25\n",
      "1299 1282767.25\n",
      "1399 1282767.25\n",
      "1499 1282767.25\n",
      "1599 1282767.25\n",
      "1699 1282767.25\n",
      "1799 1282767.25\n",
      "1899 1282767.25\n",
      "1999 1282767.25\n",
      "result: y = 0.5465489625930786 + 0.0626697987318039x + -1.600311517715454x^2 + 2.1001343727111816x^3\n"
     ]
    }
   ],
   "source": [
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "a = wta\n",
    "b = wtb\n",
    "c = wtc\n",
    "d = wtd\n",
    "\n",
    "for t in range(2000):\n",
    "    # Compute predicted y (y_hat)\n",
    "    y_hat = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_hat - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backpropagation to compute gradients of a, b, c, and d according to loss\n",
    "    grad_y_hat = 2 * (y_hat - 2)\n",
    "    grad_a = grad_y_hat.sum()\n",
    "    grad_b = (grad_y_hat * x).sum()\n",
    "    grad_c = (grad_y_hat * x ** 2).sum()\n",
    "    grad_d = (grad_y_hat * x ** 3).sum()\n",
    "\n",
    "    # Update weights\n",
    "    a -= torch_learning_rate * grad_a\n",
    "    b -= torch_learning_rate * grad_b\n",
    "    c -= torch_learning_rate * grad_c\n",
    "    d -= torch_learning_rate * grad_d\n",
    "\n",
    "print('result: y = {} + {}x + {}x^2 + {}x^3'.format(a,b,c,d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: TENSORS & AUTOGRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Run on {}\".format(torch.cuda.get_device_name()))\n",
    "\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 tensor(599568.8750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "199 tensor(599568.8750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "299 tensor(599568.8750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "399 tensor(599568.8750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "499 tensor(599568.8750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "599 tensor(599568.8750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "699 tensor(599568.8750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "799 tensor(599568.8750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "899 tensor(599568.8750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "999 tensor(599568.8750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "1099 tensor(599568.8750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "1199 tensor(599568.8750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "1299 tensor(599568.8750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "1399 tensor(599568.8750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "1499 tensor(599568.8750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "1599 tensor(599568.8750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "1699 tensor(599568.8750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "1799 tensor(599568.8750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "1899 tensor(599568.8750, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "1999 tensor(599568.8750, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Reuse old variables from previous session\n",
    "for t in range(2000):\n",
    "    y_hat = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operation on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets scalar value held in the loss variable.\n",
    "    loss = (y_hat - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7170ee380771f2003055f3cee3e2e4dd0d81d1dac73a8c82197236b1572f37c2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('sequence-processing': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
