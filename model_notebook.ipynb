{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ..\\pretrained\\3-new-12w-0 were not used when initializing DNABertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DNABertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DNABertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DNABertForSequenceClassification were not initialized from the model checkpoint at ..\\pretrained\\3-new-12w-0 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  5,  8, 17, 55, 14, 41, 24,  3]]) tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[ 0.0109, -0.1443, -0.3833]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from seqclass import DNABertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "import json\n",
    "import os\n",
    "\n",
    "pretrained_path = os.path.join(\"..\", \"pretrained\", \"3-new-12w-0\")\n",
    "model = DNABertForSequenceClassification.from_pretrained(\n",
    "    pretrained_path,\n",
    "    num_labels = 3\n",
    ")\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_path)\n",
    "input_tokens = \"AAA AAG AGA GAC ACT CTA TAG\"\n",
    "encoded = tokenizer.encode_plus(input_tokens, return_tensors=\"pt\")\n",
    "input_ids = encoded.get(\"input_ids\")\n",
    "attention_mask = encoded.get(\"attention_mask\")\n",
    "print(input_ids, attention_mask)\n",
    "\n",
    "output = model(input_ids, attention_mask)\n",
    "print(output.logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pretrained\\3-new-12w-0 were not used when initializing DNABertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing DNABertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DNABertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DNABertForTokenClassification were not initialized from the model checkpoint at pretrained\\3-new-12w-0 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import os\n",
    "from models.seqlab import DNABertForTokenClassification\n",
    "from utils.seqlab import label2id, id2label\n",
    "import torch\n",
    "\n",
    "pretrained_path = os.path.join(\"pretrained\", \"3-new-12w-0\")\n",
    "model = DNABertForTokenClassification.from_pretrained(\n",
    "    pretrained_path,\n",
    "    num_labels = 8,\n",
    "    id2label = id2label,\n",
    "    label2id = label2id,\n",
    "    output_hidden_states = True,\n",
    "    output_attentions = True)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_path)\n",
    "input_tokens = \"AAA AAG AGA GAC ACT CTA TAG\"\n",
    "labels = torch.tensor([-100, 0, 0, 4, 2, 3, 6, 7, -100])\n",
    "encoded = tokenizer.encode_plus(input_tokens, return_tensors=\"pt\")\n",
    "input_ids = encoded.get(\"input_ids\")\n",
    "attention_mask = encoded.get(\"attention_mask\")\n",
    "\n",
    "output = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "output.loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5822,  0.0397,  0.5947],\n",
      "         [-0.6133, -0.3895,  0.4082]],\n",
      "\n",
      "        [[ 1.9722,  0.8546,  0.6370],\n",
      "         [ 0.9444,  0.9289,  1.4358]],\n",
      "\n",
      "        [[ 0.2974, -0.8921,  0.5509],\n",
      "         [-1.4582, -0.0312,  0.6434]],\n",
      "\n",
      "        [[ 0.8788, -1.0179, -0.4394],\n",
      "         [-0.3536, -1.2244,  0.2192]],\n",
      "\n",
      "        [[-0.2136, -0.7131, -0.7721],\n",
      "         [-0.2241, -3.6508, -0.0841]]])\n",
      "torch.Size([5, 2])\n",
      "tensor([2, 2, 0, 2, 2, 2, 0, 2, 0, 2])\n",
      "tensor([[[0.1638, 0.3050, 0.5313],\n",
      "         [0.1989, 0.2488, 0.5523]],\n",
      "\n",
      "        [[0.6289, 0.2057, 0.1655],\n",
      "         [0.2763, 0.2721, 0.4517]],\n",
      "\n",
      "        [[0.3857, 0.1174, 0.4969],\n",
      "         [0.0749, 0.3122, 0.6129]],\n",
      "\n",
      "        [[0.7054, 0.1059, 0.1888],\n",
      "         [0.3133, 0.1312, 0.5555]],\n",
      "\n",
      "        [[0.4590, 0.2785, 0.2625],\n",
      "         [0.4581, 0.0149, 0.5270]]])\n",
      "torch.Size([5, 2])\n",
      "tensor([2, 2, 0, 2, 2, 2, 0, 2, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.randn(5, 2, 3)\n",
    "print(a)\n",
    "print(torch.argmax(a, 2).shape)\n",
    "print(torch.argmax(a, 2).view(-1))\n",
    "softmax = torch.nn.Softmax(2)\n",
    "a_softmax = softmax(a)\n",
    "print(a_softmax)\n",
    "print(torch.argmax(a_softmax, 2).shape)\n",
    "print(torch.argmax(a_softmax, 2).view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]\n",
      "[[1, 3, 4, 2, 5], [1, 3, 4, 2, 5], [1, 3, 4, 2, 5], [1, 3, 4, 2, 5], [1, 3, 4, 2, 5]]\n",
      "[1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n",
      "[1, 3, 4, 2, 5, 1, 3, 4, 2, 5, 1, 3, 4, 2, 5, 1, 3, 4, 2, 5, 1, 3, 4, 2, 5]\n",
      "0 1.0 0.0 0.0 0.0\n",
      "0 1.0 0.0 0.0 0.0\n",
      "0 1.0 0 0 0\n",
      "[0 0 0 0 0 0]\n",
      "[0 5 0 0 0 0]\n",
      "[0 0 0 0 5 0]\n",
      "[0 0 5 0 0 0]\n",
      "[0 0 0 5 0 0]\n",
      "[0 0 0 0 0 5]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "y_pred = []\n",
    "for i in range(5):\n",
    "    y_pred.append(\n",
    "        [1, 2, 3, 4, 5,]\n",
    "    )\n",
    "\n",
    "print(y_pred)\n",
    "y_pred = torch.flatten(torch.tensor(y_pred))\n",
    "\n",
    "y_target = []\n",
    "for i in range(5):\n",
    "    y_target.append(\n",
    "        [1, 3, 4, 2, 5,]\n",
    "    )\n",
    "\n",
    "print(y_target)\n",
    "y_target = torch.flatten(torch.tensor(y_target))\n",
    "\n",
    "from utils.metrics import Metrics\n",
    "\n",
    "y_pred = y_pred.tolist()\n",
    "y_target = y_target.tolist()\n",
    "print(y_pred)\n",
    "print(y_target)\n",
    "m = Metrics(y_pred, y_target, num_classes=6)\n",
    "m.calculate()\n",
    "print(m.precision(0), m.precision(1), m.precision(2), m.precision(3), m.precision(4))\n",
    "print(m.recall(0), m.recall(1), m.recall(2), m.recall(3), m.recall(4))\n",
    "print(m.f1_score(0), m.f1_score(1), m.f1_score(2), m.f1_score(3), m.f1_score(4))\n",
    "m.print_cf()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('sequence-processing')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14ae8cb2141f3f34f4e0523006ff2d6cb0f7956c0f094e5497e312072e4d0d3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
