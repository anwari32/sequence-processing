{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\.virtualenv\\deep-learning\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Extracting motif: 100%|██████████| 70868/70868 [03:07<00:00, 378.03it/s]\n",
      "Extracting motif: 100%|██████████| 17717/17717 [00:45<00:00, 389.88it/s]\n",
      "Extracting motif: 100%|██████████| 6961/6961 [00:17<00:00, 388.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# motif analysis\n",
    "import os\n",
    "import pandas as pd\n",
    "from utils.motif import donor_pattern, acceptor_pattern, default_window_size\n",
    "from utils.utils import kmer, merge_kmer\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_data = os.path.join(\"workspace\", \"seqlab-latest\", \"train.csv\")\n",
    "validation_data = os.path.join(\"workspace\", \"seqlab-latest\", \"validation.csv\")\n",
    "test_data = os.path.join(\"workspace\", \"seqlab-latest\", \"test.csv\")\n",
    "paths = [train_data, validation_data, test_data]\n",
    "\n",
    "is_exists = all([os.path.exists(p) for p in paths])\n",
    "if not is_exists:\n",
    "    raise FileNotFoundError()\n",
    "\n",
    "for p in paths:\n",
    "    df = pd.read_csv(p)\n",
    "    dest_filename = \".\".join(os.path.basename(p).split(\".\")[0:-1])\n",
    "    donor_sequences, donor_targets, donor_sequence_tokens, donor_target_tokens = [], [], [], []\n",
    "    acceptor_sequences, acceptor_targets, acceptor_sequence_tokens, acceptor_target_tokens = [], [], [], []\n",
    "    for i, r in tqdm(df.iterrows(), total=df.shape[0], desc=\"Extracting motif\"):\n",
    "        sequence_tokens = r[\"sequence\"].split(\" \")\n",
    "        target_tokens = r[\"label\"].split(\" \")\n",
    "\n",
    "        arr_s = kmer(sequence_tokens, default_window_size)\n",
    "        arr_t = kmer(target_tokens, default_window_size)\n",
    "\n",
    "        for i, j in zip(arr_s, arr_t):\n",
    "            _j = kmer(j, 2, 1)\n",
    "            if donor_pattern in _j:\n",
    "                donor_sequences.append(merge_kmer(i))\n",
    "                donor_targets.append(merge_kmer(j))\n",
    "                donor_sequence_tokens.append(\" \".join(i))\n",
    "                donor_target_tokens.append(\" \".join(j))\n",
    "            if acceptor_pattern in _j:\n",
    "                acceptor_sequences.append(merge_kmer(i))\n",
    "                acceptor_targets.append(merge_kmer(j))\n",
    "                acceptor_sequence_tokens.append(\" \".join(i))\n",
    "                acceptor_target_tokens.append(\" \".join(j))\n",
    "    \n",
    "    pd.DataFrame(data={\n",
    "        \"sequence\": donor_sequences,\n",
    "        \"target\": donor_targets,\n",
    "        \"sequence_tokens\": donor_sequence_tokens,\n",
    "        \"target_tokens\": donor_target_tokens\n",
    "    }).to_csv(os.path.join(\"motif_analysis\", \"seqlab-latest\", f\"{dest_filename}.donor.csv\"), index=False)\n",
    "    pd.DataFrame(data={\n",
    "        \"sequence\": acceptor_sequences,\n",
    "        \"target\": acceptor_targets,\n",
    "        \"sequence_tokens\": acceptor_sequence_tokens,\n",
    "        \"target_tokens\": acceptor_target_tokens\n",
    "    }).to_csv(os.path.join(\"motif_analysis\", \"seqlab-latest\", f\"{dest_filename}.acceptor.csv\"), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting unique motif: 100%|██████████| 3/3 [00:01<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# extract acceptor unique motif.\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "names = [\"train\", \"validation\", \"test\"]\n",
    "acceptors = [f\"{p}.donor.csv\" for p in names]\n",
    "acceptors = [os.path.join(\"motif_analysis\", \"seqlab-latest\", p) for p in acceptors]\n",
    "for p in tqdm(acceptors, total=len(acceptors), desc=\"Extracting unique motif\"):\n",
    "    df = pd.read_csv(p)\n",
    "    udf = df.drop_duplicates(subset=[\"sequence\", \"target\"])\n",
    "\n",
    "    dest_name = os.path.basename(p).split(\".\")[0:-1]\n",
    "    dest_name = f\"{'.'.join(dest_name)}.unique.csv\"\n",
    "    dest_file = os.path.join(\"motif_analysis\", \"seqlab-latest\", dest_name)\n",
    "    udf.to_csv(dest_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-reference training and test data to see if their acceptor motifs are intersected.\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "for p in [\"donor\", \"acceptor\"]:\n",
    "    dir_path = os.path.join(\"motif_analysis\", \"seqlab-latest\")\n",
    "    test_path = os.path.join(dir_path, f\"test.{p}.unique.csv\")\n",
    "    validation_path = os.path.join(dir_path, f\"validation.{p}.unique.csv\")\n",
    "    train_path = os.path.join(dir_path, f\"train.{p}.unique.csv\")\n",
    "\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    validation_df = pd.read_csv(validation_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    def check(row, dataframe):\n",
    "        seq = row[\"sequence\"]\n",
    "        count = dataframe[dataframe[\"sequence\"] == seq].shape[0]\n",
    "        if count > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    test_df[\"is_motif_in_train_data\"] = test_df.apply(lambda x: check(x, train_df), axis=1)\n",
    "    test_df.to_csv(\n",
    "        test_path, \n",
    "        index=False\n",
    "    )\n",
    "    validation_df[\"is_motif_in_train_data\"] = validation_df.apply(lambda x: check(x, train_df), axis=1)\n",
    "    validation_df.to_csv(\n",
    "        validation_path,\n",
    "        index=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from utils.seqlab import Index_Dictionary\n",
    "\n",
    "path = os.path.join(\"prediction\", \"log\", \"prediction_log.csv\")\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(\"pretrained\", \"3-new-12w-0\"))\n",
    "df[\"input_tokens\"] = df.apply(lambda x: \" \".join(tokenizer.convert_ids_to_tokens([int(a) for a in x[\"input_ids\"].split(\" \")])), axis=1)\n",
    "df[\"prediction_tokens\"] = df.apply(lambda x: \" \".join([Index_Dictionary[a] for a in [int(b) for b in x[\"prediction_ids\"].split(\" \")]]), axis=1)\n",
    "df[\"target_tokens\"] = df.apply(lambda x: \" \".join([Index_Dictionary[a] for a in [int(b) for b in x[\"target_ids\"].split(\" \")]]), axis=1)\n",
    "df.to_csv(os.path.join(\"prediction\", \"log\", \"prediction_log_complete.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 31)\n",
      "(180, 3)\n",
      "(180, 3)\n",
      "(0, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(os.path.join(\"prediction\", \"log\", \"prediction_log_complete.csv\"))\n",
    "ss_labels = [\"iiE\", \"iEE\", \"EEi\", \"Eii\"]\n",
    "\n",
    "all_correct_df = df[df[\"avg_f1_score\"] == 1]\n",
    "print(all_correct_df.shape)\n",
    "tokens, predictions, targets = [], [], []\n",
    "for i, r in all_correct_df.iterrows():\n",
    "    arr_tokens = r[\"input_tokens\"].split(\" \")\n",
    "    arr_predictions = r[\"prediction_tokens\"].split(\" \")\n",
    "    arr_targets = r[\"target_tokens\"].split(\" \")\n",
    "\n",
    "    all_clear = all([a == b for a, b in zip(arr_predictions, arr_targets)])\n",
    "    if not all_clear:\n",
    "        raise ValueError(f\"{arr_predictions}\\n{arr_targets}\")\n",
    "\n",
    "    for i, j, k in zip(arr_tokens, arr_predictions, arr_targets):\n",
    "        if k in ss_labels:\n",
    "            tokens.append(i)\n",
    "            predictions.append(j)\n",
    "            targets.append(k)\n",
    "\n",
    "ndf = pd.DataFrame(data={\n",
    "    \"token\": tokens,\n",
    "    \"prediction\": predictions,\n",
    "    \"target\": targets\n",
    "})\n",
    "print(ndf.shape)\n",
    "correct_df = ndf[ndf[\"prediction\"] == ndf[\"target\"]]\n",
    "print(correct_df.shape)\n",
    "false_df = ndf[ndf[\"prediction\"] != ndf[\"target\"]]\n",
    "print(false_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGA</td>\n",
       "      <td>iiE</td>\n",
       "      <td>iiE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GAA</td>\n",
       "      <td>iEE</td>\n",
       "      <td>iEE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AGG</td>\n",
       "      <td>EEi</td>\n",
       "      <td>EEi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GGT</td>\n",
       "      <td>Eii</td>\n",
       "      <td>Eii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AGG</td>\n",
       "      <td>iiE</td>\n",
       "      <td>iiE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GGA</td>\n",
       "      <td>iEE</td>\n",
       "      <td>iEE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AGG</td>\n",
       "      <td>EEi</td>\n",
       "      <td>EEi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GGT</td>\n",
       "      <td>Eii</td>\n",
       "      <td>Eii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AGG</td>\n",
       "      <td>iiE</td>\n",
       "      <td>iiE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GGA</td>\n",
       "      <td>iEE</td>\n",
       "      <td>iEE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  token prediction target\n",
       "0   AGA        iiE    iiE\n",
       "1   GAA        iEE    iEE\n",
       "2   AGG        EEi    EEi\n",
       "3   GGT        Eii    Eii\n",
       "4   AGG        iiE    iiE\n",
       "5   GGA        iEE    iEE\n",
       "6   AGG        EEi    EEi\n",
       "7   GGT        Eii    Eii\n",
       "8   AGG        iiE    iiE\n",
       "9   GGA        iEE    iEE"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>GGT</td>\n",
       "      <td>iii</td>\n",
       "      <td>iEE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>GGT</td>\n",
       "      <td>iii</td>\n",
       "      <td>iEE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3156</th>\n",
       "      <td>GGT</td>\n",
       "      <td>iii</td>\n",
       "      <td>iEE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3407</th>\n",
       "      <td>AGG</td>\n",
       "      <td>iii</td>\n",
       "      <td>iiE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>GGT</td>\n",
       "      <td>iii</td>\n",
       "      <td>iEE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3982</th>\n",
       "      <td>GGT</td>\n",
       "      <td>iii</td>\n",
       "      <td>iEE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>AGG</td>\n",
       "      <td>EEE</td>\n",
       "      <td>iiE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4024</th>\n",
       "      <td>GGT</td>\n",
       "      <td>iii</td>\n",
       "      <td>iEE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4031</th>\n",
       "      <td>AGG</td>\n",
       "      <td>iii</td>\n",
       "      <td>iiE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4032</th>\n",
       "      <td>GGT</td>\n",
       "      <td>iii</td>\n",
       "      <td>iEE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     token prediction target\n",
       "1108   GGT        iii    iEE\n",
       "2770   GGT        iii    iEE\n",
       "3156   GGT        iii    iEE\n",
       "3407   AGG        iii    iiE\n",
       "3408   GGT        iii    iEE\n",
       "3982   GGT        iii    iEE\n",
       "3995   AGG        EEE    iiE\n",
       "4024   GGT        iii    iEE\n",
       "4031   AGG        iii    iiE\n",
       "4032   GGT        iii    iEE"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sequence-processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14ae8cb2141f3f34f4e0523006ff2d6cb0f7956c0f094e5497e312072e4d0d3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
