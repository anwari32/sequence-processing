{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check if CUDA is supported.\n",
    "\"\"\"\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.device('cuda:0')\n",
    "torch.cuda.get_device_name(0)\n",
    "_device = torch.device('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_preparation import kmer\n",
    "\n",
    "def get_sequences(csv_path, n_sample=10, random_state=1337):\n",
    "    r\"\"\"\n",
    "    Get sequence from certain CSV. CSV has header such as 'sequence', 'label_prom', 'label_ss', 'label_polya'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if (n_sample > 0):\n",
    "        df = df.sample(n=n_sample, random_state=random_state)\n",
    "    sequence = list(df['sequence'])\n",
    "    label_prom = list(df['label_prom'])\n",
    "    label_ss = list(df['label_ss'])\n",
    "    label_polya = list(df['label_polya'])\n",
    "\n",
    "    return sequence, label_prom, label_ss, label_polya\n",
    "\n",
    "import torch\n",
    "def preprocessing(data, tokenizer):\n",
    "    \"\"\"\n",
    "    Preprocessing for pretrained BERT.\n",
    "    @param  data (string): string containing kmers separated by spaces.\n",
    "    @param  tokenizer (Tokenizer): tokenizer initialized from pretrained values.\n",
    "    @return input_ids (torch.Tensor): tensor of token ids to be fed to model.\n",
    "    @return attention_masks (torch.Tensor): tensor of indices (a bunch of 'indexes') specifiying which token needs to be attended by model.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    _count = 0\n",
    "    _len_data = len(data)\n",
    "    for sequence in data:\n",
    "        \"\"\"\n",
    "        Sequence is 512 characters long.\n",
    "        \"\"\"\n",
    "        _count += 1\n",
    "        if _count < _len_data:\n",
    "            print(\"Seq length = {} [{}/{}]\".format(len(sequence.split(' ')), _count, _len_data), end='\\r')\n",
    "        else:\n",
    "            print(\"Seq length = {} [{}/{}]\".format(len(sequence.split(' ')), _count, _len_data))\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sequence,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert input_ids and attention_masks to tensor.\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "\"\"\"\n",
    "Initialize tokenizer using BertTokenizer with pretrained weights from DNABert.\n",
    "\"\"\"\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('./pretrained/3-new-12w-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting and storing split to ./workspace/train.csv\n",
      "Splitting and storing split to ./workspace/validation.csv\n",
      "Splitting source ./dataset/full/train.csv: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Split dataset into two parts: train and validation.\n",
    "\"\"\"\n",
    "from data_dir import workspace_dir, dataset_full_dir\n",
    "from data_preparation import split_and_store_csv\n",
    "_src_csv = \"{}/train.csv\".format(dataset_full_dir)\n",
    "_fractions = [0.9, 0.1]\n",
    "_store_paths = [\n",
    "    \"{}/{}\".format(workspace_dir, 'train.csv'),\n",
    "    \"{}/{}\".format(workspace_dir, 'validation.csv'),\n",
    "]\n",
    "print(\"Splitting source {}: {}\".format(_src_csv, split_and_store_csv(_src_csv, fractions=_fractions, store_paths=_store_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq length = 510 [500000/500000]\n",
      "Seq length = 510 [100/100]\n",
      "# of training data: 500000\n",
      "# of training data: 100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from data_dir import workspace_dir\n",
    "\n",
    "train_seq, train_label_prom, train_label_ss, train_label_polya = get_sequences('{}/train.csv'.format(workspace_dir), n_sample=500000)\n",
    "validation_seq, val_label_prom, val_label_ss, val_label_polya = get_sequences('{}/validation.csv'.format(workspace_dir), n_sample=100)\n",
    "\n",
    "\"\"\"\n",
    "Create dataloader.\n",
    "\"\"\"\n",
    "BATCH_SIZE = 2\n",
    "EPOCH_SIZE = 4\n",
    "\n",
    "_device = torch.device('cuda:0')\n",
    "train_label_prom = torch.tensor(train_label_prom, device=_device)\n",
    "train_label_ss = torch.tensor(train_label_ss, device=_device)\n",
    "train_label_polya = torch.tensor(train_label_polya, device=_device)\n",
    "\n",
    "train_inputs_ids, train_masks = preprocessing(train_seq, tokenizer)\n",
    "train_data = TensorDataset(train_inputs_ids, train_masks, train_label_prom, train_label_ss, train_label_polya)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "val_label_prom = torch.tensor(val_label_prom, device=_device)\n",
    "val_label_ss = torch.tensor(val_label_ss, device=_device)\n",
    "val_label_polya = torch.tensor(val_label_polya, device=_device)\n",
    "\n",
    "val_input_ids, val_masks = preprocessing(validation_seq, tokenizer)\n",
    "val_data = TensorDataset(val_input_ids, val_masks, val_label_prom, val_label_ss, val_label_polya)\n",
    "val_sampler = RandomSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "print('# of training data: {}'.format(len(train_seq)))  \n",
    "print('# of training data: {}'.format(len(validation_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from multitask_learning import init_model_mtl\n",
    "from transformers import BertForMaskedLM\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "\n",
    "# model = MTModel(shared_parameters=shared_parameter, promoter_head=promoter_head, polya_head=polya_head, splice_site_head=splice_head)\n",
    "model = init_model_mtl(pretrained_3kmer_dir, head=\"bert\", config=os.path.join(\"models\", \"config\", \"mtl.json\"))\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datetime\n",
    "import os\n",
    "from data_preparation import kmer\n",
    "\n",
    "_now = datetime.datetime.now()\n",
    "\n",
    "_log_file = os.path.join('logs', 'notebooks', '2022-02.24.csv')\n",
    "os.makedirs(_log_file, exist_ok=True)\n",
    "\n",
    "#seqs = [\"ATGC\" * 128, \"GATC\" * 128, \"CCAT\" * 128]\n",
    "seqs = [\"ATGC\" * 128]\n",
    "seqs = [' '.join(kmer(s, 3)) for s in seqs]\n",
    "prom_labels = [1] #, 0, 0]\n",
    "ss_labels = [0] #, 1, 0]\n",
    "polya_labels = [0] #, 0, 1]\n",
    "\n",
    "def _format_prom_label(label):\n",
    "    return [label]\n",
    "\n",
    "def _format_other_label(label):\n",
    "    return label\n",
    "\n",
    "\"\"\"\n",
    "Initialize BERT tokenizer.\n",
    "\"\"\"\n",
    "from transformers import BertTokenizer\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_3kmer_dir)\n",
    "\n",
    "arr_input_ids = []\n",
    "arr_attention_mask = []\n",
    "arr_prom_label = []\n",
    "arr_ss_label = []\n",
    "arr_polya_label = []\n",
    "for i in range(len(seqs)):\n",
    "    s = seqs[i]\n",
    "    prom = prom_labels[i]\n",
    "    ss = ss_labels[i]\n",
    "    polya = polya_labels[i]\n",
    "\n",
    "    encoded = tokenizer.encode_plus(text=s, padding=\"max_length\", return_attention_mask=True)\n",
    "    arr_input_ids.append(encoded.get('input_ids'))\n",
    "    arr_attention_mask.append(encoded.get('attention_mask'))\n",
    "    arr_prom_label.append(_format_prom_label(prom))\n",
    "    arr_ss_label.append(_format_other_label(ss))\n",
    "    arr_polya_label.append(_format_other_label(polya))\n",
    "#endfor\n",
    "arr_input_ids = torch.tensor(arr_input_ids)\n",
    "arr_attention_mask = torch.tensor(arr_attention_mask)\n",
    "prom_labels = torch.tensor(arr_prom_label)\n",
    "ss_labels = torch.tensor(arr_ss_label)\n",
    "polya_labels = torch.tensor(arr_polya_label)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "dataset = TensorDataset(arr_input_ids, arr_attention_mask, prom_labels, ss_labels, polya_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "target = torch.tensor([1])\n",
    "target.view(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sample 3\n",
      "tensor([[0.4280]], grad_fn=<SigmoidBackward0>) tensor([1])\n",
      "tensor([[0.4357, 0.5643]], grad_fn=<SoftmaxBackward0>) tensor([0])\n",
      "tensor([[0.5731, 0.4269]], grad_fn=<SoftmaxBackward0>) tensor([0])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'torch.dtype' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mw:\\Research\\sequence-processing\\multitask_learning.ipynb Cell 8'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/multitask_learning.ipynb#ch0000006?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(pred_polya, label_polya)\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/multitask_learning.ipynb#ch0000006?line=32'>33</a>\u001b[0m label_prom \u001b[39m=\u001b[39m label_prom\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/multitask_learning.ipynb#ch0000006?line=33'>34</a>\u001b[0m label_prom \u001b[39m=\u001b[39m label_prom\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39;49mfloat32())\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/multitask_learning.ipynb#ch0000006?line=34'>35</a>\u001b[0m loss_prom \u001b[39m=\u001b[39m binary_crossentropy_function(pred_prom, label_prom)\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/multitask_learning.ipynb#ch0000006?line=35'>36</a>\u001b[0m loss_ss \u001b[39m=\u001b[39m crossentropy_function(pred_ss, label_ss)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'torch.dtype' object is not callable"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss, BCELoss\n",
    "from utils.data_generator import _data_generator_mtl\n",
    "from multitask_learning import init_model_mtl\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "\n",
    "dataloader = _data_generator_mtl()\n",
    "model = init_model_mtl(pretrained_3kmer_dir)\n",
    "crossentropy_function = CrossEntropyLoss()\n",
    "binary_crossentropy_function = BCELoss()\n",
    "model\n",
    "model.zero_grad()\n",
    "model.train()\n",
    "count_prom_correct = 0\n",
    "count_ss_correct = 0\n",
    "count_polya_correct = 0\n",
    "# for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "#for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "for step, batch in enumerate(dataloader):\n",
    "    in_ids, attn_mask, label_prom, label_ss, label_polya = tuple(t for t in batch)\n",
    "    bert_output = model.shared_layer(in_ids, attn_mask)\n",
    "    # print(bert_output[0][:,0])\n",
    "    \n",
    "    pred_prom = model.promoter_layer(bert_output)\n",
    "    pred_ss = model.splice_site_layer(bert_output)\n",
    "    pred_polya = model.polya_layer(bert_output)\n",
    "\n",
    "    print(pred_prom, label_prom)\n",
    "    print(pred_ss, label_ss)\n",
    "    print(pred_polya, label_polya)\n",
    "    \n",
    "    label_prom = label_prom.view(1,1)\n",
    "    label_prom = label_prom.to(torch.float32)\n",
    "    loss_prom = binary_crossentropy_function(pred_prom, label_prom)\n",
    "    loss_ss = crossentropy_function(pred_ss, label_ss)\n",
    "    loss_polya = crossentropy_function(pred_polya, label_polya)\n",
    "\n",
    "    print(pred_prom, label_prom, loss_prom)\n",
    "    print(pred_ss, label_ss, loss_ss)\n",
    "    print(pred_polya, label_polya, loss_polya)\n",
    "\n",
    "\n",
    "    #predicted_prom = torch.round(pred_prom).item()\n",
    "    #actual_prom = label_prom.float().item()\n",
    "    #if (predicted_prom == actual_prom):\n",
    "    #    count_prom_correct += 1\n",
    "    #print(pred_prom, label_prom, predicted_prom)\n",
    "\n",
    "    #predicted_ss, predicted_ss_index = torch.max(pred_ss, 1)\n",
    "    #predicted_ss = predicted_ss.item()\n",
    "    #predicted_ss_index = predicted_ss_index.item()\n",
    "    # print(pred_ss, label_ss, predicted_ss, predicted_ss_index)\n",
    "    #if (predicted_ss_index == label_ss):\n",
    "    #    count_ss_correct += 1\n",
    "    #print(sum_loss, sum_loss/3)\n",
    "\n",
    "    #predicted_polya, predicted_polya_index = torch.max(pred_polya, 1)\n",
    "    #predicted_polya = predicted_polya.item()\n",
    "    #predicted_polya_index = predicted_polya_index.item()\n",
    "    #if (predicted_polya_index == label_polya):\n",
    "    #    count_polya_correct += 1\n",
    "\n",
    "print('accuracy prom: {}'.format(count_prom_correct / len(dataloader) * 100))\n",
    "print('accuracy ss: {}'.format(count_ss_correct / len(dataloader) * 100))\n",
    "print('accuracy polya: {}'.format(count_polya_correct / len(dataloader) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4187,  0.8725,  0.9659,  ..., -0.0091,  0.8986, -1.3403],\n",
      "         [-1.4170, -0.9827, -0.7155,  ...,  0.0354,  0.6974, -0.8663],\n",
      "         [-0.6982, -0.7672,  1.1986,  ..., -1.6839,  0.2338, -0.5892],\n",
      "         ...,\n",
      "         [-1.3530, -0.8583, -0.9080,  ...,  0.0355,  0.7268, -0.8151],\n",
      "         [-0.7084, -0.7193,  1.1986,  ..., -1.6446,  0.1983, -0.5487],\n",
      "         [-0.4187,  0.8725,  0.9659,  ..., -0.0091,  0.8986, -1.3403]]])\n",
      "tensor([[[-0.4187,  0.8725,  0.9659,  ..., -0.0091,  0.8986, -1.3403],\n",
      "         [-1.4170, -0.9827, -0.7155,  ...,  0.0354,  0.6974, -0.8663],\n",
      "         [-0.6982, -0.7672,  1.1986,  ..., -1.6839,  0.2338, -0.5892],\n",
      "         ...,\n",
      "         [-1.3530, -0.8583, -0.9080,  ...,  0.0355,  0.7268, -0.8151],\n",
      "         [-0.7084, -0.7193,  1.1986,  ..., -1.6446,  0.1983, -0.5487],\n",
      "         [-0.4187,  0.8725,  0.9659,  ..., -0.0091,  0.8986, -1.3403]]])\n"
     ]
    }
   ],
   "source": [
    "from data_dir import pretrained_3kmer_dir\n",
    "from transformers import BertForMaskedLM\n",
    "from torch import nn\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        input_ids, attn_mask, label_prom, label_ss, label_polya = tuple(t for t in batch)\n",
    "        bert_output = model.bert(input_ids, attention_mask=attn_mask, output_hidden_states=True, output_attentions=True)\n",
    "        hidden_states = bert_output.hidden_states\n",
    "        print(bert_output.last_hidden_state)\n",
    "        print(hidden_states[12])\n",
    "        #dropout = nn.Dropout(p=0.1)(hidden_states[1])\n",
    "        \n",
    "        #print(dropout)\n",
    "        #print(bert_output.last_hidden_state)\n",
    "        #print(bert_output.pooler_output)\n",
    "        #print(bert_output.hidden_states)\n",
    "        #print(len(bert_output[1]))\n",
    "        #print(len(bert_output[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(69, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=69, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "\n",
    "bert_for_masked_lm = BertForMaskedLM.from_pretrained(pretrained_3kmer_dir)\n",
    "print(bert_for_masked_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pretrained\\3-new-12w-0 were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at pretrained\\3-new-12w-0 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(69, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "\n",
    "bert_for_seq_cls = BertForSequenceClassification.from_pretrained(pretrained_3kmer_dir)\n",
    "print(bert_for_seq_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2231)\n",
      "tensor(1.6204)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import BCELoss, CrossEntropyLoss\n",
    "from torch import tensor\n",
    "prom_pred = tensor([[0.2]])\n",
    "prom_target = tensor([[0.]])\n",
    "other_pred = tensor([[0.1, 1.5]])\n",
    "other_target = tensor([0])\n",
    "bce = BCELoss()\n",
    "cross = CrossEntropyLoss()\n",
    "print(bce(prom_pred, prom_target))\n",
    "print(cross(other_pred, other_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datetime\n",
    "import os\n",
    "from data_preparation import kmer\n",
    "from transformers import BertTokenizer\n",
    "from torch \n",
    "from data_dir import pretrained_3kmer_dir\n",
    "\n",
    "_now = datetime.datetime.now()\n",
    "\n",
    "_log_file = os.path.join('logs', 'notebooks', '2022-02.24.csv')\n",
    "os.makedirs(_log_file, exist_ok=True)\n",
    "\n",
    "seqs = [\"ATGC\" * 128, \"GATC\" * 128, \"CCAT\" * 128]\n",
    "seqs = [' '.join(kmer(s, 3)) for s in seqs]\n",
    "prom_labels = [1, 0, 0]\n",
    "ss_labels = [0, 1, 0]\n",
    "polya_labels = [0, 0, 1]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_3kmer_dir)\n",
    "encoded = tokenizer(seqs[0], seqs[1])\n",
    "print(encoded)\n",
    "print(tokenizer.convert_ids_to_tokens(encoded['input_ids']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multitask_learning import init_model_mtl\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "import os\n",
    "\n",
    "model_mtl_bert = init_model_mtl(pretrained_3kmer_dir, head=\"bert\", config=os.path.join('models', 'config', 'mtl.json'))\n",
    "preds = model_mtl_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage 0.1\n",
      "percentile 0.2\n"
     ]
    }
   ],
   "source": [
    "tuples = ('0.1', 0.2)\n",
    "titles = ['percentage', 'percentile']\n",
    "\n",
    "for a, b in zip(titles, tuples):\n",
    "    print(a, b)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7170ee380771f2003055f3cee3e2e4dd0d81d1dac73a8c82197236b1572f37c2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
