{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check if CUDA is supported.\n",
    "\"\"\"\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.device('cuda:0')\n",
    "torch.cuda.get_device_name(0)\n",
    "_device = torch.device('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_preparation import kmer\n",
    "\n",
    "def get_sequences(csv_path, n_sample=10, random_state=1337):\n",
    "    r\"\"\"\n",
    "    Get sequence from certain CSV. CSV has header such as 'sequence', 'label_prom', 'label_ss', 'label_polya'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if (n_sample > 0):\n",
    "        df = df.sample(n=n_sample, random_state=random_state)\n",
    "    sequence = list(df['sequence'])\n",
    "    label_prom = list(df['label_prom'])\n",
    "    label_ss = list(df['label_ss'])\n",
    "    label_polya = list(df['label_polya'])\n",
    "\n",
    "    return sequence, label_prom, label_ss, label_polya\n",
    "\n",
    "import torch\n",
    "def preprocessing(data, tokenizer):\n",
    "    \"\"\"\n",
    "    Preprocessing for pretrained BERT.\n",
    "    @param  data (string): string containing kmers separated by spaces.\n",
    "    @param  tokenizer (Tokenizer): tokenizer initialized from pretrained values.\n",
    "    @return input_ids (torch.Tensor): tensor of token ids to be fed to model.\n",
    "    @return attention_masks (torch.Tensor): tensor of indices (a bunch of 'indexes') specifiying which token needs to be attended by model.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    _count = 0\n",
    "    _len_data = len(data)\n",
    "    for sequence in data:\n",
    "        \"\"\"\n",
    "        Sequence is 512 characters long.\n",
    "        \"\"\"\n",
    "        _count += 1\n",
    "        if _count < _len_data:\n",
    "            print(\"Seq length = {} [{}/{}]\".format(len(sequence.split(' ')), _count, _len_data), end='\\r')\n",
    "        else:\n",
    "            print(\"Seq length = {} [{}/{}]\".format(len(sequence.split(' ')), _count, _len_data))\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sequence,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert input_ids and attention_masks to tensor.\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "\"\"\"\n",
    "Initialize tokenizer using BertTokenizer with pretrained weights from DNABert.\n",
    "\"\"\"\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('./pretrained/3-new-12w-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting and storing split to ./workspace/train.csv\n",
      "Splitting and storing split to ./workspace/validation.csv\n",
      "Splitting source ./dataset/full/train.csv: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Split dataset into two parts: train and validation.\n",
    "\"\"\"\n",
    "from data_dir import workspace_dir, dataset_full_dir\n",
    "from data_preparation import split_and_store_csv\n",
    "_src_csv = \"{}/train.csv\".format(dataset_full_dir)\n",
    "_fractions = [0.9, 0.1]\n",
    "_store_paths = [\n",
    "    \"{}/{}\".format(workspace_dir, 'train.csv'),\n",
    "    \"{}/{}\".format(workspace_dir, 'validation.csv'),\n",
    "]\n",
    "print(\"Splitting source {}: {}\".format(_src_csv, split_and_store_csv(_src_csv, fractions=_fractions, store_paths=_store_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq length = 510 [500000/500000]\n",
      "Seq length = 510 [100/100]\n",
      "# of training data: 500000\n",
      "# of training data: 100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from data_dir import workspace_dir\n",
    "\n",
    "train_seq, train_label_prom, train_label_ss, train_label_polya = get_sequences('{}/train.csv'.format(workspace_dir), n_sample=500000)\n",
    "validation_seq, val_label_prom, val_label_ss, val_label_polya = get_sequences('{}/validation.csv'.format(workspace_dir), n_sample=100)\n",
    "\n",
    "\"\"\"\n",
    "Create dataloader.\n",
    "\"\"\"\n",
    "BATCH_SIZE = 2\n",
    "EPOCH_SIZE = 4\n",
    "\n",
    "_device = torch.device('cuda:0')\n",
    "train_label_prom = torch.tensor(train_label_prom, device=_device)\n",
    "train_label_ss = torch.tensor(train_label_ss, device=_device)\n",
    "train_label_polya = torch.tensor(train_label_polya, device=_device)\n",
    "\n",
    "train_inputs_ids, train_masks = preprocessing(train_seq, tokenizer)\n",
    "train_data = TensorDataset(train_inputs_ids, train_masks, train_label_prom, train_label_ss, train_label_polya)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "val_label_prom = torch.tensor(val_label_prom, device=_device)\n",
    "val_label_ss = torch.tensor(val_label_ss, device=_device)\n",
    "val_label_polya = torch.tensor(val_label_polya, device=_device)\n",
    "\n",
    "val_input_ids, val_masks = preprocessing(validation_seq, tokenizer)\n",
    "val_data = TensorDataset(val_input_ids, val_masks, val_label_prom, val_label_ss, val_label_polya)\n",
    "val_sampler = RandomSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "print('# of training data: {}'.format(len(train_seq)))  \n",
    "print('# of training data: {}'.format(len(validation_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np\n",
    "\n",
    "from multitask_learning import PolyAHead, PromoterHead, SpliceSiteHead, MTModel\n",
    "from transformers import BertForMaskedLM\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "\n",
    "_device = \"cuda\"\n",
    "polya_head = PolyAHead(_device)\n",
    "promoter_head = PromoterHead(_device)\n",
    "splice_head = SpliceSiteHead(_device)\n",
    "\n",
    "dnabert_3_pretrained = pretrained_3kmer_dir\n",
    "shared_parameter = BertForMaskedLM.from_pretrained(dnabert_3_pretrained).bert\n",
    "\n",
    "model = MTModel(shared_parameters=shared_parameter, promoter_head=promoter_head, polya_head=polya_head, splice_site_head=splice_head).to(_device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datetime\n",
    "import os\n",
    "from data_preparation import kmer\n",
    "\n",
    "_now = datetime.datetime.now()\n",
    "\n",
    "_log_file = os.path.join('logs', 'notebooks', '2022-02.24.csv')\n",
    "os.makedirs(_log_file, exist_ok=True)\n",
    "\n",
    "seqs = [\"ATGC\" * 128, \"GATC\" * 128, \"CCAT\" * 128]\n",
    "seqs = [' '.join(kmer(s, 3)) for s in seqs]\n",
    "prom_labels = [1, 0, 0]\n",
    "ss_labels = [0, 1, 0]\n",
    "polya_labels = [0, 0, 1]\n",
    "\n",
    "\"\"\"\n",
    "Initialize BERT tokenizer.\n",
    "\"\"\"\n",
    "from transformers import BertTokenizer\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_3kmer_dir)\n",
    "\n",
    "arr_input_ids = []\n",
    "arr_attention_mask = []\n",
    "for s in seqs:\n",
    "    encoded = tokenizer.encode_plus(text=s, padding=\"max_length\", return_attention_mask=True)\n",
    "    arr_input_ids.append(encoded.get('input_ids'))\n",
    "    arr_attention_mask.append(encoded.get('attention_mask'))\n",
    "#endfor\n",
    "arr_input_ids = torch.tensor(arr_input_ids).to('cuda')\n",
    "arr_attention_mask = torch.tensor(arr_attention_mask).to('cuda')\n",
    "prom_labels = torch.tensor(prom_labels).to('cuda')\n",
    "ss_labels = torch.tensor(ss_labels).to('cuda')\n",
    "polya_labels = torch.tensor(polya_labels).to('cuda')\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "dataset = TensorDataset(arr_input_ids, arr_attention_mask, prom_labels, ss_labels, polya_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\IPython\\core\\formatters.py:707\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/core/formatters.py?line=699'>700</a>\u001b[0m stream \u001b[39m=\u001b[39m StringIO()\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/core/formatters.py?line=700'>701</a>\u001b[0m printer \u001b[39m=\u001b[39m pretty\u001b[39m.\u001b[39mRepresentationPrinter(stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/core/formatters.py?line=701'>702</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnewline,\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/core/formatters.py?line=702'>703</a>\u001b[0m     max_seq_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_seq_length,\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/core/formatters.py?line=703'>704</a>\u001b[0m     singleton_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msingleton_printers,\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/core/formatters.py?line=704'>705</a>\u001b[0m     type_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype_printers,\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/core/formatters.py?line=705'>706</a>\u001b[0m     deferred_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeferred_printers)\n\u001b[1;32m--> <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/core/formatters.py?line=706'>707</a>\u001b[0m printer\u001b[39m.\u001b[39;49mpretty(obj)\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/core/formatters.py?line=707'>708</a>\u001b[0m printer\u001b[39m.\u001b[39mflush()\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/core/formatters.py?line=708'>709</a>\u001b[0m \u001b[39mreturn\u001b[39;00m stream\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\IPython\\lib\\pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/lib/pretty.py?line=406'>407</a>\u001b[0m                         \u001b[39mreturn\u001b[39;00m meth(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/lib/pretty.py?line=407'>408</a>\u001b[0m                 \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mobject\u001b[39m \\\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/lib/pretty.py?line=408'>409</a>\u001b[0m                         \u001b[39mand\u001b[39;00m callable(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39m__repr__\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[1;32m--> <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/lib/pretty.py?line=409'>410</a>\u001b[0m                     \u001b[39mreturn\u001b[39;00m _repr_pprint(obj, \u001b[39mself\u001b[39;49m, cycle)\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/lib/pretty.py?line=411'>412</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_pprint(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/lib/pretty.py?line=412'>413</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\IPython\\lib\\pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[1;34m(obj, p, cycle)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/lib/pretty.py?line=775'>776</a>\u001b[0m \u001b[39m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/lib/pretty.py?line=776'>777</a>\u001b[0m \u001b[39m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/lib/pretty.py?line=777'>778</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mrepr\u001b[39;49m(obj)\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/lib/pretty.py?line=778'>779</a>\u001b[0m lines \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msplitlines()\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/IPython/lib/pretty.py?line=779'>780</a>\u001b[0m \u001b[39mwith\u001b[39;00m p\u001b[39m.\u001b[39mgroup():\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\torch\\_tensor.py:249\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor.py?line=246'>247</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m\u001b[39m__repr__\u001b[39m, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor.py?line=247'>248</a>\u001b[0m \u001b[39m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor.py?line=248'>249</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_tensor_str\u001b[39m.\u001b[39;49m_str(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\torch\\_tensor_str.py:415\u001b[0m, in \u001b[0;36m_str\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=412'>413</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_str\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=413'>414</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=414'>415</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m _str_intern(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\torch\\_tensor_str.py:390\u001b[0m, in \u001b[0;36m_str_intern\u001b[1;34m(inp)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=387'>388</a>\u001b[0m                 tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_dense(), indent)\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=388'>389</a>\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=389'>390</a>\u001b[0m                 tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39;49m, indent)\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=391'>392</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mstrided:\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=392'>393</a>\u001b[0m     suffixes\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39mlayout=\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout))\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\torch\\_tensor_str.py:251\u001b[0m, in \u001b[0;36m_tensor_str\u001b[1;34m(self, indent)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=248'>249</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[39mself\u001b[39m, indent, summarize, real_formatter, imag_formatter)\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=249'>250</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=250'>251</a>\u001b[0m     formatter \u001b[39m=\u001b[39m _Formatter(get_summarized_data(\u001b[39mself\u001b[39;49m) \u001b[39mif\u001b[39;00m summarize \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=251'>252</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[39mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\torch\\_tensor_str.py:285\u001b[0m, in \u001b[0;36mget_summarized_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=282'>283</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack([get_summarized_data(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m (start \u001b[39m+\u001b[39m end)])\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=283'>284</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=284'>285</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack([get_summarized_data(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m])\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\torch\\_tensor_str.py:285\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=282'>283</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack([get_summarized_data(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m (start \u001b[39m+\u001b[39m end)])\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=283'>284</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=284'>285</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack([get_summarized_data(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m])\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\torch\\_tensor_str.py:276\u001b[0m, in \u001b[0;36mget_summarized_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=273'>274</a>\u001b[0m \u001b[39mif\u001b[39;00m dim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=274'>275</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m PRINT_OPTS\u001b[39m.\u001b[39medgeitems:\n\u001b[1;32m--> <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=275'>276</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mcat((\u001b[39mself\u001b[39;49m[:PRINT_OPTS\u001b[39m.\u001b[39;49medgeitems], \u001b[39mself\u001b[39;49m[\u001b[39m-\u001b[39;49mPRINT_OPTS\u001b[39m.\u001b[39;49medgeitems:]))\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=276'>277</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/_tensor_str.py?line=277'>278</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7170ee380771f2003055f3cee3e2e4dd0d81d1dac73a8c82197236b1572f37c2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
