{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check if CUDA is supported.\n",
    "\"\"\"\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.device('cuda:0')\n",
    "torch.cuda.get_device_name(0)\n",
    "_device = torch.device('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_preparation import kmer\n",
    "\n",
    "def get_sequences(csv_path, n_sample=10, random_state=1337):\n",
    "    r\"\"\"\n",
    "    Get sequence from certain CSV. CSV has header such as 'sequence', 'label_prom', 'label_ss', 'label_polya'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if (n_sample > 0):\n",
    "        df = df.sample(n=n_sample, random_state=random_state)\n",
    "    sequence = list(df['sequence'])\n",
    "    label_prom = list(df['label_prom'])\n",
    "    label_ss = list(df['label_ss'])\n",
    "    label_polya = list(df['label_polya'])\n",
    "\n",
    "    return sequence, label_prom, label_ss, label_polya\n",
    "\n",
    "import torch\n",
    "def preprocessing(data, tokenizer):\n",
    "    \"\"\"\n",
    "    Preprocessing for pretrained BERT.\n",
    "    @param  data (string): string containing kmers separated by spaces.\n",
    "    @param  tokenizer (Tokenizer): tokenizer initialized from pretrained values.\n",
    "    @return input_ids (torch.Tensor): tensor of token ids to be fed to model.\n",
    "    @return attention_masks (torch.Tensor): tensor of indices (a bunch of 'indexes') specifiying which token needs to be attended by model.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    _count = 0\n",
    "    _len_data = len(data)\n",
    "    for sequence in data:\n",
    "        \"\"\"\n",
    "        Sequence is 512 characters long.\n",
    "        \"\"\"\n",
    "        _count += 1\n",
    "        if _count < _len_data:\n",
    "            print(\"Seq length = {} [{}/{}]\".format(len(sequence.split(' ')), _count, _len_data), end='\\r')\n",
    "        else:\n",
    "            print(\"Seq length = {} [{}/{}]\".format(len(sequence.split(' ')), _count, _len_data))\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sequence,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert input_ids and attention_masks to tensor.\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "\"\"\"\n",
    "Initialize tokenizer using BertTokenizer with pretrained weights from DNABert.\n",
    "\"\"\"\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('./pretrained/3-new-12w-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([0.1, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split dataset into two parts: train and validation.\n",
    "\"\"\"\n",
    "from data_dir import workspace_dir, dataset_full_dir\n",
    "from data_preparation import split_and_store_csv\n",
    "_src_csv = \"{}/train.csv\".format(dataset_full_dir)\n",
    "_fractions = [0.99, 0.01]\n",
    "_store_paths = [\n",
    "    \"{}/{}\".format(workspace_dir, 'train.csv'),\n",
    "    \"{}/{}\".format(workspace_dir, 'validation.csv'),\n",
    "]\n",
    "print(\"Splitting source {}: {}\".format(_src_csv, split_and_store_csv(_src_csv, fractions=_fractions, store_paths=_store_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq length = 510 [100/100]\n",
      "# of training data: 100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from data_dir import workspace_dir\n",
    "\n",
    "train_seq, train_label_prom, train_label_ss, train_label_polya = get_sequences('{}/train.csv'.format(workspace_dir), n_sample=10000)\n",
    "validation_seq, val_label_prom, val_label_ss, val_label_polya = get_sequences('{}/validation.csv'.format(workspace_dir), n_sample=100)\n",
    "\n",
    "\"\"\"\n",
    "Create dataloader.\n",
    "\"\"\"\n",
    "BATCH_SIZE = 1\n",
    "EPOCH_SIZE = 4\n",
    "\n",
    "_device = torch.device('cuda:0')\n",
    "train_label_prom = torch.tensor(train_label_prom, device=_device)\n",
    "train_label_ss = torch.tensor(train_label_ss, device=_device)\n",
    "train_label_polya = torch.tensor(train_label_polya, device=_device)\n",
    "\n",
    "train_inputs_ids, train_masks = preprocessing(train_seq, tokenizer)\n",
    "train_data = TensorDataset(train_inputs_ids, train_masks, train_label_prom, train_label_ss, train_label_polya)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "val_label_prom = torch.tensor(val_label_prom, device=_device)\n",
    "val_label_ss = torch.tensor(val_label_ss, device=_device)\n",
    "val_label_polya = torch.tensor(val_label_polya, device=_device)\n",
    "\n",
    "val_input_ids, val_masks = preprocessing(validation_seq, tokenizer)\n",
    "val_data = TensorDataset(val_input_ids, val_masks, val_label_prom, val_label_ss, val_label_polya)\n",
    "val_sampler = RandomSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "print('# of training data: {}'.format(len(train_seq)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np\n",
    "\n",
    "from multitask_learning import PolyAHead, PromoterHead, SpliceSiteHead, MTModel\n",
    "from transformers import BertForMaskedLM\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "\n",
    "polya_head = PolyAHead(_device)\n",
    "promoter_head = PromoterHead(_device)\n",
    "splice_head = SpliceSiteHead(_device)\n",
    "\n",
    "dnabert_3_pretrained = pretrained_3kmer_dir\n",
    "shared_parameter = BertForMaskedLM.from_pretrained(dnabert_3_pretrained).bert\n",
    "\n",
    "model = MTModel(shared_parameters=shared_parameter, promoter_head=promoter_head, polya_head=polya_head, splice_site_head=splice_head).to(_device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
    "training_steps = len(train_dataloader) * EPOCH_SIZE\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=training_steps)\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./logs/22-02-13/22-02-13-18-11-30.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "_now = datetime.datetime.now()\n",
    "\n",
    "_log_folder = \"./logs/{}\".format(_now.strftime(\"%y-%m-%d\"))\n",
    "if not os.path.exists(_log_folder):\n",
    "    os.mkdir(_log_folder)\n",
    "_log_name = \"{}.csv\".format(_now.strftime(\"%y-%m-%d-%H-%M-%S\"))\n",
    "_log_path = \"{}/{}\".format(_log_folder, _log_name)\n",
    "print(_log_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 99\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multitask_learning import train\n",
    "train(train_dataloader, model, loss_fn, optimizer, scheduler, BATCH_SIZE, EPOCH_SIZE, _log_path, _device, eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, './result/24012022.pth')\n",
    "import datetime\n",
    "_now = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "model.shared_layer.save_pretrained('./result/gpu/{}/'.format(_now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch loss 4.678438186645508, batch loss prom: 1.5165265798568726, batch loss ss: 1.4876850843429565, batch loss polya: 1.6742267608642578\n",
      "batch loss 1.7870014905929565, batch loss prom: 0.5639010667800903, batch loss ss: 0.6062026023864746, batch loss polya: 0.6168978214263916\n",
      "batch loss 1.5076560974121094, batch loss prom: 0.5132335424423218, batch loss ss: 0.4427833557128906, batch loss polya: 0.5516392588615417\n",
      "batch loss 1.681549310684204, batch loss prom: 0.49047523736953735, batch loss ss: 0.3959461450576782, batch loss polya: 0.7951279878616333\n",
      "batch loss 1.3229281902313232, batch loss prom: 0.4208376705646515, batch loss ss: 0.3389909863471985, batch loss polya: 0.5630995631217957\n",
      "batch loss 1.431191325187683, batch loss prom: 0.374896377325058, batch loss ss: 0.3177795112133026, batch loss polya: 0.7385154366493225\n",
      "batch loss 1.2189515829086304, batch loss prom: 0.30863240361213684, batch loss ss: 0.2716456651687622, batch loss polya: 0.6386735439300537\n",
      "batch loss 1.2208359241485596, batch loss prom: 0.30638885498046875, batch loss ss: 0.27975451946258545, batch loss polya: 0.6346924901008606\n",
      "batch loss 1.228649377822876, batch loss prom: 0.24383538961410522, batch loss ss: 0.26796218752861023, batch loss polya: 0.7168517708778381\n",
      "batch loss 1.1668400764465332, batch loss prom: 0.25864312052726746, batch loss ss: 0.1827060729265213, batch loss polya: 0.7254908084869385\n",
      "batch loss 1.3163206577301025, batch loss prom: 0.2552391588687897, batch loss ss: 0.26725468039512634, batch loss polya: 0.7938268184661865\n",
      "batch loss 0.9721713662147522, batch loss prom: 0.1677539348602295, batch loss ss: 0.10870664566755295, batch loss polya: 0.6957107782363892\n",
      "batch loss 0.8577258586883545, batch loss prom: 0.1439155489206314, batch loss ss: 0.09815406054258347, batch loss polya: 0.6156562566757202\n",
      "batch loss 0.9956932067871094, batch loss prom: 0.14006446301937103, batch loss ss: 0.07579623907804489, batch loss polya: 0.7798324823379517\n",
      "batch loss 1.4457955360412598, batch loss prom: 0.3624878525733948, batch loss ss: 0.28810304403305054, batch loss polya: 0.7952046394348145\n",
      "batch loss 0.7574858069419861, batch loss prom: 0.11141642928123474, batch loss ss: 0.09345456212759018, batch loss polya: 0.5526148080825806\n",
      "batch loss 0.6988702416419983, batch loss prom: 0.09661740064620972, batch loss ss: 0.08698343485593796, batch loss polya: 0.51526939868927\n",
      "batch loss 0.7317478060722351, batch loss prom: 0.08318975567817688, batch loss ss: 0.05786087363958359, batch loss polya: 0.590697169303894\n",
      "batch loss 0.7419110536575317, batch loss prom: 0.09632307291030884, batch loss ss: 0.10209937393665314, batch loss polya: 0.543488621711731\n",
      "batch loss 1.3621612787246704, batch loss prom: 0.3252505660057068, batch loss ss: 0.23561985790729523, batch loss polya: 0.8012908697128296\n",
      "batch loss 1.0089173316955566, batch loss prom: 0.07291258126497269, batch loss ss: 0.05053416267037392, batch loss polya: 0.8854706287384033\n",
      "batch loss 0.7697094678878784, batch loss prom: 0.1444453001022339, batch loss ss: 0.09693792462348938, batch loss polya: 0.5283262729644775\n",
      "batch loss 0.62181556224823, batch loss prom: 0.06978804618120193, batch loss ss: 0.047799624502658844, batch loss polya: 0.504227876663208\n",
      "batch loss 0.5445312857627869, batch loss prom: 0.06951741129159927, batch loss ss: 0.06244145706295967, batch loss polya: 0.4125724136829376\n",
      "batch loss 1.2556720972061157, batch loss prom: 0.07391561567783356, batch loss ss: 0.06865165382623672, batch loss polya: 1.1131048202514648\n",
      "batch loss 1.200490117073059, batch loss prom: 0.0710587278008461, batch loss ss: 0.04409566894173622, batch loss polya: 1.0853357315063477\n",
      "batch loss 0.5527443885803223, batch loss prom: 0.06316187977790833, batch loss ss: 0.03648475185036659, batch loss polya: 0.45309773087501526\n",
      "batch loss 1.0202778577804565, batch loss prom: 0.049422863870859146, batch loss ss: 0.022788196802139282, batch loss polya: 0.948066771030426\n",
      "batch loss 0.9410126209259033, batch loss prom: 0.05499830096960068, batch loss ss: 0.029824819415807724, batch loss polya: 0.856189489364624\n",
      "batch loss 0.558272123336792, batch loss prom: 0.046979110687971115, batch loss ss: 0.036210816353559494, batch loss polya: 0.4750822186470032\n",
      "batch loss 0.5131669640541077, batch loss prom: 0.03692935034632683, batch loss ss: 0.023682812228798866, batch loss polya: 0.4525548219680786\n",
      "batch loss 0.989625871181488, batch loss prom: 0.039049871265888214, batch loss ss: 0.018518179655075073, batch loss polya: 0.932057797908783\n",
      "batch loss 0.9726393222808838, batch loss prom: 0.14797048270702362, batch loss ss: 0.06710594892501831, batch loss polya: 0.7575628757476807\n",
      "batch loss 0.8702784776687622, batch loss prom: 0.03734687715768814, batch loss ss: 0.015714265406131744, batch loss polya: 0.8172173500061035\n",
      "batch loss 0.5685980916023254, batch loss prom: 0.041280072182416916, batch loss ss: 0.015546441078186035, batch loss polya: 0.511771559715271\n",
      "batch loss 0.6541244387626648, batch loss prom: 0.029189040884375572, batch loss ss: 0.01517231110483408, batch loss polya: 0.6097630858421326\n",
      "batch loss 0.6727677583694458, batch loss prom: 0.030196283012628555, batch loss ss: 0.01567894220352173, batch loss polya: 0.6268925070762634\n",
      "batch loss 0.6201951503753662, batch loss prom: 0.025639185681939125, batch loss ss: 0.01335261482745409, batch loss polya: 0.5812033414840698\n",
      "batch loss 0.6404523253440857, batch loss prom: 0.029466643929481506, batch loss ss: 0.013645701110363007, batch loss polya: 0.5973399877548218\n",
      "batch loss 0.9552599787712097, batch loss prom: 0.029851777479052544, batch loss ss: 0.012648127041757107, batch loss polya: 0.9127600789070129\n",
      "batch loss 0.8384414315223694, batch loss prom: 0.025665096938610077, batch loss ss: 0.01631058193743229, batch loss polya: 0.7964657545089722\n",
      "batch loss 0.7736086845397949, batch loss prom: 0.07433547079563141, batch loss ss: 0.029503103345632553, batch loss polya: 0.6697701215744019\n",
      "batch loss 0.8188275694847107, batch loss prom: 0.028034048154950142, batch loss ss: 0.010741510428488255, batch loss polya: 0.7800520062446594\n",
      "batch loss 0.5982924103736877, batch loss prom: 0.02312069945037365, batch loss ss: 0.012053391896188259, batch loss polya: 0.5631183385848999\n",
      "batch loss 0.5860999226570129, batch loss prom: 0.021818017587065697, batch loss ss: 0.011663931421935558, batch loss polya: 0.5526179671287537\n",
      "batch loss 0.7403062582015991, batch loss prom: 0.0691853016614914, batch loss ss: 0.028523432090878487, batch loss polya: 0.6425975561141968\n",
      "batch loss 0.4900141954421997, batch loss prom: 0.016216283664107323, batch loss ss: 0.009828626178205013, batch loss polya: 0.46396929025650024\n",
      "batch loss 0.5063410401344299, batch loss prom: 0.01567823812365532, batch loss ss: 0.012286923825740814, batch loss polya: 0.4783758819103241\n",
      "batch loss 0.5948227643966675, batch loss prom: 0.016353974118828773, batch loss ss: 0.009421057067811489, batch loss polya: 0.569047749042511\n",
      "batch loss 0.4955361485481262, batch loss prom: 0.016933148726820946, batch loss ss: 0.0116850221529603, batch loss polya: 0.4669179916381836\n",
      "batch loss 1.0123673677444458, batch loss prom: 0.01563822105526924, batch loss ss: 0.00990771222859621, batch loss polya: 0.9868214130401611\n",
      "batch loss 0.9423045516014099, batch loss prom: 0.017961585894227028, batch loss ss: 0.009417750872671604, batch loss polya: 0.914925217628479\n",
      "batch loss 1.091120719909668, batch loss prom: 0.017920956015586853, batch loss ss: 0.015666738152503967, batch loss polya: 1.0575330257415771\n",
      "batch loss 1.0323079824447632, batch loss prom: 0.017091717571020126, batch loss ss: 0.009080187417566776, batch loss polya: 1.0061360597610474\n",
      "batch loss 0.684209942817688, batch loss prom: 0.03164230287075043, batch loss ss: 0.020707137882709503, batch loss polya: 0.6318604946136475\n",
      "batch loss 1.0046863555908203, batch loss prom: 0.016448259353637695, batch loss ss: 0.009662170894443989, batch loss polya: 0.9785758852958679\n",
      "batch loss 0.7924784421920776, batch loss prom: 0.01482129842042923, batch loss ss: 0.008392896503210068, batch loss polya: 0.7692642211914062\n",
      "batch loss 0.909976065158844, batch loss prom: 0.012740643694996834, batch loss ss: 0.009351381100714207, batch loss polya: 0.8878840208053589\n",
      "batch loss 0.7836781144142151, batch loss prom: 0.017960531637072563, batch loss ss: 0.0068825320340693, batch loss polya: 0.7588350772857666\n",
      "batch loss 0.6651853919029236, batch loss prom: 0.014429625123739243, batch loss ss: 0.007765343878418207, batch loss polya: 0.6429904103279114\n",
      "batch loss 0.5785343647003174, batch loss prom: 0.01665743812918663, batch loss ss: 0.0072281756438314915, batch loss polya: 0.554648756980896\n",
      "batch loss 0.6494671702384949, batch loss prom: 0.016341308131814003, batch loss ss: 0.006687284912914038, batch loss polya: 0.626438558101654\n",
      "batch loss 0.4844070374965668, batch loss prom: 0.04921770468354225, batch loss ss: 0.01974218338727951, batch loss polya: 0.4154471457004547\n",
      "batch loss 0.43768954277038574, batch loss prom: 0.019360968843102455, batch loss ss: 0.0072508989833295345, batch loss polya: 0.41107767820358276\n",
      "batch loss 1.0018788576126099, batch loss prom: 0.01662566512823105, batch loss ss: 0.007497860584408045, batch loss polya: 0.9777553081512451\n",
      "batch loss 0.47521328926086426, batch loss prom: 0.044250331819057465, batch loss ss: 0.021106282249093056, batch loss polya: 0.4098566770553589\n",
      "batch loss 0.5331757664680481, batch loss prom: 0.014754699543118477, batch loss ss: 0.0066659701988101006, batch loss polya: 0.511755108833313\n",
      "batch loss 1.1735810041427612, batch loss prom: 0.016833048313856125, batch loss ss: 0.006416552234441042, batch loss polya: 1.1503313779830933\n",
      "batch loss 0.9479042887687683, batch loss prom: 0.017311424016952515, batch loss ss: 0.008545025251805782, batch loss polya: 0.9220478534698486\n",
      "batch loss 0.9203179478645325, batch loss prom: 0.07598850131034851, batch loss ss: 0.09599444270133972, batch loss polya: 0.7483350038528442\n",
      "batch loss 0.4360625743865967, batch loss prom: 0.0239071287214756, batch loss ss: 0.008512639440596104, batch loss polya: 0.40364280343055725\n",
      "batch loss 0.4162236452102661, batch loss prom: 0.04069400206208229, batch loss ss: 0.016209598630666733, batch loss polya: 0.3593200445175171\n",
      "batch loss 0.41229334473609924, batch loss prom: 0.06893019378185272, batch loss ss: 0.023918533697724342, batch loss polya: 0.3194446265697479\n",
      "batch loss 1.112992763519287, batch loss prom: 0.015376121737062931, batch loss ss: 0.005641609895974398, batch loss polya: 1.0919749736785889\n",
      "batch loss 1.2768504619598389, batch loss prom: 0.01639818586409092, batch loss ss: 0.00848474446684122, batch loss polya: 1.2519675493240356\n",
      "batch loss 0.46662288904190063, batch loss prom: 0.017812639474868774, batch loss ss: 0.011341976933181286, batch loss polya: 0.4374682605266571\n",
      "batch loss 0.38417157530784607, batch loss prom: 0.017864398658275604, batch loss ss: 0.008566655218601227, batch loss polya: 0.35774052143096924\n",
      "batch loss 0.3944743871688843, batch loss prom: 0.036977946758270264, batch loss ss: 0.0226039569824934, batch loss polya: 0.33489248156547546\n",
      "batch loss 0.39003950357437134, batch loss prom: 0.014265227131545544, batch loss ss: 0.0063755689188838005, batch loss polya: 0.36939871311187744\n",
      "batch loss 0.9970376491546631, batch loss prom: 0.01315333228558302, batch loss ss: 0.0058147800154984, batch loss polya: 0.978069543838501\n",
      "batch loss 1.1028733253479004, batch loss prom: 0.013894380070269108, batch loss ss: 0.006634115241467953, batch loss polya: 1.082344889640808\n",
      "batch loss 1.1491950750350952, batch loss prom: 0.012811142951250076, batch loss ss: 0.0062958477064967155, batch loss polya: 1.1300880908966064\n",
      "batch loss 0.9124151468276978, batch loss prom: 0.010818755254149437, batch loss ss: 0.008536042645573616, batch loss polya: 0.8930603265762329\n",
      "batch loss 1.0640082359313965, batch loss prom: 0.01157284714281559, batch loss ss: 0.005931987427175045, batch loss polya: 1.0465034246444702\n",
      "batch loss 0.7409303188323975, batch loss prom: 0.011310035362839699, batch loss ss: 0.005252490285784006, batch loss polya: 0.7243677973747253\n",
      "batch loss 0.5433188080787659, batch loss prom: 0.015735389664769173, batch loss ss: 0.006873534061014652, batch loss polya: 0.5207098722457886\n",
      "batch loss 0.5993797183036804, batch loss prom: 0.014582600444555283, batch loss ss: 0.010272135026752949, batch loss polya: 0.574524998664856\n",
      "batch loss 0.7760906219482422, batch loss prom: 0.012510865926742554, batch loss ss: 0.006976648699492216, batch loss polya: 0.7566031217575073\n",
      "batch loss 0.7182080745697021, batch loss prom: 0.010689027607440948, batch loss ss: 0.005467817187309265, batch loss polya: 0.7020512223243713\n",
      "batch loss 0.5260308980941772, batch loss prom: 0.02942393161356449, batch loss ss: 0.013873453252017498, batch loss polya: 0.48273348808288574\n",
      "batch loss 0.5981572866439819, batch loss prom: 0.010539112612605095, batch loss ss: 0.006867140997201204, batch loss polya: 0.5807510614395142\n",
      "batch loss 0.6903643012046814, batch loss prom: 0.012957070954144001, batch loss ss: 0.005895605776458979, batch loss polya: 0.6715116500854492\n",
      "batch loss 0.6398907899856567, batch loss prom: 0.010654942132532597, batch loss ss: 0.0061493003740906715, batch loss polya: 0.6230865716934204\n",
      "batch loss 0.6220463514328003, batch loss prom: 0.008969609625637531, batch loss ss: 0.004586533643305302, batch loss polya: 0.6084902286529541\n",
      "batch loss 0.6636485457420349, batch loss prom: 0.009301777929067612, batch loss ss: 0.004534201696515083, batch loss polya: 0.6498125791549683\n",
      "batch loss 0.8206648826599121, batch loss prom: 0.010513750836253166, batch loss ss: 0.00524667976424098, batch loss polya: 0.8049044609069824\n",
      "batch loss 0.5929375886917114, batch loss prom: 0.01044851541519165, batch loss ss: 0.006386584602296352, batch loss polya: 0.5761024951934814\n",
      "batch loss 0.6600082516670227, batch loss prom: 0.02000255696475506, batch loss ss: 0.007159530185163021, batch loss polya: 0.6328461766242981\n",
      "batch loss 0.6260239481925964, batch loss prom: 0.00941928569227457, batch loss ss: 0.005769149400293827, batch loss polya: 0.6108354926109314\n",
      "-----\n",
      "prom acc: 100.0, prom loss: 0.010474232956767082\n",
      "ss acc: 50.0, ss loss: 2.6453142166137695\n",
      "polya acc: 90.0, polya loss: 0.6650317311286926\n",
      "-----\n",
      "Epoch 1, Batch 1\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m _device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[0;32m      4\u001b[0m _now \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer, scheduler, batch_size, epoch_size, device, eval, val_dataloader)\u001b[0m\n\u001b[0;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Perform forward pass.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_attn_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Compute error.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m loss_prom \u001b[38;5;241m=\u001b[39m loss_fn(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprom\u001b[39m\u001b[38;5;124m'\u001b[39m], b_labels_prom)\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mw:\\Research\\_sequence-processing\\multitask_learning.py:86\u001b[0m, in \u001b[0;36mMTModel.forward\u001b[1;34m(self, input_ids, attention_masks)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_masks):\n\u001b[1;32m---> 86\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     x \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[0;32m     88\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpromoter_layer(x)\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:989\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    987\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 989\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    990\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    995\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    996\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    997\u001b[0m     embedding_output,\n\u001b[0;32m    998\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1006\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1007\u001b[0m )\n\u001b[0;32m   1008\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:214\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    211\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 214\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    217\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\torch\\nn\\functional.py:2044\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2038\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2039\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2040\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2041\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2042\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2043\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "_device = torch.device('cuda:0')\n",
    "train(train_dataloader, model, loss_fn, optimizer, scheduler, BATCH_SIZE, EPOCH_SIZE, _device, eval=True, val_dataloader=val_dataloader)\n",
    "import datetime\n",
    "_now = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "model.shared_layer.save_pretrained('./result/gpu/{}/'.format(_now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prom': tensor([[ 1.6117, -2.9922]], grad_fn=<AddmmBackward0>), 'ss': tensor([[ 2.3533, -1.9821]], grad_fn=<AddmmBackward0>), 'polya': tensor([[-0.2783,  1.3930]], grad_fn=<AddmmBackward0>)}\n",
      "prom pred tensor([[ 1.6117, -2.9922]], grad_fn=<AddmmBackward0>); label tensor([0])\n",
      "ss pred tensor([[ 2.3533, -1.9821]], grad_fn=<AddmmBackward0>); label tensor([1])\n",
      "polya pred tensor([[-0.2783,  1.3930]], grad_fn=<AddmmBackward0>); label tensor([0])\n",
      "loss prom 0.009962832555174828, ss 4.348450660705566, polya 1.8435773849487305\n"
     ]
    }
   ],
   "source": [
    "s = \"GTACGATCGACTAGACACTATATATA\"\n",
    "prom = 0\n",
    "ss = 0\n",
    "polya = 0\n",
    "\n",
    "kmer = create_kmer(s, 3)\n",
    "tokenizer = BertTokenizer.from_pretrained('./pretrained/3-new-12w-0')\n",
    "output = tokenizer.encode_plus(text=kmer, padding='max_length', return_attention_mask=True)\n",
    "ids = []\n",
    "attns = []\n",
    "prom_labels = []\n",
    "ss_labels = []\n",
    "polya_labels = []\n",
    "ids.append(output['input_ids'])\n",
    "attns.append(output['attention_mask'])\n",
    "prom_labels.append(0)\n",
    "ss_labels.append(1)\n",
    "polya_labels.append(0)\n",
    "\n",
    "input_ids = torch.tensor(ids)\n",
    "attention_masks = torch.tensor(ids)\n",
    "prom_labels = torch.tensor(prom_labels)\n",
    "ss_labels = torch.tensor(ss_labels)\n",
    "polya_labels = torch.tensor(polya_labels)\n",
    "\n",
    "\n",
    "outputs = model(input_ids, attention_masks)\n",
    "prom = outputs['prom']\n",
    "ss = outputs['ss']\n",
    "polya = outputs['polya']\n",
    "print(outputs)\n",
    "print('prom pred {}; label {}'.format(prom, prom_labels))\n",
    "print('ss pred {}; label {}'.format(ss, ss_labels))\n",
    "print('polya pred {}; label {}'.format(polya, polya_labels))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss_prom = loss_fn(prom, prom_labels)\n",
    "loss_ss = loss_fn(ss, ss_labels)\n",
    "loss_polya = loss_fn(polya, polya_labels)\n",
    "print('loss prom {}, ss {}, polya {}'.format(loss_prom, loss_ss, loss_polya))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2962,  0.0700,  0.0424,  ...,  0.3942,  0.8422,  0.2371],\n",
       "         [-0.2962,  0.0700,  0.0423,  ...,  0.3942,  0.8423,  0.2370],\n",
       "         [-0.2963,  0.0697,  0.0428,  ...,  0.3942,  0.8423,  0.2371],\n",
       "         ...,\n",
       "         [-0.2962,  0.0700,  0.0428,  ...,  0.3942,  0.8423,  0.2371],\n",
       "         [-0.2962,  0.0700,  0.0424,  ...,  0.3943,  0.8423,  0.2370],\n",
       "         [-0.2962,  0.0700,  0.0424,  ...,  0.3942,  0.8422,  0.2371]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = model.shared_layer\n",
    "Y = bert(input_ids=input_ids, attention_mask=attention_masks)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "print(len(Y[0][0]))\n",
    "print(len(Y[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./pretrained/3-new-12w-0 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "from dnabert import initialize_training_model, initialize_sequence_labelling_model\n",
    "\n",
    "pretrained_path = './pretrained/3-new-12w-0'\n",
    "mtl_model = initialize_training_model(pretrained_path)\n",
    "dnaseq_model = initialize_sequence_labelling_model(pretrained_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNASeqLabelling(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(69, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (stack): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnaseq_model"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7170ee380771f2003055f3cee3e2e4dd0d81d1dac73a8c82197236b1572f37c2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('sequence-processing': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
