{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check if CUDA is supported.\n",
    "\"\"\"\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.device('cuda:0')\n",
    "torch.cuda.get_device_name(0)\n",
    "_device = torch.device('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_preparation import kmer\n",
    "\n",
    "def get_sequences(csv_path, n_sample=10, random_state=1337):\n",
    "    r\"\"\"\n",
    "    Get sequence from certain CSV. CSV has header such as 'sequence', 'label_prom', 'label_ss', 'label_polya'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if (n_sample > 0):\n",
    "        df = df.sample(n=n_sample, random_state=random_state)\n",
    "    sequence = list(df['sequence'])\n",
    "    label_prom = list(df['label_prom'])\n",
    "    label_ss = list(df['label_ss'])\n",
    "    label_polya = list(df['label_polya'])\n",
    "\n",
    "    return sequence, label_prom, label_ss, label_polya\n",
    "\n",
    "import torch\n",
    "def preprocessing(data, tokenizer):\n",
    "    \"\"\"\n",
    "    Preprocessing for pretrained BERT.\n",
    "    @param  data (string): string containing kmers separated by spaces.\n",
    "    @param  tokenizer (Tokenizer): tokenizer initialized from pretrained values.\n",
    "    @return input_ids (torch.Tensor): tensor of token ids to be fed to model.\n",
    "    @return attention_masks (torch.Tensor): tensor of indices (a bunch of 'indexes') specifiying which token needs to be attended by model.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    _count = 0\n",
    "    _len_data = len(data)\n",
    "    for sequence in data:\n",
    "        \"\"\"\n",
    "        Sequence is 512 characters long.\n",
    "        \"\"\"\n",
    "        _count += 1\n",
    "        if _count < _len_data:\n",
    "            print(\"Seq length = {} [{}/{}]\".format(len(sequence.split(' ')), _count, _len_data), end='\\r')\n",
    "        else:\n",
    "            print(\"Seq length = {} [{}/{}]\".format(len(sequence.split(' ')), _count, _len_data))\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sequence,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert input_ids and attention_masks to tensor.\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "\"\"\"\n",
    "Initialize tokenizer using BertTokenizer with pretrained weights from DNABert.\n",
    "\"\"\"\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('./pretrained/3-new-12w-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting and storing split to ./workspace/train.csv\n",
      "Splitting and storing split to ./workspace/validation.csv\n",
      "Splitting source ./dataset/full/train.csv: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Split dataset into two parts: train and validation.\n",
    "\"\"\"\n",
    "from data_dir import workspace_dir, dataset_full_dir\n",
    "from data_preparation import split_and_store_csv\n",
    "_src_csv = \"{}/train.csv\".format(dataset_full_dir)\n",
    "_fractions = [0.9, 0.1]\n",
    "_store_paths = [\n",
    "    \"{}/{}\".format(workspace_dir, 'train.csv'),\n",
    "    \"{}/{}\".format(workspace_dir, 'validation.csv'),\n",
    "]\n",
    "print(\"Splitting source {}: {}\".format(_src_csv, split_and_store_csv(_src_csv, fractions=_fractions, store_paths=_store_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq length = 510 [500000/500000]\n",
      "Seq length = 510 [100/100]\n",
      "# of training data: 500000\n",
      "# of training data: 100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from data_dir import workspace_dir\n",
    "\n",
    "train_seq, train_label_prom, train_label_ss, train_label_polya = get_sequences('{}/train.csv'.format(workspace_dir), n_sample=500000)\n",
    "validation_seq, val_label_prom, val_label_ss, val_label_polya = get_sequences('{}/validation.csv'.format(workspace_dir), n_sample=100)\n",
    "\n",
    "\"\"\"\n",
    "Create dataloader.\n",
    "\"\"\"\n",
    "BATCH_SIZE = 2\n",
    "EPOCH_SIZE = 4\n",
    "\n",
    "_device = torch.device('cuda:0')\n",
    "train_label_prom = torch.tensor(train_label_prom, device=_device)\n",
    "train_label_ss = torch.tensor(train_label_ss, device=_device)\n",
    "train_label_polya = torch.tensor(train_label_polya, device=_device)\n",
    "\n",
    "train_inputs_ids, train_masks = preprocessing(train_seq, tokenizer)\n",
    "train_data = TensorDataset(train_inputs_ids, train_masks, train_label_prom, train_label_ss, train_label_polya)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "val_label_prom = torch.tensor(val_label_prom, device=_device)\n",
    "val_label_ss = torch.tensor(val_label_ss, device=_device)\n",
    "val_label_polya = torch.tensor(val_label_polya, device=_device)\n",
    "\n",
    "val_input_ids, val_masks = preprocessing(validation_seq, tokenizer)\n",
    "val_data = TensorDataset(val_input_ids, val_masks, val_label_prom, val_label_ss, val_label_polya)\n",
    "val_sampler = RandomSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "print('# of training data: {}'.format(len(train_seq)))  \n",
    "print('# of training data: {}'.format(len(validation_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np\n",
    "\n",
    "from multitask_learning import PolyAHead, PromoterHead, SpliceSiteHead, MTModel\n",
    "from transformers import BertForMaskedLM\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "\n",
    "polya_head = PolyAHead()\n",
    "promoter_head = PromoterHead()\n",
    "splice_head = SpliceSiteHead()\n",
    "\n",
    "dnabert_3_pretrained = pretrained_3kmer_dir\n",
    "shared_parameter = BertForMaskedLM.from_pretrained(dnabert_3_pretrained).bert\n",
    "\n",
    "model = MTModel(shared_parameters=shared_parameter, promoter_head=promoter_head, polya_head=polya_head, splice_site_head=splice_head)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datetime\n",
    "import os\n",
    "from data_preparation import kmer\n",
    "\n",
    "_now = datetime.datetime.now()\n",
    "\n",
    "_log_file = os.path.join('logs', 'notebooks', '2022-02.24.csv')\n",
    "os.makedirs(_log_file, exist_ok=True)\n",
    "\n",
    "seqs = [\"ATGC\" * 128, \"GATC\" * 128, \"CCAT\" * 128]\n",
    "seqs = [' '.join(kmer(s, 3)) for s in seqs]\n",
    "prom_labels = [1, 0, 0]\n",
    "ss_labels = [0, 1, 0]\n",
    "polya_labels = [0, 0, 1]\n",
    "\n",
    "def _format_prom_label(label):\n",
    "    return [label]\n",
    "\n",
    "def _format_other_label(label):\n",
    "    return label\n",
    "\n",
    "\"\"\"\n",
    "Initialize BERT tokenizer.\n",
    "\"\"\"\n",
    "from transformers import BertTokenizer\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_3kmer_dir)\n",
    "\n",
    "arr_input_ids = []\n",
    "arr_attention_mask = []\n",
    "arr_prom_label = []\n",
    "arr_ss_label = []\n",
    "arr_polya_label = []\n",
    "for i in range(len(seqs)):\n",
    "    s = seqs[i]\n",
    "    prom = prom_labels[i]\n",
    "    ss = ss_labels[i]\n",
    "    polya = polya_labels[i]\n",
    "\n",
    "    encoded = tokenizer.encode_plus(text=s, padding=\"max_length\", return_attention_mask=True)\n",
    "    arr_input_ids.append(encoded.get('input_ids'))\n",
    "    arr_attention_mask.append(encoded.get('attention_mask'))\n",
    "    arr_prom_label.append(_format_prom_label(prom))\n",
    "    arr_ss_label.append(_format_other_label(ss))\n",
    "    arr_polya_label.append(_format_other_label(polya))\n",
    "#endfor\n",
    "arr_input_ids = torch.tensor(arr_input_ids)\n",
    "arr_attention_mask = torch.tensor(arr_attention_mask)\n",
    "prom_labels = torch.tensor(arr_prom_label)\n",
    "ss_labels = torch.tensor(arr_ss_label)\n",
    "polya_labels = torch.tensor(arr_polya_label)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "dataset = TensorDataset(arr_input_ids, arr_attention_mask, prom_labels, ss_labels, polya_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prom_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:00<00:01,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5452]], grad_fn=<SigmoidBackward0>) tensor([[1]]) tensor(0.6066, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[0.4535, 0.5465]], grad_fn=<SoftmaxBackward0>) tensor([0]) tensor(0.7408, grad_fn=<NllLossBackward0>)\n",
      "tensor([[0.6232, 0.3768]], grad_fn=<SoftmaxBackward0>) tensor([0]) tensor(0.5775, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9249, grad_fn=<AddBackward0>) tensor(0.6416, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:02<00:01,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5974]], grad_fn=<SigmoidBackward0>) tensor([[0]]) tensor(0.9098, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[0.4219, 0.5781]], grad_fn=<SoftmaxBackward0>) tensor([1]) tensor(0.6181, grad_fn=<NllLossBackward0>)\n",
      "tensor([[0.5926, 0.4074]], grad_fn=<SoftmaxBackward0>) tensor([0]) tensor(0.6048, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1327, grad_fn=<AddBackward0>) tensor(0.7109, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5034]], grad_fn=<SigmoidBackward0>) tensor([[0]]) tensor(0.7001, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[0.4662, 0.5338]], grad_fn=<SoftmaxBackward0>) tensor([0]) tensor(0.7276, grad_fn=<NllLossBackward0>)\n",
      "tensor([[0.6131, 0.3869]], grad_fn=<SoftmaxBackward0>) tensor([1]) tensor(0.8126, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2402, grad_fn=<AddBackward0>) tensor(0.7467, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.nn import CrossEntropyLoss, BCELoss\n",
    "\n",
    "crossentropy_function = CrossEntropyLoss()\n",
    "binary_crossentropy_function = BCELoss()\n",
    "model.zero_grad()\n",
    "for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "    in_ids, attn_mask, label_prom, label_ss, label_polya = tuple(t for t in batch)\n",
    "    output = model(in_ids, attn_mask)\n",
    "    pred_prom = output['prom']\n",
    "    pred_ss = output['ss']\n",
    "    pred_polya = output['polya']    \n",
    "    loss_prom = binary_crossentropy_function(pred_prom, label_prom.float())\n",
    "    loss_ss = crossentropy_function(pred_ss, label_ss)\n",
    "    loss_polya = crossentropy_function(pred_polya, label_polya)\n",
    "    print(pred_prom, label_prom, loss_prom)\n",
    "    print(pred_ss, label_ss, loss_ss)\n",
    "    print(pred_polya, label_polya, loss_polya)\n",
    "    sum_loss = loss_prom + loss_ss + loss_polya\n",
    "    print(sum_loss, sum_loss/3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2231)\n",
      "tensor(1.6204)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import BCELoss, CrossEntropyLoss\n",
    "from torch import tensor\n",
    "prom_pred = tensor([[0.2]])\n",
    "prom_target = tensor([[0.]])\n",
    "other_pred = tensor([[0.1, 1.5]])\n",
    "other_target = tensor([0])\n",
    "bce = BCELoss()\n",
    "cross = CrossEntropyLoss()\n",
    "print(bce(prom_pred, prom_target))\n",
    "print(cross(other_pred, other_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datetime\n",
    "import os\n",
    "from data_preparation import kmer\n",
    "from transformers import BertTokenizer\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "\n",
    "_now = datetime.datetime.now()\n",
    "\n",
    "_log_file = os.path.join('logs', 'notebooks', '2022-02.24.csv')\n",
    "os.makedirs(_log_file, exist_ok=True)\n",
    "\n",
    "seqs = [\"ATGC\" * 128, \"GATC\" * 128, \"CCAT\" * 128]\n",
    "seqs = [' '.join(kmer(s, 3)) for s in seqs]\n",
    "prom_labels = [1, 0, 0]\n",
    "ss_labels = [0, 1, 0]\n",
    "polya_labels = [0, 0, 1]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_3kmer_dir)\n",
    "encoded = tokenizer(seqs[0], seqs[1])\n",
    "print(encoded)\n",
    "print(tokenizer.convert_ids_to_tokens(encoded['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multitask_learning import evaluate, init_model_mtl\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "\n",
    "model = init_model_mtl(pretrained_3kmer_dir, 'cuda:0')\n",
    "avg_prom_acc, avg_ss_acc, avg_polya_acc, avg_prom_loss, avg_ss_loss, avg_polya_loss = evaluate(model, )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7170ee380771f2003055f3cee3e2e4dd0d81d1dac73a8c82197236b1572f37c2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
