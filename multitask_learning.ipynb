{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check if CUDA is supported.\n",
    "\"\"\"\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.device('cuda:0')\n",
    "torch.cuda.get_device_name(0)\n",
    "_device = torch.device('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_preparation import kmer\n",
    "\n",
    "def get_sequences(csv_path, n_sample=10, random_state=1337):\n",
    "    r\"\"\"\n",
    "    Get sequence from certain CSV. CSV has header such as 'sequence', 'label_prom', 'label_ss', 'label_polya'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if (n_sample > 0):\n",
    "        df = df.sample(n=n_sample, random_state=random_state)\n",
    "    sequence = list(df['sequence'])\n",
    "    label_prom = list(df['label_prom'])\n",
    "    label_ss = list(df['label_ss'])\n",
    "    label_polya = list(df['label_polya'])\n",
    "\n",
    "    return sequence, label_prom, label_ss, label_polya\n",
    "\n",
    "import torch\n",
    "def preprocessing(data, tokenizer):\n",
    "    \"\"\"\n",
    "    Preprocessing for pretrained BERT.\n",
    "    @param  data (string): string containing kmers separated by spaces.\n",
    "    @param  tokenizer (Tokenizer): tokenizer initialized from pretrained values.\n",
    "    @return input_ids (torch.Tensor): tensor of token ids to be fed to model.\n",
    "    @return attention_masks (torch.Tensor): tensor of indices (a bunch of 'indexes') specifiying which token needs to be attended by model.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    _count = 0\n",
    "    _len_data = len(data)\n",
    "    for sequence in data:\n",
    "        \"\"\"\n",
    "        Sequence is 512 characters long.\n",
    "        \"\"\"\n",
    "        _count += 1\n",
    "        if _count < _len_data:\n",
    "            print(\"Seq length = {} [{}/{}]\".format(len(sequence.split(' ')), _count, _len_data), end='\\r')\n",
    "        else:\n",
    "            print(\"Seq length = {} [{}/{}]\".format(len(sequence.split(' ')), _count, _len_data))\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sequence,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert input_ids and attention_masks to tensor.\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "\"\"\"\n",
    "Initialize tokenizer using BertTokenizer with pretrained weights from DNABert.\n",
    "\"\"\"\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('./pretrained/3-new-12w-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting and storing split to ./workspace/train.csv\n",
      "Splitting and storing split to ./workspace/validation.csv\n",
      "Splitting source ./dataset/full/train.csv: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Split dataset into two parts: train and validation.\n",
    "\"\"\"\n",
    "from data_dir import workspace_dir, dataset_full_dir\n",
    "from data_preparation import split_and_store_csv\n",
    "_src_csv = \"{}/train.csv\".format(dataset_full_dir)\n",
    "_fractions = [0.9, 0.1]\n",
    "_store_paths = [\n",
    "    \"{}/{}\".format(workspace_dir, 'train.csv'),\n",
    "    \"{}/{}\".format(workspace_dir, 'validation.csv'),\n",
    "]\n",
    "print(\"Splitting source {}: {}\".format(_src_csv, split_and_store_csv(_src_csv, fractions=_fractions, store_paths=_store_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq length = 510 [500000/500000]\n",
      "Seq length = 510 [100/100]\n",
      "# of training data: 500000\n",
      "# of training data: 100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from data_dir import workspace_dir\n",
    "\n",
    "train_seq, train_label_prom, train_label_ss, train_label_polya = get_sequences('{}/train.csv'.format(workspace_dir), n_sample=500000)\n",
    "validation_seq, val_label_prom, val_label_ss, val_label_polya = get_sequences('{}/validation.csv'.format(workspace_dir), n_sample=100)\n",
    "\n",
    "\"\"\"\n",
    "Create dataloader.\n",
    "\"\"\"\n",
    "BATCH_SIZE = 2\n",
    "EPOCH_SIZE = 4\n",
    "\n",
    "_device = torch.device('cuda:0')\n",
    "train_label_prom = torch.tensor(train_label_prom, device=_device)\n",
    "train_label_ss = torch.tensor(train_label_ss, device=_device)\n",
    "train_label_polya = torch.tensor(train_label_polya, device=_device)\n",
    "\n",
    "train_inputs_ids, train_masks = preprocessing(train_seq, tokenizer)\n",
    "train_data = TensorDataset(train_inputs_ids, train_masks, train_label_prom, train_label_ss, train_label_polya)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "val_label_prom = torch.tensor(val_label_prom, device=_device)\n",
    "val_label_ss = torch.tensor(val_label_ss, device=_device)\n",
    "val_label_polya = torch.tensor(val_label_polya, device=_device)\n",
    "\n",
    "val_input_ids, val_masks = preprocessing(validation_seq, tokenizer)\n",
    "val_data = TensorDataset(val_input_ids, val_masks, val_label_prom, val_label_ss, val_label_polya)\n",
    "val_sampler = RandomSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "print('# of training data: {}'.format(len(train_seq)))  \n",
    "print('# of training data: {}'.format(len(validation_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from multitask_learning import init_model_mtl\n",
    "from transformers import BertForMaskedLM\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "\n",
    "# model = MTModel(shared_parameters=shared_parameter, promoter_head=promoter_head, polya_head=polya_head, splice_site_head=splice_head)\n",
    "model = init_model_mtl(pretrained_3kmer_dir, head=\"bert\", config=os.path.join(\"models\", \"config\", \"mtl.json\"))\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datetime\n",
    "import os\n",
    "from data_preparation import kmer\n",
    "\n",
    "_now = datetime.datetime.now()\n",
    "\n",
    "_log_file = os.path.join('logs', 'notebooks', '2022-02.24.csv')\n",
    "os.makedirs(_log_file, exist_ok=True)\n",
    "\n",
    "#seqs = [\"ATGC\" * 128, \"GATC\" * 128, \"CCAT\" * 128]\n",
    "seqs = [\"ATGC\" * 128]\n",
    "seqs = [' '.join(kmer(s, 3)) for s in seqs]\n",
    "prom_labels = [1] #, 0, 0]\n",
    "ss_labels = [0] #, 1, 0]\n",
    "polya_labels = [0] #, 0, 1]\n",
    "\n",
    "def _format_prom_label(label):\n",
    "    return [label]\n",
    "\n",
    "def _format_other_label(label):\n",
    "    return label\n",
    "\n",
    "\"\"\"\n",
    "Initialize BERT tokenizer.\n",
    "\"\"\"\n",
    "from transformers import BertTokenizer\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_3kmer_dir)\n",
    "\n",
    "arr_input_ids = []\n",
    "arr_attention_mask = []\n",
    "arr_prom_label = []\n",
    "arr_ss_label = []\n",
    "arr_polya_label = []\n",
    "for i in range(len(seqs)):\n",
    "    s = seqs[i]\n",
    "    prom = prom_labels[i]\n",
    "    ss = ss_labels[i]\n",
    "    polya = polya_labels[i]\n",
    "\n",
    "    encoded = tokenizer.encode_plus(text=s, padding=\"max_length\", return_attention_mask=True)\n",
    "    arr_input_ids.append(encoded.get('input_ids'))\n",
    "    arr_attention_mask.append(encoded.get('attention_mask'))\n",
    "    arr_prom_label.append(_format_prom_label(prom))\n",
    "    arr_ss_label.append(_format_other_label(ss))\n",
    "    arr_polya_label.append(_format_other_label(polya))\n",
    "#endfor\n",
    "arr_input_ids = torch.tensor(arr_input_ids)\n",
    "arr_attention_mask = torch.tensor(arr_attention_mask)\n",
    "prom_labels = torch.tensor(arr_prom_label)\n",
    "ss_labels = torch.tensor(arr_ss_label)\n",
    "polya_labels = torch.tensor(arr_polya_label)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "dataset = TensorDataset(arr_input_ids, arr_attention_mask, prom_labels, ss_labels, polya_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.1867e-01,  8.7250e-01,  9.6594e-01,  4.9828e-01, -6.9271e-01,\n",
      "          3.5690e-01, -2.3471e-01,  2.5266e-02,  1.2839e-01,  2.3239e-01,\n",
      "          3.8367e-01, -1.0836e+00,  1.4053e-01,  4.6187e-01,  6.6053e-01,\n",
      "          1.2233e+00, -9.5204e-02,  2.0922e-01,  1.1892e+00, -4.1628e-01,\n",
      "          1.7621e-01,  6.9880e-01, -3.0316e-01,  2.1829e-01, -7.1974e-03,\n",
      "          1.5341e+00,  1.5103e+00,  1.9468e-01,  5.3798e-01, -6.2879e-01,\n",
      "         -1.5484e+00,  1.6947e-01, -2.1196e-02, -2.3976e-01,  3.8130e-01,\n",
      "          4.5743e-01,  5.1212e-01, -1.3875e+00, -1.0507e-01,  3.0194e+00,\n",
      "          9.8335e-01,  2.8834e-01,  8.8160e-01,  1.0969e+00, -2.2722e-02,\n",
      "         -8.9258e-02, -7.9624e-01, -1.1956e-03,  2.7781e-01, -9.0780e-02,\n",
      "         -1.5226e+00,  1.6144e-01, -6.4207e-01,  4.6726e-02,  9.5772e-02,\n",
      "         -1.1522e+00, -6.0590e-02,  2.1621e-01,  1.0219e+00,  7.0359e-01,\n",
      "          6.6921e-01, -2.0137e-01, -1.6163e+00, -1.8081e-01, -8.1795e-01,\n",
      "         -1.8817e+00, -2.8046e-01,  4.1040e-01,  1.5483e-01, -7.7091e-01,\n",
      "          4.1177e-01, -1.0184e+00, -1.5126e+00,  3.5277e-01, -9.8714e-02,\n",
      "          8.3146e-01,  4.2767e-01,  9.6798e-01, -1.7139e-01,  1.0125e-01,\n",
      "          8.2054e-01, -4.7193e-01,  1.0840e+00, -4.8875e-01, -8.6090e-01,\n",
      "          2.8233e-01,  4.2486e-01,  1.1193e+00,  2.6923e-01, -7.9035e-01,\n",
      "         -4.4313e-01,  3.0747e-01, -8.9897e-01, -5.9644e-01,  1.1443e+00,\n",
      "         -6.2356e-02, -1.2971e-02,  1.4299e+00, -1.2726e-01, -1.0204e+00,\n",
      "          6.0796e-01,  3.7275e-01, -3.8199e-01, -1.7019e-01, -4.4414e-01,\n",
      "         -2.2883e-01, -6.0319e-01,  1.4361e+00, -6.0791e-01,  6.3127e-01,\n",
      "         -2.3752e-02,  7.5552e-01, -2.2178e+00, -8.6214e-01,  6.6666e-02,\n",
      "         -9.8493e-01, -4.9009e-01,  1.1504e-01,  1.0419e-01, -1.1152e+00,\n",
      "          5.7955e-01, -2.8515e-01,  2.8269e-01, -4.8875e-01,  4.3142e-02,\n",
      "         -3.4962e-01, -4.6312e-02, -7.3984e-01, -2.2497e-01, -2.0431e-01,\n",
      "         -1.0102e-01,  4.4809e-01,  4.6597e-01, -7.5996e-01,  1.0523e+00,\n",
      "         -6.8635e-02,  1.2896e+00,  4.7702e-01, -5.9936e-01,  1.2085e-01,\n",
      "          2.1623e-01,  5.5636e-01, -5.6177e-01, -3.5293e-01, -3.3295e-01,\n",
      "          6.6571e-01, -2.9868e-01,  2.1012e-01,  2.5314e-01, -3.2956e-01,\n",
      "         -1.0212e+00,  4.3824e-02, -9.9890e-01, -1.0787e-01, -3.4136e-01,\n",
      "         -6.7227e-01, -3.0290e-01, -5.8203e-01,  9.7241e-01, -2.2161e-01,\n",
      "         -5.3196e-01,  3.4236e-01, -1.8792e-01,  6.5667e-01,  1.1700e+00,\n",
      "         -4.1915e-01,  1.5536e-01,  2.8803e-02,  3.1738e-01, -2.5731e-01,\n",
      "          4.2692e-01,  7.3915e-01, -1.2822e-01,  1.0172e+00,  1.8792e-01,\n",
      "          3.7605e-01, -6.3386e-01, -4.9311e-01, -4.4678e-01, -3.4470e-02,\n",
      "          1.3419e+00,  5.8167e-01,  4.0040e-01, -1.6690e-01, -7.6911e-01,\n",
      "         -1.1706e+00,  1.0296e+00,  2.9626e-01,  3.4799e-01,  4.4494e-01,\n",
      "         -1.0039e+00,  2.3588e-01, -7.7406e-01, -1.2373e-01, -8.4037e-01,\n",
      "          1.1454e+00,  3.2290e-01,  5.0801e-01, -5.8358e-01,  1.5926e+00,\n",
      "         -8.6247e-01, -7.9035e-01, -6.1308e-01, -1.0293e-01,  5.2588e-01,\n",
      "          1.4605e-01,  2.0205e-01,  5.2520e-01, -2.1112e-01, -6.6861e-02,\n",
      "          1.0744e+00,  9.2684e-01, -5.4005e-01, -4.0302e-01,  2.1467e-01,\n",
      "          1.4886e-01,  8.2510e-02,  5.0963e-01, -6.7839e-01, -7.7320e-01,\n",
      "         -7.2502e-01,  1.2136e+00,  1.7968e-01,  2.8191e-01, -1.0317e+00,\n",
      "          9.3389e-01,  7.4204e-01, -5.8234e-01,  2.6014e-01,  1.0528e-02,\n",
      "          2.9011e-01,  1.1104e+00, -6.8710e-01, -1.5466e-01, -1.7178e-01,\n",
      "          1.0826e-01, -2.8016e-02, -4.4946e-01,  6.3806e-01,  1.1645e+00,\n",
      "          6.3497e-01, -8.4194e-02, -1.7593e-01,  7.2080e-01,  1.5249e+00,\n",
      "          9.2045e-03, -1.6392e-01, -1.8156e-01, -3.5799e-01, -5.3102e-02,\n",
      "          1.6728e+00, -6.4419e-01,  1.0780e-02,  9.8880e-02,  2.8483e-01,\n",
      "         -7.1017e-01,  6.7573e-01,  8.3157e-01, -4.9353e-02,  4.2233e-01,\n",
      "         -6.2857e-01, -2.1769e-01,  1.9896e-01,  1.8516e-01,  1.2187e+00,\n",
      "         -6.5911e-01, -1.0085e-01,  4.9215e-01, -3.5760e-01,  6.7385e-01,\n",
      "         -1.3293e-02, -1.1315e+00, -1.2823e+00, -1.5785e-01,  1.1769e-01,\n",
      "          2.0640e-01,  2.1824e-01,  2.8975e-01,  1.5017e-01,  8.5600e-01,\n",
      "         -8.1334e-01,  4.0621e-01, -1.4985e-01,  9.7846e-01,  1.0748e+00,\n",
      "         -5.8606e-01,  4.2327e-01, -1.5202e+00, -1.3530e-01,  1.5795e-01,\n",
      "          9.9462e-02,  1.0416e+00, -9.6775e-01,  8.8233e-01,  6.2067e-01,\n",
      "          9.5214e-01,  3.5943e-01, -2.4494e-01, -9.4126e-02,  7.2814e-01,\n",
      "          1.1291e+00, -7.5800e-01,  2.7245e-01,  5.8280e-01, -5.6734e-01,\n",
      "          6.3835e-01,  1.1671e+00,  2.5968e-01,  2.3381e-01,  4.9766e-01,\n",
      "         -6.4955e-01, -2.2598e-01, -6.3888e-01, -3.2897e-01,  1.1432e+00,\n",
      "          1.2816e+00, -3.8837e-01,  5.8619e-01, -5.3381e-01, -1.4897e+00,\n",
      "         -1.8767e+00,  3.6429e-01,  1.9555e-01, -8.2584e-01,  1.7264e-01,\n",
      "          4.9370e-01,  7.1298e-02, -3.1777e-01,  9.3797e-01, -4.0664e-01,\n",
      "          9.3178e-01,  8.5298e-02, -8.7991e-01,  1.8869e+00, -6.4916e-01,\n",
      "          4.5459e-02,  1.0449e+00, -1.9105e+00,  9.1644e-01, -1.9178e-02,\n",
      "          4.0928e-01,  6.5655e-01, -4.4194e-02,  3.5713e-01, -2.7339e-01,\n",
      "         -2.2126e-01, -6.2962e-01, -2.9369e-01, -9.7401e-01, -6.2995e-01,\n",
      "         -4.8247e-01, -4.0138e-01,  5.3864e-01, -1.2776e-01, -9.3316e-01,\n",
      "         -1.2223e+00,  2.7190e-01, -1.0820e+00, -1.9834e+00,  3.6388e-01,\n",
      "          4.8770e-01,  3.6521e-01, -3.3317e-02,  6.0449e-01,  3.0814e-01,\n",
      "          5.5291e-01,  5.8390e-01, -9.0115e-01, -2.7469e-03, -6.9300e-01,\n",
      "          2.5971e-01,  2.0703e+00, -1.2907e+00,  1.9525e+00, -5.1185e-01,\n",
      "         -9.9335e-01,  7.8215e-01,  1.7072e-01,  1.8241e-01,  4.9138e-01,\n",
      "          2.2879e-01, -2.9574e-01, -1.0486e-01, -2.2272e-01,  4.8359e-01,\n",
      "          2.6314e-01,  6.2972e-01,  3.1997e-01,  3.4939e-01, -1.2459e-01,\n",
      "         -3.6524e-01, -1.1917e+00,  1.2755e+00,  5.3296e-01, -2.6716e-01,\n",
      "          1.0685e-01, -5.9415e-01, -1.1946e+00, -2.2150e-01, -7.8729e-01,\n",
      "          2.9901e-01,  2.9884e-01, -2.5758e-01,  4.5726e-01,  3.4065e-01,\n",
      "          1.7564e+00, -8.7721e-01, -3.5915e-01,  2.6188e-01, -1.8384e+00,\n",
      "         -7.0139e-01,  2.1912e-02, -3.0263e-01,  1.0991e+00, -1.2125e+00,\n",
      "         -1.1378e+00, -4.4462e-01,  5.3544e-02, -2.5723e-02,  6.2203e-01,\n",
      "          3.7097e-02,  6.6379e-01,  1.3920e-01, -1.1429e+00,  3.0034e-01,\n",
      "          8.9804e-02,  1.7139e+00,  4.0412e-01,  5.8131e-01, -6.0687e-01,\n",
      "          1.5715e-01, -2.5965e-01,  1.1605e+00, -5.7139e-02, -9.1723e-01,\n",
      "          3.7870e-01, -7.0132e-02, -4.8260e-01,  1.0571e+00, -1.5748e-01,\n",
      "          7.1628e-01, -7.1694e-01, -2.5837e-01,  8.1844e-01,  8.0320e-01,\n",
      "         -1.0665e+00,  7.6826e-01,  4.1834e-01,  4.7482e-01, -5.1786e-01,\n",
      "         -5.2365e-01, -5.6476e-01, -3.3373e-02, -1.4193e+00,  8.8723e-01,\n",
      "          1.9712e-01,  4.1078e-01,  3.3094e-01, -9.5753e-01, -4.3114e-01,\n",
      "          2.1482e-01, -4.1920e-01,  9.2119e-02, -6.5254e-01,  3.0022e-01,\n",
      "         -1.1134e-02,  1.0024e+00, -1.5059e-01, -1.6077e+00, -1.4114e-01,\n",
      "          5.5438e-01,  9.2324e-01,  1.7686e-01,  1.4791e-02, -4.1138e-01,\n",
      "         -8.9181e-02,  2.2506e-01, -4.9755e-01, -4.6801e-01,  2.7435e-01,\n",
      "         -8.9295e-01,  7.9411e-01, -5.5994e-01,  1.1229e-01,  1.3456e-01,\n",
      "         -7.6413e-02, -2.5071e-01,  9.2660e-01,  9.2052e-02, -2.7479e-01,\n",
      "         -3.6081e-03, -8.8028e-01,  7.8813e-01,  9.8305e-01,  1.1968e+00,\n",
      "         -3.4371e-01,  1.8473e-01,  3.2032e-01,  7.2762e-01,  2.8905e-01,\n",
      "          2.6054e-01, -8.1656e-01, -2.2182e-01, -2.9280e-01, -9.6627e-01,\n",
      "         -1.1170e-01, -7.9548e-01,  2.7864e-01,  1.0717e+00,  6.2939e-01,\n",
      "         -3.5141e-01, -2.0316e-01, -3.6371e-01, -1.1767e+00,  1.1476e-01,\n",
      "         -2.9619e-01, -4.5183e-01,  6.3665e-02, -1.2509e+00, -1.0561e-01,\n",
      "          8.6844e-01, -1.2625e+00, -6.9449e-01,  2.2097e+00,  8.7025e-01,\n",
      "          1.6711e-01, -8.8326e-01, -2.8562e-01,  1.4629e-01,  4.5488e-01,\n",
      "         -2.5468e-01, -2.8361e-01, -1.6189e+00,  1.9539e+00,  6.0447e-01,\n",
      "          4.0133e-01, -6.0264e-01, -3.6488e-01,  2.3706e+00, -5.7272e-01,\n",
      "         -1.0293e+00, -1.1740e+00,  8.6497e-01,  2.7565e-01,  1.8769e+00,\n",
      "         -1.8923e+00, -1.6128e+00,  3.3841e-01,  2.2601e-01, -9.9908e-01,\n",
      "         -3.3952e-01, -3.0466e-01,  1.0309e-01,  4.5116e-01,  3.8357e-02,\n",
      "         -2.6987e-01,  9.3864e-01,  3.8829e-01, -4.0656e-03,  1.8927e+00,\n",
      "         -1.5621e-01, -3.7718e-01,  8.9542e-01, -7.6345e-01,  1.9418e-01,\n",
      "         -1.4050e+00, -4.0858e-01, -1.8754e-01, -3.5927e-01, -3.2712e-01,\n",
      "          1.0472e+00, -1.0198e+00,  3.9838e-01,  4.9637e-01, -5.0290e-01,\n",
      "         -9.8160e-01, -2.5329e-01,  9.5292e-01,  6.9754e-02, -9.4207e-02,\n",
      "         -2.6701e-01,  3.9841e-01, -3.3477e-01, -1.6928e+00,  4.0650e-02,\n",
      "         -6.4446e-01,  5.4410e-01,  7.6357e-01, -6.5159e-01,  8.7253e-01,\n",
      "          9.2967e-01, -7.8749e-01,  1.8912e-02,  5.7043e-01,  3.4840e-01,\n",
      "          1.2369e+00, -8.2429e-02, -5.3402e-01,  6.7595e-01, -3.1359e-01,\n",
      "          1.0131e+00, -1.5490e-01,  1.6143e+00, -3.8452e-01, -8.9866e-01,\n",
      "          2.5649e-01,  7.0040e-01,  1.3320e-02, -6.2869e-01, -4.8042e-01,\n",
      "         -2.3438e-01, -4.0691e-01, -1.4313e-02, -1.9642e+00, -2.4477e-01,\n",
      "         -7.8931e-01,  7.0309e-02,  8.2683e-01,  5.3485e-01, -1.7362e-01,\n",
      "          9.9902e-01, -4.8655e-01,  1.6723e-01,  2.2544e-01,  1.5837e-01,\n",
      "         -1.5756e-01,  7.4882e-01,  4.2271e-01, -1.1564e+00,  6.9794e-01,\n",
      "         -1.3785e-01, -4.2864e-01,  2.5466e-01,  1.5713e-01,  7.6341e-01,\n",
      "         -2.4895e+00,  4.4354e-01,  3.5026e-01,  1.3344e+00,  3.3921e-01,\n",
      "         -7.0753e-01,  2.9749e-02,  1.2601e+00, -3.8090e-01, -5.3624e-01,\n",
      "         -3.8737e-01,  4.0931e-01, -1.9021e-01,  3.5101e-01, -5.0790e-01,\n",
      "          8.4088e-01, -4.7234e-01,  1.5596e+00,  3.1958e-03, -7.9031e-01,\n",
      "         -6.0358e-01,  3.4475e-01, -5.5856e-02,  1.1493e+00,  7.2717e-01,\n",
      "         -1.4134e+00,  3.4827e-01,  9.3077e-01,  2.7841e-01,  1.3582e-01,\n",
      "         -5.6781e-01,  6.7121e-01,  3.2364e-01, -1.3090e+00, -3.3832e-01,\n",
      "         -4.3504e-01,  3.9038e-01, -4.2357e-02, -6.3456e-01, -1.9503e-01,\n",
      "         -7.8473e-01,  3.3160e-01,  1.4904e+00,  7.6494e-01, -1.0864e+00,\n",
      "         -5.8477e-01, -4.3396e-02,  2.5095e-01, -1.0798e+00,  2.2368e-01,\n",
      "          5.4354e-02,  1.4494e-01,  5.2756e-01, -1.0649e+00, -2.2255e-01,\n",
      "         -5.2914e-01, -1.6386e-01,  2.3582e-01, -5.3771e-02,  1.2222e+00,\n",
      "         -5.3286e-01,  5.2790e-01, -9.2077e-01,  8.9917e-01,  1.4559e+00,\n",
      "          1.0940e+00,  1.5559e-01, -1.0557e+00,  1.1369e+00, -5.9857e-01,\n",
      "         -2.1714e-01,  4.7683e-01, -4.0875e-01, -7.8683e-01, -1.1667e-01,\n",
      "         -1.7321e+00, -1.0408e+00,  3.4043e-01, -2.5474e-01, -1.2396e+00,\n",
      "          1.2180e+00, -8.0373e-01, -7.8288e-02,  3.9688e-01, -9.8494e-02,\n",
      "          1.0897e-01,  5.3479e-01, -1.6913e-01,  1.3109e+00, -1.3072e+00,\n",
      "          3.6286e-01, -1.7256e-01,  4.8727e-01, -7.7319e-01,  1.2300e+00,\n",
      "         -2.5122e-02,  5.7564e-01,  8.8225e-01, -2.1185e-01,  2.5060e-01,\n",
      "          1.6969e+00,  6.1294e-01, -7.0430e-01, -7.3563e-01, -9.6710e-02,\n",
      "          1.4497e-01, -1.1334e+00, -1.6406e+00, -6.8476e-01, -6.7161e-01,\n",
      "         -2.1598e-01, -1.6621e+00,  2.5021e-01,  2.3161e-01,  6.0645e-02,\n",
      "         -1.0792e+00,  9.3287e-04,  3.1516e-01, -3.0311e-01,  7.8640e-01,\n",
      "          4.2669e-01, -9.7441e-01, -7.3991e-01, -8.5008e-01,  1.4219e+00,\n",
      "          9.2299e-01, -1.2231e+00, -1.3153e-01,  1.4279e+00, -5.4223e-01,\n",
      "         -9.0695e-03,  8.9863e-01, -1.3403e+00]], grad_fn=<SelectBackward0>)\n",
      "accuracy prom: 0.0\n",
      "accuracy ss: 0.0\n",
      "accuracy polya: 0.0\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.nn import CrossEntropyLoss, BCELoss\n",
    "\n",
    "crossentropy_function = CrossEntropyLoss()\n",
    "binary_crossentropy_function = BCELoss()\n",
    "model.zero_grad()\n",
    "count_prom_correct = 0\n",
    "count_ss_correct = 0\n",
    "count_polya_correct = 0\n",
    "# for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "for step, batch in enumerate(dataloader):\n",
    "    in_ids, attn_mask, label_prom, label_ss, label_polya = tuple(t for t in batch)\n",
    "    bert_output = model.shared_layer(in_ids, attn_mask)\n",
    "    print(bert_output[0][:,0])\n",
    "    \n",
    "    #pred_prom = model.promoter_layer(bert_output)\n",
    "    #pred_ss = model.splice_site_layer(bert_output)\n",
    "    #pred_polya = model.polya_layer(bert_output)\n",
    "    \n",
    "    #predicted_prom = torch.round(pred_prom).item()\n",
    "    #actual_prom = label_prom.float().item()\n",
    "    #if (predicted_prom == actual_prom):\n",
    "    #    count_prom_correct += 1\n",
    "    #print(pred_prom, label_prom, predicted_prom)\n",
    "\n",
    "    #predicted_ss, predicted_ss_index = torch.max(pred_ss, 1)\n",
    "    #predicted_ss = predicted_ss.item()\n",
    "    #predicted_ss_index = predicted_ss_index.item()\n",
    "    # print(pred_ss, label_ss, predicted_ss, predicted_ss_index)\n",
    "    #if (predicted_ss_index == label_ss):\n",
    "    #    count_ss_correct += 1\n",
    "    #print(sum_loss, sum_loss/3)\n",
    "\n",
    "    #predicted_polya, predicted_polya_index = torch.max(pred_polya, 1)\n",
    "    #predicted_polya = predicted_polya.item()\n",
    "    #predicted_polya_index = predicted_polya_index.item()\n",
    "    #if (predicted_polya_index == label_polya):\n",
    "    #    count_polya_correct += 1\n",
    "\n",
    "print('accuracy prom: {}'.format(count_prom_correct / len(dataloader) * 100))\n",
    "print('accuracy ss: {}'.format(count_ss_correct / len(dataloader) * 100))\n",
    "print('accuracy polya: {}'.format(count_polya_correct / len(dataloader) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4187,  0.8725,  0.9659,  ..., -0.0091,  0.8986, -1.3403],\n",
      "         [-1.4170, -0.9827, -0.7155,  ...,  0.0354,  0.6974, -0.8663],\n",
      "         [-0.6982, -0.7672,  1.1986,  ..., -1.6839,  0.2338, -0.5892],\n",
      "         ...,\n",
      "         [-1.3530, -0.8583, -0.9080,  ...,  0.0355,  0.7268, -0.8151],\n",
      "         [-0.7084, -0.7193,  1.1986,  ..., -1.6446,  0.1983, -0.5487],\n",
      "         [-0.4187,  0.8725,  0.9659,  ..., -0.0091,  0.8986, -1.3403]]])\n",
      "tensor([[[-0.4187,  0.8725,  0.9659,  ..., -0.0091,  0.8986, -1.3403],\n",
      "         [-1.4170, -0.9827, -0.7155,  ...,  0.0354,  0.6974, -0.8663],\n",
      "         [-0.6982, -0.7672,  1.1986,  ..., -1.6839,  0.2338, -0.5892],\n",
      "         ...,\n",
      "         [-1.3530, -0.8583, -0.9080,  ...,  0.0355,  0.7268, -0.8151],\n",
      "         [-0.7084, -0.7193,  1.1986,  ..., -1.6446,  0.1983, -0.5487],\n",
      "         [-0.4187,  0.8725,  0.9659,  ..., -0.0091,  0.8986, -1.3403]]])\n"
     ]
    }
   ],
   "source": [
    "from data_dir import pretrained_3kmer_dir\n",
    "from transformers import BertForMaskedLM\n",
    "from torch import nn\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        input_ids, attn_mask, label_prom, label_ss, label_polya = tuple(t for t in batch)\n",
    "        bert_output = model.bert(input_ids, attention_mask=attn_mask, output_hidden_states=True, output_attentions=True)\n",
    "        hidden_states = bert_output.hidden_states\n",
    "        print(bert_output.last_hidden_state)\n",
    "        print(hidden_states[12])\n",
    "        #dropout = nn.Dropout(p=0.1)(hidden_states[1])\n",
    "        \n",
    "        #print(dropout)\n",
    "        #print(bert_output.last_hidden_state)\n",
    "        #print(bert_output.pooler_output)\n",
    "        #print(bert_output.hidden_states)\n",
    "        #print(len(bert_output[1]))\n",
    "        #print(len(bert_output[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(69, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=69, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "\n",
    "bert_for_masked_lm = BertForMaskedLM.from_pretrained(pretrained_3kmer_dir)\n",
    "print(bert_for_masked_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pretrained\\3-new-12w-0 were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at pretrained\\3-new-12w-0 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(69, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "\n",
    "bert_for_seq_cls = BertForSequenceClassification.from_pretrained(pretrained_3kmer_dir)\n",
    "print(bert_for_seq_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2231)\n",
      "tensor(1.6204)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import BCELoss, CrossEntropyLoss\n",
    "from torch import tensor\n",
    "prom_pred = tensor([[0.2]])\n",
    "prom_target = tensor([[0.]])\n",
    "other_pred = tensor([[0.1, 1.5]])\n",
    "other_target = tensor([0])\n",
    "bce = BCELoss()\n",
    "cross = CrossEntropyLoss()\n",
    "print(bce(prom_pred, prom_target))\n",
    "print(cross(other_pred, other_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datetime\n",
    "import os\n",
    "from data_preparation import kmer\n",
    "from transformers import BertTokenizer\n",
    "from torch \n",
    "from data_dir import pretrained_3kmer_dir\n",
    "\n",
    "_now = datetime.datetime.now()\n",
    "\n",
    "_log_file = os.path.join('logs', 'notebooks', '2022-02.24.csv')\n",
    "os.makedirs(_log_file, exist_ok=True)\n",
    "\n",
    "seqs = [\"ATGC\" * 128, \"GATC\" * 128, \"CCAT\" * 128]\n",
    "seqs = [' '.join(kmer(s, 3)) for s in seqs]\n",
    "prom_labels = [1, 0, 0]\n",
    "ss_labels = [0, 1, 0]\n",
    "polya_labels = [0, 0, 1]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_3kmer_dir)\n",
    "encoded = tokenizer(seqs[0], seqs[1])\n",
    "print(encoded)\n",
    "print(tokenizer.convert_ids_to_tokens(encoded['input_ids']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multitask_learning import init_model_mtl\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "import os\n",
    "\n",
    "model_mtl_bert = init_model_mtl(pretrained_3kmer_dir, head=\"bert\", config=os.path.join('models', 'config', 'mtl.json'))\n",
    "preds = model_mtl_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage 0.1\n",
      "percentile 0.2\n"
     ]
    }
   ],
   "source": [
    "tuples = ('0.1', 0.2)\n",
    "titles = ['percentage', 'percentile']\n",
    "\n",
    "for a, b in zip(titles, tuples):\n",
    "    print(a, b)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7170ee380771f2003055f3cee3e2e4dd0d81d1dac73a8c82197236b1572f37c2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
