{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.dnabert.models import sequence_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "from utils.data_generator import _data_generator_mtl\n",
    "from multitask_learning import init_model_mtl\n",
    "import os\n",
    "dataloader = _data_generator_mtl()\n",
    "model = init_model_mtl(pretrained_3kmer_dir, head=\"bert\", config=os.path.join('models', 'config', 'mtl.json'))\n",
    "for step, batch in enumerate(dataloader):\n",
    "    input_ids, attn_mask, label_prom, label_ss, label_polya = tuple(t for t in batch)\n",
    "    output = model(input_ids, attn_mask)\n",
    "    # print(output.keys())\n",
    "    bert = model.shared_layer\n",
    "    bert_output = bert(input_ids, attn_mask)[0]\n",
    "    print(bert_output.shape)\n",
    "    print(output['prom'].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "pred = torch.tensor([\n",
    "        [[0.5, 0.25], [0.5, 0.25], [0.5, 0.25]]\n",
    "    ])\n",
    "label = torch.tensor([[1, 0, 0]])\n",
    "for p, l in zip(pred, label):\n",
    "    loss = loss_fn(\n",
    "        p,\n",
    "        l\n",
    "    )\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "pred = torch.tensor([[0.0, 0.9]])\n",
    "label = torch.tensor([0])\n",
    "loss = loss_fn(pred, label)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "model = BertForMaskedLM.from_pretrained(pretrained_3kmer_dir)\n",
    "bert_layer = model.bert\n",
    "sum(p.numel() for p in bert_layer.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtl_seq = [\n",
    "    [\"ATGC\" * 128, 0, 0, 1],\n",
    "    [\"TGCG\" * 128, 1, 0, 0],\n",
    "    [\"GACT\" * 128, 0, 1, 0],\n",
    "    [\"CACG\" * 128, 0, 0, 0],\n",
    "    [\"CCAT\" * 128, 0, 0, 0],\n",
    "]\n",
    "import pandas as pd\n",
    "import os\n",
    "df = pd.DataFrame(mtl_seq, columns=[\"sequence\", \"label_prom\", \"label_ss\", \"label_polya\"])\n",
    "mtl_sample_csv = os.path.join(\"sample\", \"mtl\", \"sample.csv\")\n",
    "os.makedirs(os.path.dirname(mtl_sample_csv), exist_ok=True)\n",
    "if os.path.exists(mtl_sample_csv):\n",
    "    os.remove(mtl_sample_csv)\n",
    "df.to_csv(mtl_sample_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import os\n",
    "import pandas as pd\n",
    "sequences = ['ATGC' * 128, 'TGAC' * 128, 'GATC' * 128, \"AGCC\" * 128, \"TGGA\" * 128]\n",
    "labels = [''.join(['E' if randint(0, 255) % 2 == 0 else '.' for i in range(len(s))]) for s in sequences]\n",
    "\n",
    "seq2seq_datasample_csv = os.path.join(\"sample\", \"seq2seq\", \"sample.csv\")\n",
    "os.makedirs(os.path.dirname(seq2seq_datasample_csv), exist_ok=True)\n",
    "if os.path.exists(seq2seq_datasample_csv):\n",
    "    os.remove(seq2seq_datasample_csv)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    [[seq, label] for seq, label in zip(sequences, labels)],\n",
    "    columns=['sequence', 'label']\n",
    ")\n",
    "df.to_csv(seq2seq_datasample_csv, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequential_labelling import init_adamw_optimizer, init_seq2seq_model\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "from utils.utils import load_checkpoint\n",
    "\n",
    "model = init_seq2seq_model(pretrained_3kmer_dir)\n",
    "optimizer = init_adamw_optimizer(model.parameters())\n",
    "\n",
    "model, optimizer, config = load_checkpoint(os.path.join(\"result\", \"sample\", \"2022-03-17\", \"checkpoint-4.pth\"), model, optimizer)\n",
    "print(model)\n",
    "print(optimizer)\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get random 100 instance from csv file.\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "src_df = pd.read_csv(os.path.join(\"workspace\", \"mtl\", \"train.sample.csv\"))\n",
    "target_df = src_df.sample(150, random_state=1337)\n",
    "target_df.to_csv(os.path.join(\"workspace\", \"mtl\", \"train.sample.150.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(\"pretrained\", \"3-new-12w-0\"))\n",
    "tokenizer.all_special_tokens\n",
    "tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "gene_table = os.path.join(\"data\", \"genome_ucsc_gene_annotation_tables\", \"genome_ucsc_gene_annotation_tables\")\n",
    "df = pd.read_csv(gene_table)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "gene_annotation = os.path.join(\"data\", \"human\", \"ncbi-genomes-2022-07-21\", \"GCF_000001405.39_GRCh38.p13_genomic.gff\", \"GCF_000001405.39_GRCh38.p13_genomic.with-header.gff\")\n",
    "df = pd.read_csv(gene_annotation, sep='\\t')\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ref_seq_genes = df[(df[\"source\"] == \"BestRefSeq\") & (df[\"type\"] == \"gene\")]\n",
    "best_ref_seq_genes.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adamw\n",
    "\n",
    "optimizer = Adamw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertForTokenClassification, BertConfig\n",
    "pretrained_path = os.path.join(\"pretrained\", \"3-new-12w-0\")\n",
    "\n",
    "# initialize config\n",
    "config = BertConfig.from_pretrained(pretrained_path)\n",
    "\n",
    "# modify config\n",
    "config.architectures[0] = \"BertForTokenClassification\"\n",
    "config.num_labels = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token classification model\n",
    "model = BertForTokenClassification.from_pretrained(pretrained_path, config=config)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import DNABertForTokenClassification\n",
    "\n",
    "m = DNABertForTokenClassification.from_pretrained(pretrained_path, config=config)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input_ids = torch.randint(0, 69, (5, 512))\n",
    "attn_mask = torch.Tensor([[0 for j in range(512)] for i in range(5)])\n",
    "\n",
    "output = m(input_ids, attn_mask)\n",
    "print(output[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM\n",
    "import os\n",
    "\n",
    "pretrained = os.path.join(\"pretrained\", \"3-new-12w-0\")\n",
    "config = BertConfig.from_pretrained(pretrained)\n",
    "model = BertForMaskedLM.from_pretrained(pretrained, config=config)\n",
    "model.bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM\n",
    "import os\n",
    "import json\n",
    "\n",
    "from models.seqlab import DNABERT_SL\n",
    "\n",
    "pretrained = os.path.join(\"pretrained\", \"3-new-12w-0\")\n",
    "config = BertConfig.from_pretrained(pretrained)\n",
    "model = BertForMaskedLM.from_pretrained(pretrained, config=config)\n",
    "bert = model.bert\n",
    "model_config = os.path.join(\"models\", \"config\", \"seqlab\", \"base.json\")\n",
    "config = json.load(open(model_config, \"r\"))\n",
    "model = DNABERT_SL(bert, config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "def merge_kmer_token(value):\n",
    "    from data_preparation import merge_kmer\n",
    "    original_sequence = merge_kmer(\n",
    "        value.split(' ')\n",
    "    )\n",
    "    return original_sequence\n",
    "\n",
    "\n",
    "log_path = os.path.join(\"prediction\", \"error_analysis_log_sorted.csv\")\n",
    "log_df = pd.read_csv(log_path)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    os.path.join(\"pretrained\", \"3-new-12w-0\")\n",
    ")\n",
    "\n",
    "data_dir = os.path.join(\"workspace\", \"seqlab-latest\")\n",
    "train_data_path = os.path.join(data_dir, \"gene_index.01_train_validation_ss_all_pos_train.csv\")\n",
    "train_df = pd.read_csv(train_data_path)\n",
    "validation_data_path = os.path.join(data_dir, \"gene_index.01_train_validation_ss_all_pos_validation.csv\")\n",
    "validation_df = pd.read_csv(validation_data_path)\n",
    "test_data_path = os.path.join(\"workspace\", \"seqlab-latest\", \"gene_index.01_test_ss_all_pos.csv\")\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "# compute cosine similarity worst item in log.\n",
    "train_npy = np.load(os.path.join(\"workspace\", \"seqlab-latest\", \"train_data.npy\"))\n",
    "arr_scores = []\n",
    "arr_max = []\n",
    "arr_min = []\n",
    "arr_mean = []\n",
    "for i, r in tqdm(test_df.iterrows(), total=test_df.shape[0], desc=\"Cosine Similarity\"):\n",
    "    test_input_ids = tokenizer.encode(r[\"sequence\"])\n",
    "    similarity = cosine_similarity([test_input_ids], train_npy)\n",
    "    scores = similarity[0]\n",
    "    arr_max.append(np.max(scores))\n",
    "    arr_min.append(np.min(scores))\n",
    "    arr_mean.append(np.mean(scores))\n",
    "    scores_str = \" \".join([str(a) for a in scores])\n",
    "    arr_scores.append(scores_str)\n",
    "\n",
    "dataframe = pd.DataFrame(data={\n",
    "    # \"score\": arr_scores,\n",
    "    \"max\": arr_max,\n",
    "    \"min\": arr_min,\n",
    "    \"mean\": arr_mean\n",
    "})\n",
    "dataframe.to_csv(\n",
    "    os.path.join(\"error-analysis\", \"data-comparison\", \"cosine_similarity_compact.csv\"), index=False\n",
    ")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity([[1,2,3,4,5]], [[1,3,5,2,4], [2,4,6,8,1]])\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path = os.path.join(\"workspace\\seqlab-latest\\gene_index.01_train_validation_ss_all_pos_validation.csv\")\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_length(row):\n",
    "    seq = row['sequence'].split(' ')\n",
    "    length = len(seq)\n",
    "    return length\n",
    "\n",
    "df[\"length\"] = df.apply(lambda row: compute_length(row), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.unique(df[\"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"CTCCCTGGGAGGGCGTGGATGATGGTGGGAGAGGAGCCCCACTGTGGAAGTCTGACCCCCACATCGCCCCACCTTCCCCAG\"\n",
    "from data_preparation import kmer\n",
    "\n",
    "a = kmer(s, 3, 1)\n",
    "len(a)\n",
    "b = \" \".join(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "ret = tf.keras.utils.to_categorical(2, 3)\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_baseline_basic import preprocessing\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tf_model.wisesty import bilstm\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "work_dir = os.path.join(\"workspace\", \"baseline\", \"basic\")\n",
    "training_data_path = os.path.join(work_dir, \"train_validation_train.csv\")\n",
    "validation_data_path = os.path.join(work_dir, \"train_validation_validation.csv\")\n",
    "test_data_path = os.path.join(work_dir, \"test.csv\")\n",
    "\n",
    "# X_train, Y_train = preprocessing(training_data_path, num_classes=num_classes)\n",
    "X_val, Y_val = preprocessing(validation_data_path, num_classes=num_classes)\n",
    "# X_test, Y_test = preprocessing(test_data_path, num_classes=num_classes)\n",
    "\n",
    "model = bilstm(num_classes=num_classes)\n",
    "\n",
    "print(X_val[0:2], Y_val[0:2])\n",
    "\n",
    "history = model.fit(X_val, Y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "pred = torch.tensor([[0, 0, 0.5, 1]])\n",
    "target = torch.tensor([-100])\n",
    "loss = cross_entropy(pred, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation import kmer\n",
    "\n",
    "s = \"abcdefghijk\"\n",
    "kmer(s, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_csv(os.path.abspath(\"X:/ss-stride.1/gene_test.csv\"))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dna2vec.multi_k_model import MultiKModel\n",
    "import os\n",
    "\n",
    "filepath = os.path.join(\"pretrained\", \"dna2vec\", \"dna2vec.w2v\")\n",
    "mk_model = MultiKModel(filepath)\n",
    "mk_model.vector('AAA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\.virtualenv\\deep-learning\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at pretrained\\3-new-12w-0 were not used when initializing DNABERTLSTMForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DNABERTLSTMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DNABERTLSTMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DNABERTLSTMForTokenClassification were not initialized from the model checkpoint at pretrained\\3-new-12w-0 and are newly initialized: ['rnn.bias_hh_l1', 'rnn.weight_hh_l1_reverse', 'rnn.weight_hh_l1', 'rnn.weight_hh_l0_reverse', 'rnn.weight_ih_l1_reverse', 'rnn.weight_hh_l0', 'rnn.bias_hh_l0', 'head.classifier.bias', 'rnn.weight_ih_l0', 'rnn.weight_ih_l1', 'head.classifier.weight', 'rnn.bias_ih_l1_reverse', 'rnn.bias_ih_l0_reverse', 'rnn.weight_ih_l0_reverse', 'rnn.bias_hh_l1_reverse', 'rnn.bias_hh_l0_reverse', 'rnn.bias_ih_l1', 'rnn.bias_ih_l0', 'head.linear.hidden-block-0.linear.weight', 'head.linear.hidden-block-0.linear.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNABERTLSTMForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(69, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (rnn): LSTM(768, 768, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (head): Head(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear): Sequential(\n",
       "      (hidden-block-0): HeadBlock(\n",
       "        (linear): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (classifier): Linear(in_features=1536, out_features=8, bias=True)\n",
       "  )\n",
       "  (activation): Softmax(dim=2)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.dnabert import DNABERTLSTMForTokenClassification, RNNConfig\n",
    "import os\n",
    "import json\n",
    "\n",
    "pretrained_path = os.path.join(\"pretrained\", \"3-new-12w-0\")\n",
    "lin1_pretrained_path = os.path.join(\"pretrained\", \"dnabert-sl-lin1\")\n",
    "rnn_config = RNNConfig(hidden_size=768, num_layers=2, bidirectional=True)\n",
    "\n",
    "head_config_path = os.path.join(\"models\", \"config\", \"seqlab\", \"base.lin1.json\")\n",
    "head_config = json.load(open(head_config_path, \"r\"))\n",
    "\n",
    "model = DNABERTLSTMForTokenClassification.from_pretrained(pretrained_path, rnn_config, head_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 512, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "input_ids = torch.randint(0, 69, (5, 512))\n",
    "attention_mask = torch.ones(5, 512)\n",
    "\n",
    "output, rnn_output, bert_output = model(input_ids, attention_mask)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sequence-processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14ae8cb2141f3f34f4e0523006ff2d6cb0f7956c0f094e5497e312072e4d0d3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
