{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate poly-A dataset from human data only.\n",
    "Poly-A data is concluded from DeeReCT-PolyA and the DeeReCT-PolyA model uses 5-fold cross validation.\n",
    "Three of them for training, one for validation, and one for testing.\n",
    "For this section, first, second, and third fold are used for training; fourth for validation; and fifth for testing.\n",
    "\n",
    "DeeReCT-PolyA uses multiple datasets: dragon human (Kalkatawi et. al., 2012) and Omni human (Magana-Mora et. al., 2017).\n",
    "Omni dataset is chosen because it's relatively new (2017 vs 2012) and contains more data (Xia et. al., 2018).\n",
    "\"\"\"\n",
    "dragon_human_pos_dir = './data/poly-a/deerectpolya/human/dragon_polyA_data/positive5fold'\n",
    "dragon_human_neg_dir = './data/poly-a/deerectpolya/human/dragon_polyA_data/negative5fold'\n",
    "omni_human_pos_dir = './data/poly-a/deerectpolya/human/omni_polyA_data/positive'\n",
    "omni_human_neg_dir = './data/poly-a/deerectpolya/human/omni_polyA_data/negative'\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, basename\n",
    "\n",
    "pos_dir = omni_human_pos_dir\n",
    "neg_dir = omni_human_neg_dir\n",
    "pos_files = listdir(pos_dir)\n",
    "pos_files = ['{}/{}'.format(pos_dir, a) for a in listdir(pos_dir) if isfile('{}/{}'.format(pos_dir, a))]\n",
    "neg_files = listdir(neg_dir)\n",
    "neg_files = ['{}/{}'.format(neg_dir, a) for a in listdir(neg_dir) if isfile('{}/{}'.format(neg_dir, a))]\n",
    "\n",
    "#print(len(pos_files))\n",
    "#print(len(neg_files))\n",
    "\n",
    "dataset_dir = './dataset/poly-a'\n",
    "pos_dataset_path = '{}/pos_polya.csv'.format(dataset_dir)\n",
    "neg_dataset_path = '{}/neg_polya.csv'.format(dataset_dir)\n",
    "\n",
    "files = [(pos_files, pos_dataset_path, '1'), (neg_files, neg_dataset_path, '0')]\n",
    "for p in files:\n",
    "    fs = p[0]\n",
    "    dataset_path = p[1]\n",
    "    label = p[2]\n",
    "\n",
    "    if os.path.exists(dataset_path):\n",
    "        os.remove(dataset_path)\n",
    "    t = open(dataset_path, 'x')\n",
    "    t.write('{}\\n'.format(','.join(['sequence', 'label'])))\n",
    "    for fpath in fs:\n",
    "        f = {}\n",
    "        try:\n",
    "            f = open(fpath, 'r')\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                t.write('{},{}\\n'.format(line, label))\n",
    "        except Exception as e:\n",
    "            print('Error {}'.format(e))\n",
    "            f.close()\n",
    "        finally:\n",
    "            f.close()\n",
    "\n",
    "    t.close()\n",
    "\n",
    "dataset_dir = './dataset/poly-a'\n",
    "pos_dataset_path = '{}/pos_polya.csv'.format(dataset_dir)\n",
    "neg_dataset_path = '{}/neg_polya.csv'.format(dataset_dir)\n",
    "\n",
    "n_sample = 100\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "polya_pos_df = pd.read_csv(pos_dataset_path)\n",
    "polya_neg_df = pd.read_csv(neg_dataset_path)\n",
    "\n",
    "sample_polya_pos_df = polya_pos_df.sample(n=n_sample, random_state=1337)\n",
    "sample_polya_neg_df = polya_pos_df.sample(n=n_sample, random_state=1337)\n",
    "\n",
    "pos_train_df = sample_polya_pos_df.sample(frac=0.8, random_state=1337)\n",
    "neg_train_df = sample_polya_neg_df.sample(frac=0.8, random_state=1337)\n",
    "pos_val_df = sample_polya_pos_df.drop(pos_train_df.index)\n",
    "neg_val_df = sample_polya_neg_df.drop(neg_train_df.index)\n",
    "pos_test_df = pos_val_df.sample(frac=0.5, random_state=1337)\n",
    "neg_test_df = neg_val_df.sample(frac=0.5, random_state=1337)\n",
    "pos_val_df = pos_val_df.drop(pos_test_df.index)\n",
    "neg_val_df = neg_val_df.drop(neg_test_df.index)\n",
    "\n",
    "# Merge pos & neg train, val, and test data.\n",
    "polya_sample_train = pos_train_df.append(neg_train_df)\n",
    "polya_sample_val = pos_val_df.append(neg_val_df)\n",
    "polya_sample_test = pos_test_df.append(neg_test_df)\n",
    "\n",
    "# Write those datasets.\n",
    "polya_sample_train.to_csv('./sample/polya/training_sample.csv')\n",
    "polya_sample_val.to_csv('./sample/polya/validation_sample.csv')\n",
    "polya_sample_test.to_csv('./sample/polya/test_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both are 18786. data balance.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Split positive and negative polya data into three parts for training, validation, and test set.\n",
    "Process both data using pandas.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import os\n",
    "from data_preparation import generate_datasets, generate_sample\n",
    "\n",
    "polya_dataset_dir = './dataset/polya'\n",
    "pos_polya_dataset = '{}/pos_polya.csv'.format(polya_dataset_dir)\n",
    "neg_polya_dataset = '{}/neg_polya.csv'.format(polya_dataset_dir)\n",
    "pos_polya_balanced = '{}/pos_polya_balanced.csv'.format(polya_dataset_dir)\n",
    "neg_polya_balanced = '{}/neg_polya_balanced.csv'.format(polya_dataset_dir)\n",
    "pos_polya_dataset_dir = '{}/pos_polya_dataset'.format(polya_dataset_dir)\n",
    "neg_polya_dataset_dir = '{}/neg_polya_dataset'.format(polya_dataset_dir)\n",
    "\n",
    "pos_polya_df = pd.read_csv(pos_polya_dataset)\n",
    "neg_polya_df = pd.read_csv(neg_polya_dataset)\n",
    "pos_polya_df_size = len(pos_polya_df)\n",
    "neg_polya_df_size = len(neg_polya_df)\n",
    "\n",
    "# Check if both positive and negative polya datasets are balanced.\n",
    "# If not, make it balanced.\n",
    "count = 0\n",
    "if pos_polya_df_size == neg_polya_df_size:\n",
    "    print('both are {}. data balance.'.format(pos_polya_df_size))\n",
    "    count = pos_polya_df_size\n",
    "else:\n",
    "    count = pos_polya_df_size if pos_polya_df_size < neg_polya_df_size else neg_polya_df_size\n",
    "    print('data imbalance at pos = {} and neg = {}.\\nSelect count = {}.'.format(pos_polya_df_size, neg_polya_df_size, count))\n",
    "\n",
    "pos_polya_df_balanced = pos_polya_df.sample(n=count, random_state=1337)\n",
    "neg_polya_df_balanced = neg_polya_df.sample(n=count, random_state=1337)\n",
    "pos_polya_df_balanced.to_csv(pos_polya_balanced, index=False)\n",
    "neg_polya_df_balanced.to_csv(neg_polya_balanced, index=False)\n",
    "\n",
    "# Split positive data into three parts.\n",
    "generate_datasets(pos_polya_balanced, pos_polya_dataset_dir)\n",
    "generate_datasets(neg_polya_balanced, neg_polya_dataset_dir)\n",
    "\n",
    "polya_dataset_dirs = [pos_polya_dataset_dir, neg_polya_dataset_dir]\n",
    "files = ['train', 'validation', 'test']\n",
    "n_sample = {\n",
    "    'train': 80,\n",
    "    'validation': 10,\n",
    "    'test': 10\n",
    "}\n",
    "for d in polya_dataset_dirs:\n",
    "    for f in files:\n",
    "        fpath = '{}/{}.csv'.format(d, f)\n",
    "        gpath = '{}/{}_sample.csv'.format(d, f)\n",
    "        if (os.path.exists(gpath)):\n",
    "            os.remove(gpath)\n",
    "        g = open(gpath, 'x')\n",
    "        g.write('{},{}\\n'.format('sequence', 'label'))\n",
    "        df = pd.read_csv(fpath)\n",
    "        sample_df = df.sample(n=n_sample[f], random_state=1337)\n",
    "\n",
    "        for i, r in sample_df.iterrows():\n",
    "            seq = r['sequence']\n",
    "            label = r['label']\n",
    "            if len(seq) > 512:\n",
    "                kmers = [seq[i:i+512] for i in range(len(seq)+1-512)]\n",
    "                for mer in kmers:\n",
    "                    g.write('{},{}\\n'.format(mer, label))\n",
    "            else:\n",
    "                g.write('{},{}\\n'.format(seq, label))\n",
    "        g.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create splice-site dataset.\n",
    "\"\"\"\n",
    "from os.path import basename\n",
    "\n",
    "ss_dir = './data/splice-sites/splice-deep/'\n",
    "pos_acc_ss_hs = '{}/positive_DNA_seqs_acceptor_hs.fa'.format(ss_dir)\n",
    "pos_don_ss_hs = '{}/positive_DNA_seqs_donor_hs.fa'.format(ss_dir)\n",
    "neg_acc_ss_hs = '{}/negative_DNA_seqs_acceptor_hs.fa'.format(ss_dir)\n",
    "neg_don_ss_hs = '{}/negative_DNA_seqs_donor_hs.fa'.format(ss_dir)\n",
    "\n",
    "ss_dataset_dir = './dataset/splice-sites'\n",
    "pos_ss_acc_dataset = '{}/pos_ss_acc_hs.csv'.format(ss_dataset_dir)\n",
    "pos_ss_don_dataset = '{}/pos_ss_don_hs.csv'.format(ss_dataset_dir)\n",
    "neg_ss_acc_dataset = '{}/neg_ss_acc_hs.csv'.format(ss_dataset_dir)\n",
    "neg_ss_don_dataset = '{}/neg_ss_don_hs.csv'.format(ss_dataset_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [(pos_acc_ss_hs, 1, 'acc', pos_ss_acc_dataset), \n",
    "            (pos_don_ss_hs, 1, 'don', pos_ss_don_dataset), \n",
    "            (neg_acc_ss_hs, 0, 'acc', neg_ss_acc_dataset), \n",
    "            (neg_don_ss_hs, 0, 'don', neg_ss_don_dataset)]\n",
    "for p in files:\n",
    "    fname = p[0]\n",
    "    label = p[1]\n",
    "    acc_don = p[2]\n",
    "    dataset_path = p[3]\n",
    "\n",
    "    f = {}\n",
    "    t = {}\n",
    "    if os.path.exists(dataset_path):\n",
    "        os.remove(dataset_path)\n",
    "    try:\n",
    "        f = open(fname, 'r')\n",
    "        t = open(dataset_path, 'x')\n",
    "        t.write('{}\\n'.format(','.join(['sequence', 'label'])))\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            #if len(line) > 512:\n",
    "                #kmers = [line[i:i+512] for i in range(0, len(line)+1-512)] # Make sure everything is 512 character.\n",
    "                #for mer in kmers:\n",
    "                #    t.write('{},{}\\n'.format(mer, label))        \n",
    "            #else:\n",
    "            t.write('{},{}\\n'.format(line, label))\n",
    "        t.close()\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        print('Error {}'.format(e))\n",
    "        t.close()\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create train, validation, and test set for splice site. To do that, the data need to be balance.\n",
    "If not the sampling based on smallest count is required. Processing is done using pandas.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "pos_ss_acc_df = pd.read_csv(pos_ss_acc_dataset)\n",
    "pos_ss_don_df = pd.read_csv(pos_ss_don_dataset)\n",
    "neg_ss_acc_df = pd.read_csv(neg_ss_acc_dataset)\n",
    "neg_ss_don_df = pd.read_csv(neg_ss_don_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset imbalance\n",
      "pos acc 248150\n",
      "pos don 250400\n",
      "neg acc 248150\n",
      "neg don 250400\n",
      "count = 248150\n"
     ]
    }
   ],
   "source": [
    "# Because loading the dataframe is time consuming, leave the loading at cell above and do later processing here.\n",
    "pos_ss_acc_size = len(pos_ss_acc_df)\n",
    "pos_ss_don_size = len(pos_ss_don_df)\n",
    "neg_ss_acc_size = len(neg_ss_acc_df)\n",
    "neg_ss_don_size = len(neg_ss_don_df)\n",
    "\n",
    "count = 0\n",
    "if pos_ss_acc_size == pos_ss_don_size == neg_ss_acc_size == neg_ss_don_size:\n",
    "    print('dataset balance')\n",
    "    count = pos_ss_acc_size\n",
    "else:\n",
    "    print('dataset imbalance')\n",
    "    print('pos acc {}\\npos don {}\\nneg acc {}\\nneg don {}'.format(pos_ss_acc_size, pos_ss_don_size, neg_ss_acc_size, neg_ss_don_size))\n",
    "    count = min([pos_ss_acc_size, pos_ss_don_size, neg_ss_acc_size, neg_ss_don_size])\n",
    "    print('count = {}'.format(count))\n",
    "\n",
    "pos_ss_acc_df_sample = pos_ss_acc_df.sample(n=count, replace=False, random_state=1337)\n",
    "pos_ss_don_df_sample = pos_ss_don_df.sample(n=count, replace=False, random_state=1337)\n",
    "neg_ss_acc_df_sample = neg_ss_acc_df.sample(n=count, replace=False, random_state=1337)\n",
    "neg_ss_don_df_sample = neg_ss_don_df.sample(n=count, replace=False, random_state=1337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ss_acc_balanced = './dataset/splice-sites/pos_ss_acc_balanced.csv'\n",
    "pos_ss_don_balanced = './dataset/splice-sites/pos_ss_don_balanced.csv'\n",
    "neg_ss_acc_balanced = './dataset/splice-sites/neg_ss_acc_balanced.csv'\n",
    "neg_ss_don_balanced = './dataset/splice-sites/neg_ss_don_balanced.csv'\n",
    "\n",
    "pos_ss_acc_df_sample.to_csv(pos_ss_acc_balanced, index=False)\n",
    "pos_ss_don_df_sample.to_csv(pos_ss_don_balanced, index=False)\n",
    "neg_ss_acc_df_sample.to_csv(neg_ss_acc_balanced, index=False)\n",
    "neg_ss_don_df_sample.to_csv(neg_ss_don_balanced, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./dataset/splice-sites/pos_ss_acc_dataset/train.csv', './dataset/splice-sites/pos_ss_acc_dataset/validation.csv', './dataset/splice-sites/pos_ss_acc_dataset/test.csv']\n",
      "['./dataset/splice-sites/pos_ss_don_dataset/train.csv', './dataset/splice-sites/pos_ss_don_dataset/validation.csv', './dataset/splice-sites/pos_ss_don_dataset/test.csv']\n",
      "['./dataset/splice-sites/neg_ss_acc_dataset/train.csv', './dataset/splice-sites/neg_ss_acc_dataset/validation.csv', './dataset/splice-sites/neg_ss_acc_dataset/test.csv']\n",
      "['./dataset/splice-sites/neg_ss_don_dataset/train.csv', './dataset/splice-sites/neg_ss_don_dataset/validation.csv', './dataset/splice-sites/neg_ss_don_dataset/test.csv']\n"
     ]
    }
   ],
   "source": [
    "from data_preparation import generate_datasets\n",
    "\n",
    "pos_ss_acc_balanced_dir = './dataset/splice-sites/pos_ss_acc_dataset'\n",
    "pos_ss_don_balanced_dir = './dataset/splice-sites/pos_ss_don_dataset'\n",
    "neg_ss_acc_balanced_dir = './dataset/splice-sites/neg_ss_acc_dataset'\n",
    "neg_ss_don_balanced_dir = './dataset/splice-sites/neg_ss_don_dataset'\n",
    "\n",
    "dirs = [pos_ss_acc_balanced_dir, pos_ss_don_balanced_dir, neg_ss_acc_balanced_dir, neg_ss_don_balanced_dir]\n",
    "for d in dirs:\n",
    "    if os.path.exists(d):\n",
    "        os.mkdir(d)\n",
    "\n",
    "pos_ss_acc_dataset = generate_datasets(pos_ss_acc_balanced, pos_ss_acc_balanced_dir)\n",
    "pos_ss_don_dataset = generate_datasets(pos_ss_don_balanced, pos_ss_don_balanced_dir)\n",
    "neg_ss_acc_dataset = generate_datasets(neg_ss_acc_balanced, neg_ss_acc_balanced_dir)\n",
    "neg_ss_don_dataset = generate_datasets(neg_ss_don_balanced, neg_ss_don_balanced_dir)\n",
    "\n",
    "print(pos_ss_acc_dataset)\n",
    "print(pos_ss_don_dataset)\n",
    "print(neg_ss_acc_dataset)\n",
    "print(neg_ss_don_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation import generate_sample\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "Create sample splice sites training and validation data.\n",
    "\"\"\"\n",
    "pos_ss_acc_balanced_dir = './dataset/splice-sites/pos_ss_acc_dataset'\n",
    "pos_ss_don_balanced_dir = './dataset/splice-sites/pos_ss_don_dataset'\n",
    "neg_ss_acc_balanced_dir = './dataset/splice-sites/neg_ss_acc_dataset'\n",
    "neg_ss_don_balanced_dir = './dataset/splice-sites/neg_ss_don_dataset'\n",
    "\n",
    "n_sample = {\n",
    "    'train': 80,\n",
    "    'validation': 10,\n",
    "    'test': 10\n",
    "}\n",
    "ss_dir = [pos_ss_acc_balanced_dir, pos_ss_don_balanced_dir, neg_ss_acc_balanced_dir, neg_ss_don_balanced_dir]\n",
    "ss_file = ['train', 'validation', 'test']\n",
    "\n",
    "for d in ss_dir:\n",
    "    for s in ss_file:\n",
    "        filepath = '{}/{}.csv'.format(d, s)\n",
    "        targetpath = '{}/{}_sample.csv'.format(d, s)\n",
    "        df = pd.read_csv(filepath)\n",
    "        sample_df = df.sample(n=n_sample[s], random_state=1337)\n",
    "        sample_df.to_csv(targetpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence length is more than 512 characters so it needs to be expanded.\n",
    "import pandas as pd\n",
    "pos_ss_acc_balanced_dir = './dataset/splice-sites/pos_ss_acc_dataset'\n",
    "pos_ss_don_balanced_dir = './dataset/splice-sites/pos_ss_don_dataset'\n",
    "neg_ss_acc_balanced_dir = './dataset/splice-sites/neg_ss_acc_dataset'\n",
    "neg_ss_don_balanced_dir = './dataset/splice-sites/neg_ss_don_dataset'\n",
    "\n",
    "n_sample = {\n",
    "    'train': 80,\n",
    "    'validation': 10,\n",
    "    'test': 10\n",
    "}\n",
    "ss_dir = [pos_ss_acc_balanced_dir, pos_ss_don_balanced_dir, neg_ss_acc_balanced_dir, neg_ss_don_balanced_dir]\n",
    "ss_file = ['train', 'validation', 'test']\n",
    "\n",
    "for s in ss_dir:\n",
    "    for f in ss_file:\n",
    "        fpath = '{}/{}_sample.csv'.format(s, f)\n",
    "        gpath = '{}/{}_sample_expanded.csv'.format(s, f)\n",
    "        if os.path.exists(gpath):\n",
    "            os.remove(gpath)\n",
    "        g = open(gpath, 'x')\n",
    "        g.write('{},{}\\n'.format('sequence', 'label'))\n",
    "\n",
    "        df = pd.read_csv(fpath)\n",
    "        for i, r in df.iterrows():\n",
    "            seq = r['sequence']\n",
    "            label = r['label']\n",
    "            if len(seq)> 512:\n",
    "                kmers = [seq[i:i+512] for i in range(len(seq)+1-512)]\n",
    "                for mer in kmers:\n",
    "                    g.write('{},{}\\n'.format(mer, label))\n",
    "            else:\n",
    "                g.write('{},{}\\n'.format(seq, label))\n",
    "\n",
    "        g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./dataset/promoter/neg_human_prom_dataset/train.csv',\n",
       " './dataset/promoter/neg_human_prom_dataset/validation.csv',\n",
       " './dataset/promoter/neg_human_prom_dataset/test.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generating promoter dataset from EPD datasets (Dreos et. al., 2013).\n",
    "`human_tata_data` and `human_non_tata_data` are already expanded.\n",
    "From each positive and negative set, generate train, validation, and test set.\n",
    "\"\"\"\n",
    "from data_preparation import generate_datasets, generate_sample\n",
    "import pandas as pd\n",
    "\n",
    "human_tata_data = './data/epd/human_tata.csv'\n",
    "human_non_tata_data = './data/epd/human_non_tata.csv'\n",
    "\n",
    "human_prom_dataset_dir = './dataset/promoter'\n",
    "human_prom_pos_dataset_dir = '{}/pos_human_prom_dataset'.format(human_prom_dataset_dir)\n",
    "human_prom_neg_dataset_dir = '{}/neg_human_prom_dataset'.format(human_prom_dataset_dir)\n",
    "\n",
    "generate_datasets(human_tata_data, human_prom_pos_dataset_dir)\n",
    "generate_datasets(human_non_tata_data, human_prom_neg_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate sample from each train, validation, and test from positive and negative promoter set.\n",
    "i.e. train.csv => train_sample.csv\n",
    "\"\"\"\n",
    "from data_preparation import generate_sample\n",
    "import pandas as pd\n",
    "\n",
    "n_sample = {\n",
    "    'train': 80,\n",
    "    'validation': 10,\n",
    "    'test': 10\n",
    "}\n",
    "\n",
    "human_prom_dataset_dir = './dataset/promoter'\n",
    "human_prom_pos_dataset_dir = '{}/pos_human_prom_dataset'.format(human_prom_dataset_dir)\n",
    "human_prom_neg_dataset_dir = '{}/neg_human_prom_dataset'.format(human_prom_dataset_dir)\n",
    "\n",
    "prom_dir = [human_prom_pos_dataset_dir, human_prom_neg_dataset_dir]\n",
    "prom_file = ['train', 'validation', 'test']\n",
    "\n",
    "for p in prom_dir:\n",
    "    for f in prom_file:\n",
    "        fpath = '{}/{}.csv'.format(p, f)\n",
    "        gpath = '{}/{}_sample.csv'.format(p, f)\n",
    "        generate_sample(fpath, gpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate sample training and testing data from promoter, splice-sites, and poly-a.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "dataset_dir = './dataset'\n",
    "dataset_train_sample = '{}/train_sample.csv'.format(dataset_dir)\n",
    "dataset_validation_sample = '{}/validation_sample.csv'.format(dataset_dir)\n",
    "dataset_test_sample = '{}/test_sample.csv'.format(dataset_dir)\n",
    "\n",
    "prom_dataset_dir = '{}/promoter'.format(dataset_dir)\n",
    "prom_dataset_pos_dir = '{}/pos_human_prom_dataset'.format(prom_dataset_dir)\n",
    "prom_dataset_pos_train_sample = '{}/train_sample.csv'.format(prom_dataset_pos_dir)\n",
    "prom_dataset_pos_validation_sample = '{}/validation_sample.csv'.format(prom_dataset_pos_dir)\n",
    "prom_dataset_pos_test_sample = '{}/test_sample.csv'.format(prom_dataset_pos_dir)\n",
    "\n",
    "prom_dataset_neg_dir = '{}/neg_human_prom_dataset'.format(prom_dataset_dir)\n",
    "prom_dataset_neg_train_sample = '{}/train_sample.csv'.format(prom_dataset_neg_dir)\n",
    "prom_dataset_neg_validation_sample = '{}/validation_sample.csv'.format(prom_dataset_neg_dir)\n",
    "prom_dataset_neg_test_sample = '{}/test_sample.csv'.format(prom_dataset_neg_dir)\n",
    "\n",
    "polya_dataset_dir = '{}/polya'.format(dataset_dir)\n",
    "polya_dataset_pos_dir = '{}/pos_polya_dataset'.format(polya_dataset_dir)\n",
    "polya_dataset_pos_train_sample = '{}/train_sample.csv'.format(polya_dataset_pos_dir)\n",
    "polya_dataset_pos_validation_sample = '{}/validation_sample.csv'.format(polya_dataset_pos_dir)\n",
    "polya_dataset_pos_test_sample = '{}/test_sample.csv'.format(polya_dataset_pos_dir)\n",
    "\n",
    "polya_dataset_neg_dir = '{}/neg_polya_dataset'.format(polya_dataset_dir)\n",
    "polya_dataset_neg_train_sample = '{}/train_sample.csv'.format(polya_dataset_neg_dir)\n",
    "polya_dataset_neg_validation_sample = '{}/validation_sample.csv'.format(polya_dataset_neg_dir)\n",
    "polya_dataset_neg_test_sample = '{}/test_sample.csv'.format(polya_dataset_neg_dir)\n",
    "\n",
    "ss_dataset_dir = '{}/splice-sites'.format(dataset_dir)\n",
    "ss_dataset_acc_pos_dir = '{}/pos_ss_acc_dataset'.format(ss_dataset_dir)\n",
    "ss_dataset_acc_pos_train_sample = '{}/train_sample_expanded.csv'.format(ss_dataset_acc_pos_dir)\n",
    "ss_dataset_acc_pos_validation_sample = '{}/validation_sample_expanded.csv'.format(ss_dataset_acc_pos_dir)\n",
    "ss_dataset_acc_pos_test_sample = '{}/test_sample_expanded.csv'.format(ss_dataset_acc_pos_dir)\n",
    "\n",
    "ss_dataset_don_pos_dir = '{}/pos_ss_don_dataset'.format(ss_dataset_dir)\n",
    "ss_dataset_don_pos_train_sample = '{}/train_sample_expanded.csv'.format(ss_dataset_don_pos_dir)\n",
    "ss_dataset_don_pos_validation_sample = '{}/validation_sample_expanded.csv'.format(ss_dataset_don_pos_dir)\n",
    "ss_dataset_don_pos_test_sample = '{}/test_sample_expanded.csv'.format(ss_dataset_don_pos_dir)\n",
    "\n",
    "ss_dataset_acc_neg_dir = '{}/neg_ss_acc_dataset'.format(ss_dataset_dir)\n",
    "ss_dataset_acc_neg_train_sample = '{}/train_sample_expanded.csv'.format(ss_dataset_acc_neg_dir)\n",
    "ss_dataset_acc_neg_validation_sample = '{}/validation_sample_expanded.csv'.format(ss_dataset_acc_neg_dir)\n",
    "ss_dataset_acc_neg_test_sample = '{}/test_sample_expanded.csv'.format(ss_dataset_acc_neg_dir)\n",
    "\n",
    "ss_dataset_don_neg_dir = '{}/neg_ss_don_dataset'.format(ss_dataset_dir)\n",
    "ss_dataset_don_neg_train_sample = '{}/train_sample_expanded.csv'.format(ss_dataset_don_neg_dir)\n",
    "ss_dataset_don_neg_validation_sample = '{}/validation_sample_expanded.csv'.format(ss_dataset_don_neg_dir)\n",
    "ss_dataset_don_neg_test_sample = '{}/test_sample_expanded.csv'.format(ss_dataset_don_neg_dir)\n",
    "\n",
    "columns = ['sequence','label_prom','label_ss', 'label_polya']\n",
    "header = ','.join(columns)\n",
    "\n",
    "target_paths = [dataset_train_sample, dataset_validation_sample, dataset_test_sample]\n",
    "files = ['train_sample', 'validation_sample', 'test_sample']\n",
    "for t in target_paths:\n",
    "    if os.path.exists(t):\n",
    "        os.remove(t)\n",
    "\n",
    "# Merge promoter.\n",
    "prom_dfps = [\n",
    "    (prom_dataset_pos_train_sample, dataset_train_sample),\n",
    "    (prom_dataset_pos_validation_sample, dataset_validation_sample), \n",
    "    (prom_dataset_pos_test_sample, dataset_test_sample),\n",
    "    (prom_dataset_neg_train_sample, dataset_train_sample),\n",
    "    (prom_dataset_neg_validation_sample, dataset_validation_sample),\n",
    "    (prom_dataset_neg_test_sample, dataset_test_sample)\n",
    "]\n",
    "\n",
    "ss_dfps = [\n",
    "    (ss_dataset_acc_pos_train_sample, dataset_train_sample), \n",
    "    (ss_dataset_acc_pos_validation_sample, dataset_validation_sample),\n",
    "    (ss_dataset_acc_pos_test_sample, dataset_test_sample),\n",
    "    (ss_dataset_acc_neg_train_sample, dataset_train_sample),\n",
    "    (ss_dataset_acc_neg_validation_sample, dataset_validation_sample),\n",
    "    (ss_dataset_acc_neg_test_sample, dataset_test_sample),\n",
    "    (ss_dataset_don_pos_train_sample, dataset_train_sample), \n",
    "    (ss_dataset_don_pos_validation_sample, dataset_validation_sample),\n",
    "    (ss_dataset_don_pos_test_sample, dataset_test_sample),\n",
    "    (ss_dataset_don_neg_train_sample, dataset_train_sample),\n",
    "    (ss_dataset_don_neg_validation_sample, dataset_validation_sample),\n",
    "    (ss_dataset_don_neg_test_sample, dataset_test_sample),\n",
    "]\n",
    "\n",
    "polya_dfps = [\n",
    "    (polya_dataset_pos_train_sample, dataset_train_sample),\n",
    "    (polya_dataset_pos_validation_sample, dataset_validation_sample),\n",
    "    (polya_dataset_pos_test_sample, dataset_test_sample),\n",
    "    (polya_dataset_neg_train_sample, dataset_train_sample),\n",
    "    (polya_dataset_neg_validation_sample, dataset_validation_sample),\n",
    "    (polya_dataset_neg_test_sample, dataset_test_sample),\n",
    "]\n",
    "\n",
    "def merge_dataset(dfps, target_label=\"\"):\n",
    "    for dfp in dfps:\n",
    "        src_path = dfp[0]\n",
    "        target_path = dfp[1]\n",
    "        src_df = pd.read_csv(src_path)\n",
    "        target_df = {}\n",
    "        if os.path.exists(target_path):\n",
    "            target_df = pd.read_csv(target_path)\n",
    "        else:\n",
    "            target_df = pd.DataFrame(columns=columns)\n",
    "        for i, r in src_df.iterrows():\n",
    "            row = {\n",
    "                'sequence': r['sequence'],\n",
    "                'label_prom': r['label'] if target_label == \"prom\" else 0,\n",
    "                'label_ss': r['label'] if target_label == \"ss\" else 0,\n",
    "                'label_polya': r['label'] if target_label == \"polya\" else 0\n",
    "            }\n",
    "            target_df = target_df.append(row, ignore_index=True)\n",
    "        target_df.to_csv(target_path, index=False)\n",
    "\n",
    "merge_dataset(prom_dfps, target_label=\"prom\")\n",
    "merge_dataset(ss_dfps, target_label=\"ss\")\n",
    "merge_dataset(polya_dfps, target_label=\"polya\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7170ee380771f2003055f3cee3e2e4dd0d81d1dac73a8c82197236b1572f37c2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('sequence-processing': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
