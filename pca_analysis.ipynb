{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing Data test: 100%|██████████| 6961/6961 [00:55<00:00, 125.73it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import os\n",
    "from utils.seqlab import preprocessing_kmer\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda:0\"\n",
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(\"pretrained\", \"3-new-12w-0\"))\n",
    "test_file = os.path.join(\"workspace\", \"seqlab-latest\", \"test.csv\")\n",
    "batch_size = 8\n",
    "test_dataloader = preprocessing_kmer(test_file, tokenizer, batch_size)\n",
    "test_size = len(test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pretrained\\3-new-12w-0 were not used when initializing DNABERT_SL: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DNABERT_SL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DNABERT_SL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DNABERT_SL were not initialized from the model checkpoint at pretrained\\3-new-12w-0 and are newly initialized: ['head.classifier.bias', 'head.linear.hidden-block-0.linear.weight', 'head.linear.hidden-block-0.linear.bias', 'head.classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# somehow I do PCA analysis.\n",
    "from models import seqlab, pretrained\n",
    "from transformers import BertForMaskedLM\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "m = seqlab.DNABERT_SL(\n",
    "    BertForMaskedLM.from_pretrained(os.path.join(\"pretrained\", \"3-new-12w-0\")).bert,\n",
    "    json.load(open(os.path.join(\"models\", \"config\", \"seqlab\", \"base.lin1.json\"), \"r\"))\n",
    ")\n",
    "n = pretrained.DNABERT_SL.from_pretrained(\n",
    "    os.path.join(\"pretrained\", \"3-new-12w-0\"),\n",
    "    json.load(open(os.path.join(\"models\", \"config\", \"seqlab\", \"base.lin1.json\"), \"r\"))\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(\n",
    "    os.path.join(\"run\", \"sso01-adamw-lr5e-5-base.lin1-2w1boplw\", \"latest\", \"checkpoint.pth\"), \n",
    "    map_location=\"cuda:0\") # force to cuda:0 device\n",
    "m.load_state_dict(checkpoint.get(\"model\"))\n",
    "n.load_state_dict(checkpoint.get(\"model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.save_pretrained(os.path.join(\"pretrained\", \"dnabert-sl-lin1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.train()\n",
    "n.train()\n",
    "str(m.state_dict()) == str(n.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 871/871 [1:28:01<00:00,  6.06s/it]   \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>target_id</th>\n",
       "      <th>bert_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>-100</td>\n",
       "      <td>1.148287057876587 1.622141718864441 -1.0630100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>0.2518802285194397 1.3096948862075806 0.932622...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>0.2287495732307434 1.3885688781738281 0.312197...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.41064363718032837 1.1078848838806152 0.12063...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>0.8710600137710571 1.133277416229248 0.5390868...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token_id  target_id                                         bert_value\n",
       "0         2       -100  1.148287057876587 1.622141718864441 -1.0630100...\n",
       "1        29          7  0.2518802285194397 1.3096948862075806 0.932622...\n",
       "2        38          7  0.2287495732307434 1.3885688781738281 0.312197...\n",
       "3        10          7  0.41064363718032837 1.1078848838806152 0.12063...\n",
       "4        28          7  0.8710600137710571 1.133277416229248 0.5390868..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from utils.seqlab import id2label\n",
    "import pandas as pd\n",
    "\n",
    "m.eval()\n",
    "# n.eval()\n",
    "m.to(device)\n",
    "# n.to(device)\n",
    "token_ids, target_ids, bert_outputs = [], [], []\n",
    "for step, batch in tqdm(enumerate(test_dataloader), total=test_size, desc=\"Testing\"):\n",
    "    input_ids, attn_mask, token_type_ids, target_labels = tuple(t.to(device) for t in batch)\n",
    "    with torch.no_grad():\n",
    "        out, out_bert, out_head = m(input_ids, attn_mask)\n",
    "        for b_input_ids, b_target, b_bert in zip(input_ids, target_labels, out_bert):\n",
    "            for i, j, k in zip(b_input_ids, b_target, b_bert):\n",
    "                token_ids.append(i.item())\n",
    "                target_ids.append(j.item())\n",
    "                bert_outputs.append(\" \".join([str(a) for a in k.tolist()]))\n",
    "\n",
    "df = pd.DataFrame(data={\n",
    "    \"token_id\": token_ids,\n",
    "    \"target_id\": target_ids,\n",
    "    \"bert_value\": bert_outputs\n",
    "})\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(\"motif_analysis\", \"token_analysis\", \"token_bert_value.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNABERT_SL(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(69, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Head(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear): Sequential(\n",
       "      (hidden-block-0): HeadBlock(\n",
       "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
       "  )\n",
       "  (activation): Softmax(dim=2)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_sequence_id</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>prediction_ids</th>\n",
       "      <th>target_ids</th>\n",
       "      <th>step</th>\n",
       "      <th>f1_score-iii</th>\n",
       "      <th>precision-iii</th>\n",
       "      <th>recall-iii</th>\n",
       "      <th>f1_score-iiE</th>\n",
       "      <th>precision-iiE</th>\n",
       "      <th>...</th>\n",
       "      <th>prediction_tokens</th>\n",
       "      <th>target_tokens</th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "      <th>nucl_precision_intron</th>\n",
       "      <th>nucl_precision_exon</th>\n",
       "      <th>nucl_recall_intron</th>\n",
       "      <th>nucl_recall_exon</th>\n",
       "      <th>nucl_f1_intron</th>\n",
       "      <th>nucl_f1_exon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>491</td>\n",
       "      <td>ATG TGA GAA AAT ATC TCC CCA CAC ACT CTC TCA CA...</td>\n",
       "      <td>7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 ...</td>\n",
       "      <td>7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 ...</td>\n",
       "      <td>1301</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.858</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EE...</td>\n",
       "      <td>EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EE...</td>\n",
       "      <td>EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE...</td>\n",
       "      <td>EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE...</td>\n",
       "      <td>0.860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>515</td>\n",
       "      <td>CCA CAC ACT CTC TCA CAA AAA AAT ATG TGT GTT TT...</td>\n",
       "      <td>7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 ...</td>\n",
       "      <td>7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 ...</td>\n",
       "      <td>1355</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.862</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EE...</td>\n",
       "      <td>EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EE...</td>\n",
       "      <td>EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEiiiiii...</td>\n",
       "      <td>EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEiiiiii...</td>\n",
       "      <td>0.863</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>517</td>\n",
       "      <td>GCC CCA CAT ATC TCC CCC CCA CAG AGC GCC CCA CA...</td>\n",
       "      <td>7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 ...</td>\n",
       "      <td>7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 ...</td>\n",
       "      <td>1357</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EE...</td>\n",
       "      <td>EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EE...</td>\n",
       "      <td>EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEiiiiii...</td>\n",
       "      <td>EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEiiiiii...</td>\n",
       "      <td>0.989</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>561</td>\n",
       "      <td>GCC CCA CAT ATG TGA GAC ACA CAA AAC ACA CAG AG...</td>\n",
       "      <td>7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 ...</td>\n",
       "      <td>7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 ...</td>\n",
       "      <td>1451</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.973</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EE...</td>\n",
       "      <td>EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EE...</td>\n",
       "      <td>EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEiiiiiiiiiiiiiii...</td>\n",
       "      <td>EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEiiiiiiiiiiiiiii...</td>\n",
       "      <td>0.969</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>670</td>\n",
       "      <td>ACA CAT ATT TTA TAT ATC TCA CAG AGG GGT GTA TA...</td>\n",
       "      <td>7 7 7 7 7 7 7 7 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>7 7 7 7 7 7 7 7 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>1702</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>EEE EEE EEE EEE EEE EEE EEE EEE EEi Eii iii ii...</td>\n",
       "      <td>EEE EEE EEE EEE EEE EEE EEE EEE EEi Eii iii ii...</td>\n",
       "      <td>EEEEEEEEEEiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii...</td>\n",
       "      <td>EEEEEEEEEEiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii...</td>\n",
       "      <td>0.992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_sequence_id                                       input_tokens  \\\n",
       "0                491  ATG TGA GAA AAT ATC TCC CCA CAC ACT CTC TCA CA...   \n",
       "1                515  CCA CAC ACT CTC TCA CAA AAA AAT ATG TGT GTT TT...   \n",
       "2                517  GCC CCA CAT ATC TCC CCC CCA CAG AGC GCC CCA CA...   \n",
       "3                561  GCC CCA CAT ATG TGA GAC ACA CAA AAC ACA CAG AG...   \n",
       "4                670  ACA CAT ATT TTA TAT ATC TCA CAG AGG GGT GTA TA...   \n",
       "\n",
       "                                      prediction_ids  \\\n",
       "0  7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 ...   \n",
       "1  7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 ...   \n",
       "2  7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 ...   \n",
       "3  7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 ...   \n",
       "4  7 7 7 7 7 7 7 7 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 ...   \n",
       "\n",
       "                                          target_ids  step  f1_score-iii  \\\n",
       "0  7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 ...  1301         0.924   \n",
       "1  7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 ...  1355         0.926   \n",
       "2  7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 ...  1357         0.997   \n",
       "3  7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 ...  1451         0.986   \n",
       "4  7 7 7 7 7 7 7 7 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  1702         0.998   \n",
       "\n",
       "   precision-iii  recall-iii  f1_score-iiE  precision-iiE  ...  \\\n",
       "0          0.858         1.0           1.0            1.0  ...   \n",
       "1          0.862         1.0           1.0            1.0  ...   \n",
       "2          0.994         1.0           1.0            1.0  ...   \n",
       "3          0.973         1.0           1.0            1.0  ...   \n",
       "4          0.996         1.0           1.0            1.0  ...   \n",
       "\n",
       "                                   prediction_tokens  \\\n",
       "0  EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EE...   \n",
       "1  EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EE...   \n",
       "2  EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EE...   \n",
       "3  EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EE...   \n",
       "4  EEE EEE EEE EEE EEE EEE EEE EEE EEi Eii iii ii...   \n",
       "\n",
       "                                       target_tokens  \\\n",
       "0  EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EE...   \n",
       "1  EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EE...   \n",
       "2  EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EE...   \n",
       "3  EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EEE EE...   \n",
       "4  EEE EEE EEE EEE EEE EEE EEE EEE EEi Eii iii ii...   \n",
       "\n",
       "                                          prediction  \\\n",
       "0  EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE...   \n",
       "1  EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEiiiiii...   \n",
       "2  EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEiiiiii...   \n",
       "3  EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEiiiiiiiiiiiiiii...   \n",
       "4  EEEEEEEEEEiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii...   \n",
       "\n",
       "                                              target  nucl_precision_intron  \\\n",
       "0  EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE...                  0.860   \n",
       "1  EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEiiiiii...                  0.863   \n",
       "2  EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEiiiiii...                  0.989   \n",
       "3  EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEiiiiiiiiiiiiiii...                  0.969   \n",
       "4  EEEEEEEEEEiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii...                  0.992   \n",
       "\n",
       "   nucl_precision_exon  nucl_recall_intron  nucl_recall_exon  nucl_f1_intron  \\\n",
       "0                  1.0                 1.0             0.430           0.925   \n",
       "1                  1.0                 1.0             0.407           0.926   \n",
       "2                  1.0                 1.0             0.898           0.994   \n",
       "3                  1.0                 1.0             0.694           0.984   \n",
       "4                  1.0                 1.0             0.714           0.996   \n",
       "\n",
       "   nucl_f1_exon  \n",
       "0         0.601  \n",
       "1         0.579  \n",
       "2         0.946  \n",
       "3         0.819  \n",
       "4         0.833  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get token prediction from categorized dataframes.\n",
    "import os\n",
    "import pandas as pd\n",
    "path = os.path.join(\"prediction\", \"dataframe-0.9=F1 Score-1.0.csv\")\n",
    "df = pd.read_csv(path)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/210 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20512\\3697900894.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mb_attention_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     )\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mb_pred_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;31m# print(b_input_ids.shape, b_label_ids.shape, b_out_bert.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_bert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_ids\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_label_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_out_bert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_pred_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from utils.seqlab import _process_sequence_and_label, id2label, label2id\n",
    "from models import seqlab\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch import tensor, no_grad, argmax\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(\"pretrained\", \"3-new-12w-0\"))\n",
    "model = seqlab.DNABERT_SL(\n",
    "    BertForMaskedLM.from_pretrained(os.path.join(\"pretrained\", \"3-new-12w-0\")).bert,\n",
    "    json.load(open(os.path.join(\"models\", \"config\", \"seqlab\", \"base.lin1.json\"), \"r\"))\n",
    ")\n",
    "model.eval()\n",
    "device = \"cuda:0\"\n",
    "model.to(device)\n",
    "ss_labels = [\n",
    "    \"iiE\",\n",
    "    \"iEE\", \n",
    "    \"EEi\", \n",
    "    \"Eii\"\n",
    "]\n",
    "ss_label_ids = [label2id[a] for a in ss_labels]\n",
    "arr_tokens, arr_labels, arr_bert_values, arr_pred_tokens = [], [], [], []\n",
    "for i, r in tqdm(df.iterrows(), total=df.shape[0], desc=\"Testing\"):\n",
    "    with no_grad():\n",
    "        b_input_ids, b_attention_mask, b_token_type_ids, b_label_ids = _process_sequence_and_label(\n",
    "            r[\"input_tokens\"],\n",
    "            r[\"target_tokens\"],\n",
    "            tokenizer\n",
    "        )\n",
    "        b_input_ids = tensor([b_input_ids]).to(device)\n",
    "        b_attention_mask = tensor([b_attention_mask]).to(device)\n",
    "        b_label_ids = tensor([b_label_ids]).to(device)\n",
    "    b_out, b_out_bert, b_out_head = model(\n",
    "        b_input_ids, \n",
    "        b_attention_mask\n",
    "    )\n",
    "    b_pred_ids = argmax(b_out)\n",
    "    # print(b_input_ids.shape, b_label_ids.shape, b_out_bert.shape)\n",
    "    for input_ids, label_ids, out_bert, pred_ids in zip(b_input_ids, b_label_ids, b_out_bert, b_pred_ids):\n",
    "        for i, j, k, p in zip(input_ids, label_ids, out_bert, pred_ids):\n",
    "            label_id = j.item()\n",
    "            if label_id >= 0:\n",
    "                if label_id in ss_label_ids:\n",
    "                    token_label = id2label[label_id]\n",
    "                    token = tokenizer.ids_to_tokens[i.item()]\n",
    "                    pred_token= id2label[p.item()]\n",
    "                    bert_values_str = [str(a) for a in k.tolist()]\n",
    "                    arr_tokens.append(token)\n",
    "                    arr_pred_tokens.append(pred_token)\n",
    "                    arr_labels.append(token_label)\n",
    "                    arr_bert_values.append(\" \".join(bert_values_str))\n",
    "\n",
    "ndf = pd.DataFrame(data={\n",
    "    \"token\": arr_tokens,\n",
    "    \"label\": arr_labels,\n",
    "    \"bert_value\": arr_bert_values\n",
    "})\n",
    "ndf.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = pd.DataFrame(data={\n",
    "    \"token\": arr_tokens,\n",
    "    \"label\": arr_labels,\n",
    "    \"bert_value\": arr_bert_values\n",
    "})\n",
    "ndf.head(5)\n",
    "ndf.to_csv(os.path.join(\"motif_analysis\", \"token_analysis\", \"df_0.9=F1-1.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb381ed8bacaf36aa3bfaca5a0502d4671ddf79cb6e63c342c2d7fda9a71fcc6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
