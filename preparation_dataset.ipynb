{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate expanded training data: Truerain.csv: 330/330\n",
      "Generate expanded training data: Truealidation.csv: 42/42\n",
      "Generate expanded training data: Trueest.csv: 42/42\n",
      "Generate expanded training data: Truees/train.csv: 40/40\n",
      "Generate expanded training data: Truees/validation.csv: 40/40\n",
      "Generate expanded training data: Truees/test.csv: 40/40\n",
      "Processing source ./sample/polya/train.csv: 275/404\r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate tokenized and expanded datasets.\n",
    "Data from sample or data folder are not expanded (shouldn't be expanded).\n",
    "Data will be expanded when they are moving to dataset folder.\n",
    "Here, we will be preparing samples. \n",
    "\"\"\"\n",
    "from data_dir import dataset_sample_polya_dir, dataset_sample_prom_dir, dataset_sample_ss_dir, sample_ss_dir, sample_prom_dir, sample_polya_dir\n",
    "from data_preparation import expand_by_sliding_window\n",
    "\n",
    "dirs = [(sample_prom_dir, dataset_sample_prom_dir), (sample_ss_dir, dataset_sample_ss_dir), (sample_polya_dir, dataset_sample_polya_dir)]\n",
    "for src_dir, target_dir in dirs:\n",
    "    print(\"Generate expanded training data: {}\".format(expand_by_sliding_window(\"{}/train.csv\".format(src_dir), \"{}/train.csv\".format(target_dir))))\n",
    "    print(\"Generate expanded training data: {}\".format(expand_by_sliding_window(\"{}/validation.csv\".format(src_dir), \"{}/validation.csv\".format(target_dir))))\n",
    "    print(\"Generate expanded training data: {}\".format(expand_by_sliding_window(\"{}/test.csv\".format(src_dir), \"{}/test.csv\".format(target_dir))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate expanded training data: Truen.csv: 404/404\n",
      "Generate expanded validation data: Truetion.csv: 50/50\n",
      "Generate expanded test data: Truetest.csv: 50/50\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate tokenized and expanded dataset from polya data. \n",
    "This script is created becaused the cell above failed to work due to python kernel crash.\n",
    "\"\"\"\n",
    "from data_dir import dataset_sample_polya_dir, dataset_sample_prom_dir, dataset_sample_ss_dir, sample_ss_dir, sample_prom_dir, sample_polya_dir\n",
    "from data_preparation import expand_by_sliding_window\n",
    "\n",
    "dirs = [(sample_polya_dir, dataset_sample_polya_dir)]\n",
    "for src_dir, target_dir in dirs:\n",
    "    print(\"Generate expanded training data: {}\".format(expand_by_sliding_window(\"{}/train.csv\".format(src_dir), \"{}/train.csv\".format(target_dir))))\n",
    "    print(\"Generate expanded validation data: {}\".format(expand_by_sliding_window(\"{}/validation.csv\".format(src_dir), \"{}/validation.csv\".format(target_dir))))\n",
    "    print(\"Generate expanded test data: {}\".format(expand_by_sliding_window(\"{}/test.csv\".format(src_dir), \"{}/test.csv\".format(target_dir))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging dataset sample True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Merge promoter, ss, and polya into single datasets containing train.csv, validation.csv, and test.csv.\n",
    "\"\"\"\n",
    "from data_dir import dataset_sample_prom_dir, dataset_sample_ss_dir, dataset_sample_polya_dir, dataset_sample_dir\n",
    "from data_preparation import merge_dataset\n",
    "\n",
    "print(\"Merging dataset sample {}\".format(merge_dataset(dataset_sample_prom_dir, dataset_sample_ss_dir, dataset_sample_polya_dir, dataset_sample_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promoter: Merging <./data/epd/human_non_tata_kmer/train.csv> & <./data/epd/human_tata_kmer/train.csv>: True\n",
      "Poly A:  Merging <./data/poly-a/grch38/negative/train.csv> & <./data/poly-a/grch38/positive/train.csv>: True\n",
      "Promoter: Merging <./data/epd/human_non_tata_kmer/validation.csv> & <./data/epd/human_tata_kmer/validation.csv>: True\n",
      "Poly A:  Merging <./data/poly-a/grch38/negative/validation.csv> & <./data/poly-a/grch38/positive/validation.csv>: True\n",
      "Promoter: Merging <./data/epd/human_non_tata_kmer/test.csv> & <./data/epd/human_tata_kmer/test.csv>: True\n",
      "Poly A:  Merging <./data/poly-a/grch38/negative/test.csv> & <./data/poly-a/grch38/positive/test.csv>: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Merge positive and negative csv for promoter, ss, and polya into single files and into their respective directory.\n",
    "\"\"\"\n",
    "from data_dir import epd_neg_tata_kmer_dir, epd_pos_tata_kmer_dir, data_epd_dir\n",
    "from data_dir import polya_grch38_negative_dir, polya_grch38_positive_dir, polya_grch38_dir\n",
    "\n",
    "from data_preparation import merge_csv\n",
    "_files = [\"train.csv\", 'validation.csv', 'test.csv']\n",
    "for fname in _files:\n",
    "    neg_file = \"{}/{}\".format(epd_neg_tata_kmer_dir, fname)\n",
    "    pos_file = \"{}/{}\".format(epd_pos_tata_kmer_dir, fname)\n",
    "    target_file = \"{}/{}\".format(data_epd_dir, fname)\n",
    "    print(\"Promoter: Merging <{}> & <{}>: {}\".format(neg_file, pos_file, merge_csv([neg_file, pos_file], target_file)))\n",
    "\n",
    "    pos_file = \"{}/{}\".format(polya_grch38_positive_dir, fname)\n",
    "    neg_file = \"{}/{}\".format(polya_grch38_negative_dir, fname)\n",
    "    target_file = \"{}/{}\".format(polya_grch38_dir, fname)\n",
    "    print(\"Poly A:  Merging <{}> & <{}>: {}\".format(neg_file, pos_file, merge_csv([neg_file, pos_file], target_file)))\n",
    "#endfor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing source ./data/poly-a/grch38/train.csv: 1000/4032\r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Process Poly-A data by expanding tokenized data from original directory into dataset directory.\n",
    "\"\"\"\n",
    "from data_dir import polya_grch38_dir, dataset_full_polya_dir\n",
    "from data_preparation import expand_by_sliding_window\n",
    "\n",
    "_files = ['train.csv', 'validation.csv', 'test.csv']\n",
    "for fname in _files:\n",
    "    src_csv = \"{}/{}\".format(polya_grch38_dir, fname)\n",
    "    target_csv = \"{}/{}\".format(dataset_full_polya_dir, fname)\n",
    "    print(\"Expanding source {} => {}: {} \".format(src_csv, target_csv, expand_by_sliding_window(src_csv, target_csv, length=510)))\n",
    "#endfor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding ./data/splice-sites/splice-deep/train.csv => ./dataset/full/polya/train.csv: True\n",
      "Expanding ./data/splice-sites/splice-deep/validation.csv => ./dataset/full/polya/validation.csv: True\n",
      "Expanding ./data/splice-sites/splice-deep/test.csv => ./dataset/full/polya/test.csv: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Expand Splice-sites and store the result into dataset folder.\n",
    "\"\"\"\n",
    "from data_dir import ss_dir, dataset_full_ss_dir\n",
    "from data_preparation import expand_by_sliding_window\n",
    "_files = [\"train.csv\", 'validation.csv', 'test.csv']\n",
    "for fname in _files:\n",
    "    src_csv = \"{}/{}\".format(ss_dir, fname)\n",
    "    target_csv = \"{}/{}\".format(dataset_full_ss_dir, fname)\n",
    "    print(\"Expanding {} => {}: {}\".format(src_csv, target_csv, expand_by_sliding_window(src_csv, target_csv, length=510)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding ./data/poly-a/grch38/train.csv => ./dataset/full/polya/train.csv: True\n",
      "Expanding ./data/poly-a/grch38/validation.csv => ./dataset/full/polya/validation.csv: True\n",
      "Expanding ./data/poly-a/grch38/test.csv => ./dataset/full/polya/test.csv: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Expand Poly-A datasets (training, validation, and testing) and store the expanded datasets into dataset folder.\n",
    "\"\"\"\n",
    "from data_dir import polya_grch38_dir, dataset_full_polya_dir, _generic_filenames\n",
    "from data_preparation import expand_by_sliding_window_no_pandas\n",
    "\n",
    "for fname in _generic_filenames:\n",
    "    src_path = \"{}/{}\".format(polya_grch38_dir, fname)\n",
    "    target_path = \"{}/{}\".format(dataset_full_polya_dir, fname)\n",
    "    print(\"Expanding {} => {}: {}\".format(src_path, target_path, expand_by_sliding_window_no_pandas(src_path, target_path, length=510)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding ./data/epd/train.csv => ./dataset/full/promoter/train.csv: True\n",
      "Expanding ./data/poly-a/grch38/train.csv => ./dataset/full/polya/train.csv: True\n",
      "Expanding ./data/splice-sites/splice-deep/train.csv => ./dataset/full/splice-sites/train.csv: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Expand train, validation, and test data into 510 tokens for each sequence and store the sequence into dataset folder.\n",
    "Since these are not samples then use `./dataset/full/promoter` folder.\n",
    "\"\"\"\n",
    "from data_dir import epd_train_csv, polya_grch38_train_csv, ss_train_csv, dataset_full_prom_train_csv, dataset_full_ss_train_csv, dataset_full_polya_train_csv  \n",
    "from data_preparation import expand_by_sliding_window_no_pandas\n",
    "\n",
    "_files = [(epd_train_csv, dataset_full_prom_train_csv), (polya_grch38_train_csv, dataset_full_polya_train_csv), (ss_train_csv, dataset_full_ss_train_csv)]\n",
    "for src, target in _files:\n",
    "    print(\"Expanding {} => {}: {}\".format(src, target, expand_by_sliding_window_no_pandas(src, target, length=510)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging all datasets: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Merge promoter, poly A, and splice sites data.\n",
    "\"\"\"\n",
    "from data_dir import dataset_full_prom_dir, dataset_full_ss_dir, dataset_full_polya_dir, dataset_full_dir\n",
    "from data_preparation import merge_dataset\n",
    "\n",
    "print(\"Merging all datasets: {}\".format(merge_dataset(dataset_full_prom_dir, dataset_full_ss_dir, dataset_full_polya_dir, dataset_full_dir, file_to_merge=['train.csv'])))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "473c7453bcb969eece5b07ef8b7f234e7c84010927f6bebce35f0aeb1f8c121e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('sequence-processing-py39': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
