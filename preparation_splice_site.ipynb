{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert fasta to sequence with label in CSV format\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "path = os.path.join(\"data\", \"splice-sites\", \"splice-deep\", \"raw\")\n",
    "target_path = os.path.join(\"data\", \"splice-sites\", \"splice-deep\")\n",
    "positives = [\n",
    "    os.path.join(path, \"positive_DNA_seqs_acceptor_hs.fa\"),\n",
    "    os.path.join(path, \"positive_DNA_seqs_donor_hs.fa\")\n",
    "]\n",
    "for p in positives:\n",
    "    bname = os.path.basename(p)\n",
    "    tname = os.path.join(target_path, f\"{bname.split('.')[0]}.csv\")\n",
    "    if os.path.exists(tname):\n",
    "        os.remove(tname)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(tname), exist_ok=True)\n",
    "    df = pd.DataFrame(columns=[\"sequence\", \"label\"])\n",
    "    f = open(p, \"r\")\n",
    "    t = open(tname, \"x\")\n",
    "    t.write(\"sequence,label\\n\")\n",
    "    for line in f:\n",
    "        t.write(f\"{line.strip()},{1}\\n\")\n",
    "    f.close()\n",
    "    t.close()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "path = os.path.join(\"data\", \"splice-sites\", \"splice-deep\", \"raw\")\n",
    "target_path = os.path.join(\"data\", \"splice-sites\", \"splice-deep\")\n",
    "\n",
    "negatives = [\n",
    "    os.path.join(path, \"negative_DNA_seqs_acceptor_hs.fa\"),\n",
    "    os.path.join(path, \"negative_DNA_seqs_donor_hs.fa\")\n",
    "]\n",
    "for p in negatives:\n",
    "    bname = os.path.basename(p)\n",
    "    tname = os.path.join(target_path, f\"{bname.split('.')[0]}.csv\")\n",
    "    if os.path.exists(tname):\n",
    "        os.remove(tname)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(tname), exist_ok=True)\n",
    "    df = pd.DataFrame(columns=[\"sequence\", \"label\"])\n",
    "    f = open(p, \"r\")\n",
    "    t = open(tname, \"x\")\n",
    "    t.write(\"sequence,label\\n\")\n",
    "    for line in f:\n",
    "        t.write(f\"{line.strip()},{0}\\n\")\n",
    "    f.close()\n",
    "    t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positive_DNA_seqs_acceptor_hs.csv: 100%|██████████| 248150/248150 [00:27<00:00, 9070.25it/s] \n",
      "positive_DNA_seqs_donor_hs.csv: 100%|██████████| 250400/250400 [00:26<00:00, 9438.57it/s] \n"
     ]
    }
   ],
   "source": [
    "\"\"\"Creating 512-char-chunks from single sequence\"\"\"\n",
    "\n",
    "import os\n",
    "from data_dir import ss_dir\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from data_preparation import str_kmer\n",
    "\n",
    "positives = [\n",
    "    os.path.join(ss_dir, \"positive_DNA_seqs_acceptor_hs.csv\"),\n",
    "    os.path.join(ss_dir, \"positive_DNA_seqs_donor_hs.csv\"),\n",
    "]\n",
    "\n",
    "positives_chunk = [\n",
    "    os.path.join(ss_dir, \"positive_DNA_seqs_acceptor_hs.expanded.csv\"),\n",
    "    os.path.join(ss_dir, \"positive_DNA_seqs_donor_hs.expanded.csv\"),\n",
    "]\n",
    "\n",
    "for p, pc in zip(positives, positives_chunk):\n",
    "    bname = os.path.basename(p)\n",
    "    dest_path = pc\n",
    "    if os.path.exists(dest_path):\n",
    "        os.remove(dest_path)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "    dest = open(dest_path, \"x\")\n",
    "    dest.write(f\"sequence,label\\n\")\n",
    "    df = pd.read_csv(p)\n",
    "    for i, r in tqdm(df.iterrows(), total=df.shape[0], desc=f\"{bname}\"):\n",
    "        s = list(r[\"sequence\"])\n",
    "        label = r[\"label\"]\n",
    "        ln = len(s)\n",
    "        delta = ln-512\n",
    "        s_left  = s[0:ln-delta]\n",
    "        s_middle = s[int(delta/2):ln-int(delta/2)]\n",
    "        s_right = s[delta:ln]\n",
    "        dest.write(f\"{''.join(s_left)},{label}\\n\")\n",
    "        dest.write(f\"{''.join(s_middle)},{label}\\n\")\n",
    "        dest.write(f\"{''.join(s_right)},{label}\\n\")\n",
    "    #endfor\n",
    "    dest.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "negative_DNA_seqs_acceptor_hs.csv: 100%|██████████| 248150/248150 [00:26<00:00, 9281.63it/s]\n",
      "negative_DNA_seqs_donor_hs.csv: 100%|██████████| 250400/250400 [00:26<00:00, 9465.24it/s] \n"
     ]
    }
   ],
   "source": [
    "\"\"\"Creating 512-char-chunks from single sequence\"\"\"\n",
    "\n",
    "negatives = [\n",
    "    os.path.join(ss_dir, \"negative_DNA_seqs_acceptor_hs.csv\"),\n",
    "    os.path.join(ss_dir, \"negative_DNA_seqs_donor_hs.csv\"),\n",
    "]\n",
    "\n",
    "negatives_chunk = [\n",
    "    os.path.join(ss_dir, \"negative_DNA_seqs_acceptor_hs.expanded.csv\"),\n",
    "    os.path.join(ss_dir, \"negative_DNA_seqs_donor_hs.expanded.csv\"),\n",
    "]\n",
    "\n",
    "for p, pc in zip(negatives, negatives_chunk):\n",
    "    bname = os.path.basename(p)\n",
    "    dest_path = pc\n",
    "    if os.path.exists(dest_path):\n",
    "        os.remove(dest_path)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "    dest = open(dest_path, \"x\")\n",
    "    dest.write(f\"sequence,label\\n\")\n",
    "    df = pd.read_csv(p)\n",
    "    for i, r in tqdm(df.iterrows(), total=df.shape[0], desc=f\"{bname}\"):\n",
    "        s = list(r[\"sequence\"])\n",
    "        label = r[\"label\"]\n",
    "        ln = len(s)\n",
    "        delta = ln-512\n",
    "        s_left  = s[0:ln-delta]\n",
    "        s_middle = s[int(delta/2):ln-int(delta/2)]\n",
    "        s_right = s[delta:ln]\n",
    "        dest.write(f\"{''.join(s_left)},{label}\\n\")\n",
    "        dest.write(f\"{''.join(s_middle)},{label}\\n\")\n",
    "        dest.write(f\"{''.join(s_right)},{label}\\n\")\n",
    "    #endfor\n",
    "    dest.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:21<00:00, 10.99s/it]\n",
      "100%|██████████| 2/2 [00:25<00:00, 12.69s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Merge positives and negatives\n",
    "\"\"\"\n",
    "import os\n",
    "from data_dir import ss_dir\n",
    "negatives_chunk = [\n",
    "    os.path.join(ss_dir, \"negative_DNA_seqs_acceptor_hs.expanded.csv\"),\n",
    "    os.path.join(ss_dir, \"negative_DNA_seqs_donor_hs.expanded.csv\"),\n",
    "]\n",
    "\n",
    "positives_chunk = [\n",
    "    os.path.join(ss_dir, \"positive_DNA_seqs_acceptor_hs.expanded.csv\"),\n",
    "    os.path.join(ss_dir, \"positive_DNA_seqs_donor_hs.expanded.csv\"),\n",
    "]\n",
    "\n",
    "from data_preparation import merge_csv\n",
    "positive = os.path.join(ss_dir, \"positive_DNA_seqs.csv\")\n",
    "negative = os.path.join(ss_dir, \"negative_DNA_seqs.csv\")\n",
    "\n",
    "merge_csv(positives_chunk, positive)\n",
    "merge_csv(negatives_chunk, negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create balanced dataset based on promoter size 6943.\"\"\"\n",
    "import os\n",
    "from data_dir import ss_dir\n",
    "positive = os.path.join(ss_dir, \"positive_DNA_seqs.csv\")\n",
    "negative = os.path.join(ss_dir, \"negative_DNA_seqs.csv\")\n",
    "positive_balanced = os.path.join(ss_dir, \"positive_DNA_seqs.balanced.csv\")\n",
    "negative_balanced = os.path.join(ss_dir, \"negative_DNA_seqs.balanced.csv\")\n",
    "\n",
    "import pandas as pd\n",
    "positive_balanced_df = pd.read_csv(positive).sample(n=6943)\n",
    "positive_balanced_df.to_csv(positive_balanced, index=False)\n",
    "negative_balanced_df = pd.read_csv(negative).sample(n=6943)\n",
    "negative_balanced_df.to_csv(negative_balanced, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Split positive balanced and negative balanced into train and validation\"\"\"\n",
    "from data_preparation import split_csv\n",
    "from data_dir import ss_dir\n",
    "positive_balanced = os.path.join(ss_dir, \"positive_DNA_seqs.balanced.csv\")\n",
    "positive_balanced_train = os.path.join(ss_dir, \"positive_DNA_seqs.balanced.train.csv\")\n",
    "positive_balanced_validation = os.path.join(ss_dir, \"positive_DNA_seqs.balanced.validation.csv\")\n",
    "\n",
    "negative_balanced = os.path.join(ss_dir, \"negative_DNA_seqs.balanced.csv\")\n",
    "negative_balanced_train = os.path.join(ss_dir, \"negative_DNA_seqs.balanced.train.csv\")\n",
    "negative_balanced_validation = os.path.join(ss_dir, \"negative_DNA_seqs.balanced.validation.csv\")\n",
    "\n",
    "split_csv(positive_balanced, [0.8, 0.2], [positive_balanced_train, positive_balanced_validation])\n",
    "split_csv(negative_balanced, [0.8, 0.2], [negative_balanced_train, negative_balanced_validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 13.70it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 43.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Merge training and validation\n",
    "\"\"\"\n",
    "from data_preparation import merge_csv\n",
    "from data_dir import ss_dir\n",
    "positive_balanced_train = os.path.join(ss_dir, \"positive_DNA_seqs.balanced.train.csv\")\n",
    "positive_balanced_validation = os.path.join(ss_dir, \"positive_DNA_seqs.balanced.validation.csv\")\n",
    "negative_balanced_train = os.path.join(ss_dir, \"negative_DNA_seqs.balanced.train.csv\")\n",
    "negative_balanced_validation = os.path.join(ss_dir, \"negative_DNA_seqs.balanced.validation.csv\")\n",
    "ss_train_balanced = os.path.join(ss_dir, \"ss_train.balanced.csv\")\n",
    "ss_validation_balanced = os.path.join(ss_dir, \"ss_validation.balanced.csv\")\n",
    "\n",
    "merge_csv([positive_balanced_train, negative_balanced_train], ss_train_balanced)\n",
    "merge_csv([positive_balanced_validation, negative_balanced_validation], ss_validation_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating kmer for <data\\splice-sites/splice-deep\\negative_DNA_seqs_donor_hs.expanded.csv>: 751200/751200                                                         \r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate kmer format.\n",
    "\"\"\"\n",
    "import os\n",
    "from data_dir import ss_dir\n",
    "positives = [\n",
    "    os.path.join(ss_dir, \"positive_DNA_seqs_acceptor_hs.expanded.csv\"),\n",
    "    os.path.join(ss_dir, \"positive_DNA_seqs_donor_hs.expanded.csv\"),\n",
    "]\n",
    "negatives = [\n",
    "    os.path.join(ss_dir, \"negative_DNA_seqs_acceptor_hs.expanded.csv\"),\n",
    "    os.path.join(ss_dir, \"negative_DNA_seqs_donor_hs.expanded.csv\"),\n",
    "]\n",
    "\n",
    "from data_preparation import generate_kmer_csv\n",
    "for p in positives:\n",
    "    bname = os.path.basename(p)\n",
    "    target_path = os.path.join(ss_dir, f\"{bname.split('.')[0]}.expanded.kmer.csv\")\n",
    "    generate_kmer_csv(p, target_path)\n",
    "for n in negatives:\n",
    "    bname = os.path.basename(n)\n",
    "    target_path = os.path.join(ss_dir, f\"{bname.split('.')[0]}.expanded.kmer.csv\")\n",
    "    generate_kmer_csv(n, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split into training and validation data.\n",
    "\"\"\"\n",
    "import os\n",
    "from data_dir import ss_dir\n",
    "positives = [\n",
    "    os.path.join(ss_dir, \"positive_DNA_seqs_acceptor_hs.expanded.csv\"),\n",
    "    os.path.join(ss_dir, \"positive_DNA_seqs_donor_hs.expanded.csv\"),\n",
    "]\n",
    "\n",
    "negatives = [\n",
    "    os.path.join(ss_dir, \"negative_DNA_seqs_acceptor_hs.expanded.csv\"),\n",
    "    os.path.join(ss_dir, \"negative_DNA_seqs_donor_hs.expanded.csv\"),\n",
    "]\n",
    "\n",
    "positives_negatives = [positives, negatives]\n",
    "\n",
    "for pn in positives_negatives:\n",
    "    for p in pn:\n",
    "        df = pd.read_csv(p)\n",
    "        train_df = df.sample(frac=0.8)\n",
    "        validation_df = df.drop(train_df.index)\n",
    "        train_df.to_csv(\n",
    "            os.path.join(ss_dir, f\"{os.path.basename(p).split('.')[0]}.train.csv\"),\n",
    "            index=False\n",
    "        )\n",
    "        validation_df.to_csv(\n",
    "            os.path.join(ss_dir, f\"{os.path.basename(p).split('.')[0]}.validation.csv\"),\n",
    "            index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:10<00:00,  5.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging training data: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging validation data: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Merge training and validation.\n",
    "\"\"\"\n",
    "from data_preparation import merge_csv\n",
    "from data_dir import ss_dir\n",
    "import os\n",
    "trains = [\n",
    "    os.path.join(ss_dir, \"positive_DNA_seqs_acceptor_hs.train.csv\"),\n",
    "    os.path.join(ss_dir, \"negative_DNA_seqs_acceptor_hs.train.csv\")\n",
    "]\n",
    "validations = [\n",
    "    os.path.join(ss_dir, \"positive_DNA_seqs_acceptor_hs.validation.csv\"),\n",
    "    os.path.join(ss_dir, \"negative_DNA_seqs_acceptor_hs.validation.csv\")\n",
    "]\n",
    "print(f'Merging training data: {merge_csv(trains, os.path.join(ss_dir, \"ss_train.csv\"))}')\n",
    "print(f'Merging validation data: {merge_csv(validations, os.path.join(ss_dir, \"ss_validation.csv\"))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error File workspace\\ss\\pos_ss_acc_hs.csv not found.\n",
      "Error File workspace\\ss\\pos_ss_don_hs.csv not found.\n",
      "Error File workspace\\ss\\neg_ss_acc_hs.csv not found.\n",
      "Error File workspace\\ss\\neg_ss_don_hs.csv not found.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Positive and negative acceptor and donor are available by some previous processes. \n",
    "Generate kmer version of those data.\n",
    "\"\"\"\n",
    "from data_preparation import generate_kmer_csv\n",
    "from data_dir import ss_pos_acc_hs_csv, ss_pos_don_hs_csv, ss_neg_acc_hs_csv, ss_neg_don_hs_csv, ss_pos_acc_hs_kmer_csv, ss_pos_don_hs_kmer_csv, ss_neg_acc_hs_kmer_csv, ss_neg_don_hs_kmer_csv\n",
    "\n",
    "file_pairs = [\n",
    "    (ss_pos_acc_hs_csv, ss_pos_acc_hs_kmer_csv),\n",
    "    (ss_pos_don_hs_csv, ss_pos_don_hs_kmer_csv),\n",
    "    (ss_neg_acc_hs_csv, ss_neg_acc_hs_kmer_csv),\n",
    "    (ss_neg_don_hs_csv, ss_neg_don_hs_kmer_csv)\n",
    "]\n",
    "\n",
    "for src, target_dir in file_pairs:\n",
    "    generate_kmer_csv(src, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate kmer version of splice site data.\n",
    "\"\"\"\n",
    "from data_dir import ss_dir\n",
    "from data_preparation import generate_kmer_csv\n",
    "src_train = \"{}/train_no_kmer.csv\".format(ss_dir)\n",
    "src_validation = \"{}/validation_no_kmer.csv\".format(ss_dir)\n",
    "src_test = \"{}/test_no_kmer.csv\".format(ss_dir)\n",
    "\n",
    "target_train = \"{}/train.csv\".format(ss_dir)\n",
    "target_validation = \"{}/validation.csv\".format(ss_dir)\n",
    "target_test = \"{}/test.csv\".format(ss_dir)\n",
    "\n",
    "_pairs = [(src_validation, target_validation), (src_test, target_test)]\n",
    "#for src, target in _pairs:\n",
    "#    print(\"Generate kmer csv for {} => {}: {}\".format(src, target, generate_kmer_csv(src, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate kmer csv for ./data/splice-sites/splice-deep/validation_no_kmer.csv => ./data/splice-sites/splice-deep/validation.csv: True\n",
      "Generate kmer csv for ./data/splice-sites/splice-deep/test_no_kmer.csv => ./data/splice-sites/splice-deep/test.csv: True\n"
     ]
    }
   ],
   "source": [
    "_pairs = [(src_validation, target_validation), (src_test, target_test)]\n",
    "for src, target in _pairs:\n",
    "    print(\"Generate kmer csv for {} => {}: {}\".format(src, target, generate_kmer_csv(src, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error [Errno 28] No space left on device\n",
      "Error Traceback (most recent call last):\n",
      "  File \"w:\\Research\\_sequence-processing\\data_preparation.py\", line 1034, in expand_by_sliding_window_no_pandas\n",
      "    if _count < _len:\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "Expanding ./data/splice-sites/splice-deep/train.csv => ./dataset/full/splice-sites/train.csv: False\n",
      "Error [Errno 28] No space left on device\n",
      "Error Traceback (most recent call last):\n",
      "  File \"w:\\Research\\_sequence-processing\\data_preparation.py\", line 1034, in expand_by_sliding_window_no_pandas\n",
      "    if _count < _len:\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "Expanding ./data/splice-sites/splice-deep/validation.csv => ./dataset/full/splice-sites/validation.csv: False\n",
      "Error [Errno 28] No space left on device\n",
      "Error Traceback (most recent call last):\n",
      "  File \"w:\\Research\\_sequence-processing\\data_preparation.py\", line 1034, in expand_by_sliding_window_no_pandas\n",
      "    if _count < _len:\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "Expanding ./data/splice-sites/splice-deep/test.csv => ./dataset/full/splice-sites/test.csv: False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Expand Splice-sites and store the result into dataset folder.\n",
    "\"\"\"\n",
    "from data_dir import ss_dir, dataset_full_ss_dir\n",
    "from data_preparation import expand_by_sliding_window_no_pandas\n",
    "_files = [\"train.csv\", 'validation.csv', 'test.csv']\n",
    "for fname in _files:\n",
    "    src_csv = \"{}/{}\".format(ss_dir, fname)\n",
    "    target_csv = \"{}/{}\".format(dataset_full_ss_dir, fname)\n",
    "    print(\"Expanding {} => {}: {}\".format(src_csv, target_csv, expand_by_sliding_window_no_pandas(src_csv, target_csv, length=510)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ss pos acc 248150\n",
      "ss pos don 250400\n",
      "ss neg acc 248150\n",
      "ss neg don 250400\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checking the size of dataset. It appears that generating expanded sequence resulted in too-large file.\n",
    "\"\"\"\n",
    "from data_dir import ss_neg_don_hs_csv, ss_neg_acc_hs_csv, ss_pos_acc_hs_csv, ss_pos_don_hs_csv\n",
    "import pandas as pd\n",
    "\n",
    "ss_neg_acc_df = pd.read_csv(ss_neg_acc_hs_csv)\n",
    "ss_neg_don_df = pd.read_csv(ss_neg_don_hs_csv)\n",
    "ss_pos_acc_df = pd.read_csv(ss_pos_acc_hs_csv)\n",
    "ss_pos_don_df = pd.read_csv(ss_pos_don_hs_csv)\n",
    "\n",
    "print(\"ss pos acc {}\".format(len(ss_pos_acc_df)))\n",
    "print(\"ss pos don {}\".format(len(ss_pos_don_df)))\n",
    "print(\"ss neg acc {}\".format(len(ss_neg_acc_df)))\n",
    "print(\"ss neg don {}\".format(len(ss_neg_don_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate sample for ./data/splice-sites/splice-deep/neg_ss_acc_hs.csv => ./data/splice-sites/splice-deep/ss_neg_acc_hs.1300.csv: ./data/splice-sites/splice-deep/ss_neg_acc_hs.1300.csv\n",
      "Generate sample for ./data/splice-sites/splice-deep/neg_ss_don_hs.csv => ./data/splice-sites/splice-deep/ss_neg_don_hs.1300.csv: ./data/splice-sites/splice-deep/ss_neg_don_hs.1300.csv\n",
      "Generate sample for ./data/splice-sites/splice-deep/pos_ss_acc_hs.csv => ./data/splice-sites/splice-deep/ss_pos_acc_hs.1300.csv: ./data/splice-sites/splice-deep/ss_pos_acc_hs.1300.csv\n",
      "Generate sample for ./data/splice-sites/splice-deep/pos_ss_don_hs.csv => ./data/splice-sites/splice-deep/ss_pos_don_hs.1300.csv: ./data/splice-sites/splice-deep/ss_pos_don_hs.1300.csv\n",
      "Generate sample for ./data/splice-sites/splice-deep/neg_ss_acc_hs.csv => ./data/splice-sites/splice-deep/ss_neg_acc_hs.2000.csv: ./data/splice-sites/splice-deep/ss_neg_acc_hs.2000.csv\n",
      "Generate sample for ./data/splice-sites/splice-deep/neg_ss_don_hs.csv => ./data/splice-sites/splice-deep/ss_neg_don_hs.2000.csv: ./data/splice-sites/splice-deep/ss_neg_don_hs.2000.csv\n",
      "Generate sample for ./data/splice-sites/splice-deep/pos_ss_acc_hs.csv => ./data/splice-sites/splice-deep/ss_pos_acc_hs.2000.csv: ./data/splice-sites/splice-deep/ss_pos_acc_hs.2000.csv\n",
      "Generate sample for ./data/splice-sites/splice-deep/pos_ss_don_hs.csv => ./data/splice-sites/splice-deep/ss_pos_don_hs.2000.csv: ./data/splice-sites/splice-deep/ss_pos_don_hs.2000.csv\n",
      "Generate sample for ./data/splice-sites/splice-deep/neg_ss_acc_hs.csv => ./data/splice-sites/splice-deep/ss_neg_acc_hs.3000.csv: ./data/splice-sites/splice-deep/ss_neg_acc_hs.3000.csv\n",
      "Generate sample for ./data/splice-sites/splice-deep/neg_ss_don_hs.csv => ./data/splice-sites/splice-deep/ss_neg_don_hs.3000.csv: ./data/splice-sites/splice-deep/ss_neg_don_hs.3000.csv\n",
      "Generate sample for ./data/splice-sites/splice-deep/pos_ss_acc_hs.csv => ./data/splice-sites/splice-deep/ss_pos_acc_hs.3000.csv: ./data/splice-sites/splice-deep/ss_pos_acc_hs.3000.csv\n",
      "Generate sample for ./data/splice-sites/splice-deep/pos_ss_don_hs.csv => ./data/splice-sites/splice-deep/ss_pos_don_hs.3000.csv: ./data/splice-sites/splice-deep/ss_pos_don_hs.3000.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Both positive and negative splice site data is too large. This code is for sampling.\n",
    "Sampling is done because local storage limitation.\n",
    "\"\"\"\n",
    "from data_preparation import generate_sample\n",
    "from data_dir import (\n",
    "    ss_neg_acc_hs_non_kmer_csv, ss_neg_don_hs_non_kmer_csv, ss_pos_acc_hs_non_kmer_csv, ss_pos_don_hs_non_kmer_csv,\n",
    "    ss_neg_acc_hs_csv, ss_neg_don_hs_csv, ss_pos_acc_hs_csv, ss_pos_don_hs_csv,\n",
    "    ss_dir\n",
    ")\n",
    "_src = [ss_neg_acc_hs_csv, ss_neg_don_hs_csv, ss_pos_acc_hs_csv, ss_pos_don_hs_csv]\n",
    "_target = [ss_neg_acc_hs_non_kmer_csv, ss_neg_don_hs_non_kmer_csv, ss_pos_acc_hs_non_kmer_csv, ss_pos_don_hs_non_kmer_csv]\n",
    "# _target_500 = [\"{}/{}\".format(ss_dir, fname) for fname in [\"ss_neg_acc_hs.500.csv\", \"ss_neg_don_hs.500.csv\", \"ss_pos_acc_hs.500.csv\", \"ss_pos_don_hs.500.csv\"]]\n",
    "_target_1300 = [\"{}/{}\".format(ss_dir, fname) for fname in [\"ss_neg_acc_hs.1300.csv\", \"ss_neg_don_hs.1300.csv\", \"ss_pos_acc_hs.1300.csv\", \"ss_pos_don_hs.1300.csv\"]]\n",
    "_target_2000 = [\"{}/{}\".format(ss_dir, fname) for fname in [\"ss_neg_acc_hs.2000.csv\", \"ss_neg_don_hs.2000.csv\", \"ss_pos_acc_hs.2000.csv\", \"ss_pos_don_hs.2000.csv\"]]\n",
    "_target_3000 = [\"{}/{}\".format(ss_dir, fname) for fname in [\"ss_neg_acc_hs.3000.csv\", \"ss_neg_don_hs.3000.csv\", \"ss_pos_acc_hs.3000.csv\", \"ss_pos_don_hs.3000.csv\"]]\n",
    "\n",
    "#for src, target in zip(_src, _target_500):\n",
    "#    print(\"Generate sample for {} => {}: {}\".format(src, target, generate_sample(src, target, n_sample=515)))\n",
    "\n",
    "for src, target in zip(_src, _target_1300):\n",
    "    print(\"Generate sample for {} => {}: {}\".format(src, target, generate_sample(src, target, n_sample=1300)))\n",
    "\n",
    "for src, target in zip(_src, _target_2000):\n",
    "    print(\"Generate sample for {} => {}: {}\".format(src, target, generate_sample(src, target, n_sample=2000)))\n",
    "\n",
    "for src, target in zip(_src, _target_3000):\n",
    "    print(\"Generate sample for {} => {}: {}\".format(src, target, generate_sample(src, target, n_sample=3000)))\n",
    "\n",
    "\n",
    "#_pairs = [(ss_neg_acc_hs_csv, ss_neg_acc_hs_non_kmer_csv), (ss_neg_don_hs_csv, ss_neg_don_hs_non_kmer_csv), (ss_pos_acc_hs_csv, ss_pos_acc_hs_non_kmer_csv), (ss_pos_don_hs_csv, ss_pos_don_hs_non_kmer_csv)]\n",
    "#for src, target in _pairs:\n",
    "#    print(\"Generate sample for {} => {}: {}\".format(src, target, generate_sample(src, target, n_sample=500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate kmer csv for ./data/splice-sites/splice-deep/ss_neg_acc_hs.1300.csv => ./data/splice-sites/splice-deep/ss_neg_acc_hs.1300.kmer.csv: True\n",
      "Generate kmer csv for ./data/splice-sites/splice-deep/ss_neg_don_hs.1300.csv => ./data/splice-sites/splice-deep/ss_neg_don_hs.1300.kmer.csv: True\n",
      "Generate kmer csv for ./data/splice-sites/splice-deep/ss_pos_acc_hs.1300.csv => ./data/splice-sites/splice-deep/ss_pos_acc_hs.1300.kmer.csv: True\n",
      "Generate kmer csv for ./data/splice-sites/splice-deep/ss_pos_don_hs.1300.csv => ./data/splice-sites/splice-deep/ss_pos_don_hs.1300.kmer.csv: True\n",
      "Generate kmer csv for ./data/splice-sites/splice-deep/ss_neg_acc_hs.2000.csv => ./data/splice-sites/splice-deep/ss_neg_acc_hs.2000.kmer.csv: True\n",
      "Generate kmer csv for ./data/splice-sites/splice-deep/ss_neg_don_hs.2000.csv => ./data/splice-sites/splice-deep/ss_neg_don_hs.2000.kmer.csv: True\n",
      "Generate kmer csv for ./data/splice-sites/splice-deep/ss_pos_acc_hs.2000.csv => ./data/splice-sites/splice-deep/ss_pos_acc_hs.2000.kmer.csv: True\n",
      "Generate kmer csv for ./data/splice-sites/splice-deep/ss_pos_don_hs.2000.csv => ./data/splice-sites/splice-deep/ss_pos_don_hs.2000.kmer.csv: True\n",
      "Generate kmer csv for ./data/splice-sites/splice-deep/ss_neg_acc_hs.3000.csv => ./data/splice-sites/splice-deep/ss_neg_acc_hs.3000.kmer.csv: True\n",
      "Generate kmer csv for ./data/splice-sites/splice-deep/ss_neg_don_hs.3000.csv => ./data/splice-sites/splice-deep/ss_neg_don_hs.3000.kmer.csv: True\n",
      "Generate kmer csv for ./data/splice-sites/splice-deep/ss_pos_acc_hs.3000.csv => ./data/splice-sites/splice-deep/ss_pos_acc_hs.3000.kmer.csv: True\n",
      "Generate kmer csv for ./data/splice-sites/splice-deep/ss_pos_don_hs.3000.csv => ./data/splice-sites/splice-deep/ss_pos_don_hs.3000.kmer.csv: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate kmer version for splice sites data for positive and negative acceptor and donor.\n",
    "\"\"\"\n",
    "from data_preparation import generate_kmer_csv\n",
    "from data_dir import (\n",
    "    ss_neg_acc_hs_non_kmer_csv, ss_neg_don_hs_non_kmer_csv, ss_pos_acc_hs_non_kmer_csv, ss_pos_don_hs_non_kmer_csv,\n",
    "    ss_neg_acc_hs_kmer_csv, ss_neg_don_hs_kmer_csv, ss_pos_acc_hs_kmer_csv, ss_pos_don_hs_kmer_csv,\n",
    ")\n",
    "\n",
    "#_src_500 = [\"{}/{}\".format(ss_dir, fname) for fname in [\"ss_neg_acc_hs.500.csv\", \"ss_neg_don_hs.500.csv\", \"ss_pos_acc_hs.500.csv\", \"ss_pos_don_hs.500.csv\"]]\n",
    "#_target_500 = [\"{}/{}\".format(ss_dir, fname) for fname in [\"ss_neg_acc_hs.500.kmer.csv\", \"ss_neg_don_hs.500.kmer.csv\", \"ss_pos_acc_hs.500.kmer.csv\", \"ss_pos_don_hs.500.kmer.csv\"]]\n",
    "_src_1300 = [\"{}/{}\".format(ss_dir, fname) for fname in [\"ss_neg_acc_hs.1300.csv\", \"ss_neg_don_hs.1300.csv\", \"ss_pos_acc_hs.1300.csv\", \"ss_pos_don_hs.1300.csv\"]]\n",
    "_target_1300 = [\"{}/{}\".format(ss_dir, fname) for fname in [\"ss_neg_acc_hs.1300.kmer.csv\", \"ss_neg_don_hs.1300.kmer.csv\", \"ss_pos_acc_hs.1300.kmer.csv\", \"ss_pos_don_hs.1300.kmer.csv\"]]\n",
    "_src_2000 = [\"{}/{}\".format(ss_dir, fname) for fname in [\"ss_neg_acc_hs.2000.csv\", \"ss_neg_don_hs.2000.csv\", \"ss_pos_acc_hs.2000.csv\", \"ss_pos_don_hs.2000.csv\"]]\n",
    "_target_2000 = [\"{}/{}\".format(ss_dir, fname) for fname in [\"ss_neg_acc_hs.2000.kmer.csv\", \"ss_neg_don_hs.2000.kmer.csv\", \"ss_pos_acc_hs.2000.kmer.csv\", \"ss_pos_don_hs.2000.kmer.csv\"]]\n",
    "_src_3000 = [\"{}/{}\".format(ss_dir, fname) for fname in [\"ss_neg_acc_hs.3000.csv\", \"ss_neg_don_hs.3000.csv\", \"ss_pos_acc_hs.3000.csv\", \"ss_pos_don_hs.3000.csv\"]]\n",
    "_target_3000 = [\"{}/{}\".format(ss_dir, fname) for fname in [\"ss_neg_acc_hs.3000.kmer.csv\", \"ss_neg_don_hs.3000.kmer.csv\", \"ss_pos_acc_hs.3000.kmer.csv\", \"ss_pos_don_hs.3000.kmer.csv\"]]\n",
    "\n",
    "for zip_set in [zip(_src_1300, _target_1300), zip(_src_2000, _target_2000), zip(_src_3000, _target_3000)]:\n",
    "    for src, target in zip_set:\n",
    "        print(\"Generate kmer csv for {} => {}: {}\".format(src, target, generate_kmer_csv(src, target)))\n",
    "\n",
    "\n",
    "\n",
    "#_pairs = [(ss_neg_acc_hs_non_kmer_csv, ss_neg_acc_hs_kmer_csv), (ss_neg_don_hs_non_kmer_csv, ss_neg_don_hs_kmer_csv), (ss_pos_acc_hs_non_kmer_csv, ss_pos_acc_hs_kmer_csv), (ss_pos_don_hs_non_kmer_csv, ss_pos_don_hs_kmer_csv)]\n",
    "#for src, target in _pairs:\n",
    "#    print(\"Generate kmer csv for {} => {}: {}\".format(src, target, generate_kmer_csv(src, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting and storing split to ./data/splice-sites/splice-deep/train.ss_pos_acc_hs.1300.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/validation.ss_pos_acc_hs.1300.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/train.ss_pos_don_hs.1300.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/validation.ss_pos_don_hs.1300.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/train.ss_neg_acc_hs.1300.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/validation.ss_neg_acc_hs.1300.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/train.ss_neg_don_hs.1300.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/validation.ss_neg_don_hs.1300.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/train.ss_pos_acc_hs.2000.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/validation.ss_pos_acc_hs.2000.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/train.ss_pos_don_hs.2000.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/validation.ss_pos_don_hs.2000.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/train.ss_neg_acc_hs.2000.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/validation.ss_neg_acc_hs.2000.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/train.ss_neg_don_hs.2000.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/validation.ss_neg_don_hs.2000.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/train.ss_pos_acc_hs.3000.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/validation.ss_pos_acc_hs.3000.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/train.ss_pos_don_hs.3000.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/validation.ss_pos_don_hs.3000.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/train.ss_neg_acc_hs.3000.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/validation.ss_neg_acc_hs.3000.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/train.ss_neg_don_hs.3000.kmer.csv\n",
      "Splitting and storing split to ./data/splice-sites/splice-deep/validation.ss_neg_don_hs.3000.kmer.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Split each file into train.csv and validation.csv\n",
    "\"\"\"\n",
    "from data_preparation import split_and_store_csv\n",
    "from data_dir import ss_dir\n",
    "_sizes = [1300, 2000, 3000]\n",
    "_fraction = [0.9, 0.1]\n",
    "for _s in _sizes:\n",
    "    pos_acc = '{}/ss_pos_acc_hs.{}.kmer.csv'.format(ss_dir, _s)\n",
    "    pos_acc_train = '{}/train.ss_pos_acc_hs.{}.kmer.csv'.format(ss_dir, _s)\n",
    "    pos_acc_validation = '{}/validation.ss_pos_acc_hs.{}.kmer.csv'.format(ss_dir, _s)\n",
    "    split_and_store_csv(pos_acc, _fraction, [pos_acc_train, pos_acc_validation])\n",
    "\n",
    "    pos_don = '{}/ss_pos_don_hs.{}.kmer.csv'.format(ss_dir, _s)\n",
    "    pos_don_train = '{}/train.ss_pos_don_hs.{}.kmer.csv'.format(ss_dir, _s)\n",
    "    pos_don_validation = '{}/validation.ss_pos_don_hs.{}.kmer.csv'.format(ss_dir, _s)\n",
    "    split_and_store_csv(pos_don, _fraction, [pos_don_train, pos_don_validation])\n",
    "    \n",
    "    neg_acc = '{}/ss_neg_acc_hs.{}.kmer.csv'.format(ss_dir, _s)\n",
    "    neg_acc_train = '{}/train.ss_neg_acc_hs.{}.kmer.csv'.format(ss_dir, _s)\n",
    "    neg_acc_validation = '{}/validation.ss_neg_acc_hs.{}.kmer.csv'.format(ss_dir, _s)\n",
    "    split_and_store_csv(neg_acc, _fraction, [neg_acc_train, neg_acc_validation])\n",
    "\n",
    "    neg_don = '{}/ss_neg_don_hs.{}.kmer.csv'.format(ss_dir, _s)\n",
    "    neg_don_train = '{}/train.ss_neg_don_hs.{}.kmer.csv'.format(ss_dir, _s)\n",
    "    neg_don_validation = '{}/validation.ss_neg_don_hs.{}.kmer.csv'.format(ss_dir, _s)\n",
    "    split_and_store_csv(neg_don, _fraction, [neg_don_train, neg_don_validation])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging ['./data/splice-sites/splice-deep/train.ss_pos_acc_hs.1300.kmer.csv', './data/splice-sites/splice-deep/train.ss_pos_don_hs.1300.kmer.csv', './data/splice-sites/splice-deep/train.ss_neg_acc_hs.1300.kmer.csv', './data/splice-sites/splice-deep/train.ss_neg_don_hs.1300.kmer.csv'] into ./data/splice-sites/splice-deep/train.1300.kmer.csv: True\n",
      "Merging ['./data/splice-sites/splice-deep/train.ss_pos_acc_hs.3000.kmer.csv', './data/splice-sites/splice-deep/train.ss_pos_don_hs.3000.kmer.csv', './data/splice-sites/splice-deep/train.ss_neg_acc_hs.3000.kmer.csv', './data/splice-sites/splice-deep/train.ss_neg_don_hs.3000.kmer.csv'] into ./data/splice-sites/splice-deep/train.3000.kmer.csv: True\n",
      "Merging ['./data/splice-sites/splice-deep/validation.ss_pos_acc_hs.1300.kmer.csv', './data/splice-sites/splice-deep/validation.ss_pos_don_hs.1300.kmer.csv', './data/splice-sites/splice-deep/validation.ss_neg_acc_hs.1300.kmer.csv', './data/splice-sites/splice-deep/validation.ss_neg_don_hs.1300.kmer.csv'] into ./data/splice-sites/splice-deep/validation.1300.kmer.csv: True\n",
      "Merging ['./data/splice-sites/splice-deep/validation.ss_pos_acc_hs.3000.kmer.csv', './data/splice-sites/splice-deep/validation.ss_pos_don_hs.3000.kmer.csv', './data/splice-sites/splice-deep/validation.ss_neg_acc_hs.3000.kmer.csv', './data/splice-sites/splice-deep/validation.ss_neg_don_hs.3000.kmer.csv'] into ./data/splice-sites/splice-deep/validation.3000.kmer.csv: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Merge all positive acceptor and donor, and negative acceptor and donor into single file for training and single file for validation\n",
    "\"\"\"\n",
    "from data_dir import ss_neg_acc_hs_kmer_csv, ss_neg_don_hs_kmer_csv, ss_pos_acc_hs_kmer_csv, ss_pos_don_hs_kmer_csv, ss_dir\n",
    "from data_dir import ss_train_csv\n",
    "from data_preparation import merge_csv\n",
    "\n",
    "sizes = [1300, 3000]\n",
    "for _type in ['train', 'validation']:\n",
    "    for _size in sizes:\n",
    "        ss_pos_acc_hs = '{}/{}.ss_pos_acc_hs.{}.kmer.csv'.format(ss_dir, _type, _size)\n",
    "        ss_pos_don_hs = '{}/{}.ss_pos_don_hs.{}.kmer.csv'.format(ss_dir, _type, _size)\n",
    "        ss_neg_acc_hs = '{}/{}.ss_neg_acc_hs.{}.kmer.csv'.format(ss_dir, _type, _size)\n",
    "        ss_neg_don_hs = '{}/{}.ss_neg_don_hs.{}.kmer.csv'.format(ss_dir, _type, _size)\n",
    "        _files = [ss_pos_acc_hs, ss_pos_don_hs, ss_neg_acc_hs, ss_neg_don_hs]\n",
    "        _train_file = '{}/{}.{}.kmer.csv'.format(ss_dir, _type, _size)\n",
    "        print(\"Merging {} into {}: {}\".format(_files, _train_file, merge_csv(_files, _train_file)))\n",
    "\n",
    "#files = [ss_neg_acc_hs_kmer_csv, ss_neg_don_hs_kmer_csv, ss_pos_acc_hs_kmer_csv, ss_pos_don_hs_kmer_csv]\n",
    "#print(\"Merging {} into {}: {}\".format(files, ss_train_csv, merge_csv(files, ss_train_csv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error File ./data/splice-sites/splice-deep/train.1250.kmer.csv not found.\n",
      "Error Traceback (most recent call last):\n",
      "  File \"w:\\Research\\sequence-processing\\data_preparation.py\", line 1175, in expand_by_sliding_window_no_pandas\n",
      "    print(\"Expanding {} [{}/{}]\".format(src_csv, _count, _len_src), end='\\r')\n",
      "Exception: File ./data/splice-sites/splice-deep/train.1250.kmer.csv not found.\n",
      "\n",
      "Expanding ./data/splice-sites/splice-deep/train.1250.kmer.csv => ./data/splice-sites/splice-deep/train.1250.kmer.expanded.csv: False\n",
      "Expanding ./data/splice-sites/splice-deep/train.2000.kmer.csv [5648/7201]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mw:\\Research\\sequence-processing\\preparation_splice_site.ipynb Cell 10'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/preparation_splice_site.ipynb#ch0000008?line=9'>10</a>\u001b[0m _source \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.kmer.csv\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(ss_dir, _type, _size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/preparation_splice_site.ipynb#ch0000008?line=10'>11</a>\u001b[0m _target \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.kmer.expanded.csv\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(ss_dir, _type, _size)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/preparation_splice_site.ipynb#ch0000008?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mExpanding \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m => \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(_source,_target,expand_by_sliding_window_no_pandas(_source,_target, length\u001b[39m=\u001b[39;49m\u001b[39m510\u001b[39;49m)))\n",
      "File \u001b[1;32mw:\\Research\\sequence-processing\\data_preparation.py:1202\u001b[0m, in \u001b[0;36mexpand_by_sliding_window_no_pandas\u001b[1;34m(src_csv, target_csv, sliding_window_size, length)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from data_dir import ss_dir\n",
    "from data_preparation import expand_by_sliding_window_no_pandas\n",
    "\"\"\"\n",
    "Expand training data (train.csv) and store the expanded file in dataset full ss folder.\n",
    "\"\"\"\n",
    "sizes = [1300, 3000]\n",
    "types = ['train', 'validation']\n",
    "for _type in types:\n",
    "    for _size in sizes:\n",
    "        _source = '{}/{}.{}.kmer.csv'.format(ss_dir, _type, _size)\n",
    "        _target = '{}/{}.{}.kmer.expanded.csv'.format(ss_dir, _type, _size)\n",
    "        print(\"Expanding {} => {}: {}\".format(_source,_target,expand_by_sliding_window_no_pandas(_source,_target, length=510)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 3108/248150 [02:23<3:09:05, 21.60it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9956/1722853449.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mtarget_csv_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'workspace'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'splice-sites'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pos_acc_hs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mss_pos_acc_hs_fasta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0m_prep_ss_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mss_pos_acc_hs_fasta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_csv_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9956/1722853449.py\u001b[0m in \u001b[0;36m_prep_ss_csv\u001b[1;34m(file_path, target_csv_dir, label)\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{line_number},{chunks[i]},{label}\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                 \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mtarget_csv_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'workspace'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'splice-sites'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pos_acc_hs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from data_preparation import kmer\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _prep_ss_csv(file_path, target_csv_dir, label):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File {file_path} not found. Please provide source.\")\n",
    "    f = open(file_path, 'r')\n",
    "    _columns = ['index', 'sequence', 'label']\n",
    "    _columns = ','.join(_columns)\n",
    "    src_name = os.path.basename(file_path)\n",
    "    if not os.path.exists(target_csv_dir):\n",
    "        os.makedirs(target_csv_dir, exist_ok=True)\n",
    "    line_number = 0\n",
    "    for line in tqdm(f, total=248150):\n",
    "        # A line always has 602 characters so break it into 512 chunks.\n",
    "        chunks = kmer(line, 512)\n",
    "        line_number += 1\n",
    "        # Each chunk is saved into different files.\n",
    "        for i in range(len(chunks)):\n",
    "            _tname = src_name.split('.')[0]\n",
    "            tname = \"{}.{}.csv\".format(_tname, i)\n",
    "            tpath = os.path.join(target_csv_dir, tname)\n",
    "            if not os.path.exists(tpath):\n",
    "                t = open(tpath, 'x')\n",
    "                t.write(f\"{_columns}\\n\")\n",
    "                t.close()\n",
    "            else:\n",
    "                t = open(tpath, 'a')\n",
    "                t.write(f\"{line_number},{chunks[i]},{label}\\n\")\n",
    "                t.close()\n",
    "\n",
    "target_csv_dir = os.path.join('workspace', 'splice-sites', 'pos_acc_hs')\n",
    "from data_dir import ss_pos_acc_hs_fasta\n",
    "_prep_ss_csv(ss_pos_acc_hs_fasta, target_csv_dir, 1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "473c7453bcb969eece5b07ef8b7f234e7c84010927f6bebce35f0aeb1f8c121e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('sequence-processing-py39': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
