{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preparation for splice site data.\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "ss_dir = './data/splice-sites/splice-deep/'\n",
    "pos_acc_ss_hs = '{}/positive_DNA_seqs_acceptor_hs.fa'.format(ss_dir)\n",
    "pos_don_ss_hs = '{}/positive_DNA_seqs_donor_hs.fa'.format(ss_dir)\n",
    "neg_acc_ss_hs = '{}/negative_DNA_seqs_acceptor_hs.fa'.format(ss_dir)\n",
    "neg_don_ss_hs = '{}/negative_DNA_seqs_donor_hs.fa'.format(ss_dir)\n",
    "\n",
    "ss_dataset_dir = './dataset/splice-sites'\n",
    "pos_ss_acc_dataset = '{}/pos_ss_acc_hs.csv'.format(ss_dataset_dir)\n",
    "pos_ss_don_dataset = '{}/pos_ss_don_hs.csv'.format(ss_dataset_dir)\n",
    "neg_ss_acc_dataset = '{}/neg_ss_acc_hs.csv'.format(ss_dataset_dir)\n",
    "neg_ss_don_dataset = '{}/neg_ss_don_hs.csv'.format(ss_dataset_dir)\n",
    "\n",
    "files = [(pos_acc_ss_hs, 1, 'acc', pos_ss_acc_dataset), \n",
    "            (pos_don_ss_hs, 1, 'don', pos_ss_don_dataset), \n",
    "            (neg_acc_ss_hs, 0, 'acc', neg_ss_acc_dataset), \n",
    "            (neg_don_ss_hs, 0, 'don', neg_ss_don_dataset)]\n",
    "for p in files:\n",
    "    fname = p[0]\n",
    "    label = p[1]\n",
    "    acc_don = p[2]\n",
    "    dataset_path = p[3]\n",
    "\n",
    "    f = {}\n",
    "    t = {}\n",
    "    if os.path.exists(dataset_path):\n",
    "        os.remove(dataset_path)\n",
    "    try:\n",
    "        f = open(fname, 'r')\n",
    "        t = open(dataset_path, 'x')\n",
    "        t.write('{}\\n'.format(','.join(['sequence', 'label'])))\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            #if len(line) > 512:\n",
    "                #kmers = [line[i:i+512] for i in range(0, len(line)+1-512)] # Make sure everything is 512 character.\n",
    "                #for mer in kmers:\n",
    "                #    t.write('{},{}\\n'.format(mer, label))        \n",
    "            #else:\n",
    "            t.write('{},{}\\n'.format(line, label))\n",
    "        t.close()\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        print('Error {}'.format(e))\n",
    "        t.close()\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create train, validation, and test set for splice site. To do that, the data need to be balance.\n",
    "If not the sampling based on smallest count is required. Processing is done using pandas.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "pos_ss_acc_df = pd.read_csv(pos_ss_acc_dataset)\n",
    "pos_ss_don_df = pd.read_csv(pos_ss_don_dataset)\n",
    "neg_ss_acc_df = pd.read_csv(neg_ss_acc_dataset)\n",
    "neg_ss_don_df = pd.read_csv(neg_ss_don_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because loading the dataframe is time consuming, leave the loading at cell above and do later processing here.\n",
    "pos_ss_acc_size = len(pos_ss_acc_df)\n",
    "pos_ss_don_size = len(pos_ss_don_df)\n",
    "neg_ss_acc_size = len(neg_ss_acc_df)\n",
    "neg_ss_don_size = len(neg_ss_don_df)\n",
    "\n",
    "count = 0\n",
    "if pos_ss_acc_size == pos_ss_don_size == neg_ss_acc_size == neg_ss_don_size:\n",
    "    print('dataset balance')\n",
    "    count = pos_ss_acc_size\n",
    "else:\n",
    "    print('dataset imbalance')\n",
    "    print('pos acc {}\\npos don {}\\nneg acc {}\\nneg don {}'.format(pos_ss_acc_size, pos_ss_don_size, neg_ss_acc_size, neg_ss_don_size))\n",
    "    count = min([pos_ss_acc_size, pos_ss_don_size, neg_ss_acc_size, neg_ss_don_size])\n",
    "    print('count = {}'.format(count))\n",
    "\n",
    "pos_ss_acc_df_sample = pos_ss_acc_df.sample(n=count, replace=False, random_state=1337)\n",
    "pos_ss_don_df_sample = pos_ss_don_df.sample(n=count, replace=False, random_state=1337)\n",
    "neg_ss_acc_df_sample = neg_ss_acc_df.sample(n=count, replace=False, random_state=1337)\n",
    "neg_ss_don_df_sample = neg_ss_don_df.sample(n=count, replace=False, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ss_acc_balanced = './dataset/splice-sites/pos_ss_acc_balanced.csv'\n",
    "pos_ss_don_balanced = './dataset/splice-sites/pos_ss_don_balanced.csv'\n",
    "neg_ss_acc_balanced = './dataset/splice-sites/neg_ss_acc_balanced.csv'\n",
    "neg_ss_don_balanced = './dataset/splice-sites/neg_ss_don_balanced.csv'\n",
    "\n",
    "pos_ss_acc_df_sample.to_csv(pos_ss_acc_balanced, index=False)\n",
    "pos_ss_don_df_sample.to_csv(pos_ss_don_balanced, index=False)\n",
    "neg_ss_acc_df_sample.to_csv(neg_ss_acc_balanced, index=False)\n",
    "neg_ss_don_df_sample.to_csv(neg_ss_don_balanced, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation import generate_datasets\n",
    "\n",
    "pos_ss_acc_balanced_dir = './dataset/splice-sites/pos_ss_acc_dataset'\n",
    "pos_ss_don_balanced_dir = './dataset/splice-sites/pos_ss_don_dataset'\n",
    "neg_ss_acc_balanced_dir = './dataset/splice-sites/neg_ss_acc_dataset'\n",
    "neg_ss_don_balanced_dir = './dataset/splice-sites/neg_ss_don_dataset'\n",
    "\n",
    "dirs = [pos_ss_acc_balanced_dir, pos_ss_don_balanced_dir, neg_ss_acc_balanced_dir, neg_ss_don_balanced_dir]\n",
    "for d in dirs:\n",
    "    if os.path.exists(d):\n",
    "        os.mkdir(d)\n",
    "\n",
    "pos_ss_acc_dataset = generate_datasets(pos_ss_acc_balanced, pos_ss_acc_balanced_dir)\n",
    "pos_ss_don_dataset = generate_datasets(pos_ss_don_balanced, pos_ss_don_balanced_dir)\n",
    "neg_ss_acc_dataset = generate_datasets(neg_ss_acc_balanced, neg_ss_acc_balanced_dir)\n",
    "neg_ss_don_dataset = generate_datasets(neg_ss_don_balanced, neg_ss_don_balanced_dir)\n",
    "\n",
    "print(pos_ss_acc_dataset)\n",
    "print(pos_ss_don_dataset)\n",
    "print(neg_ss_acc_dataset)\n",
    "print(neg_ss_don_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation import generate_sample\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "Create sample splice sites training and validation data.\n",
    "\"\"\"\n",
    "pos_ss_acc_balanced_dir = './dataset/splice-sites/pos_ss_acc_dataset'\n",
    "pos_ss_don_balanced_dir = './dataset/splice-sites/pos_ss_don_dataset'\n",
    "neg_ss_acc_balanced_dir = './dataset/splice-sites/neg_ss_acc_dataset'\n",
    "neg_ss_don_balanced_dir = './dataset/splice-sites/neg_ss_don_dataset'\n",
    "\n",
    "n_sample = {\n",
    "    'train': 80,\n",
    "    'validation': 10,\n",
    "    'test': 10\n",
    "}\n",
    "ss_dir = [pos_ss_acc_balanced_dir, pos_ss_don_balanced_dir, neg_ss_acc_balanced_dir, neg_ss_don_balanced_dir]\n",
    "ss_file = ['train', 'validation', 'test']\n",
    "\n",
    "for d in ss_dir:\n",
    "    for s in ss_file:\n",
    "        filepath = '{}/{}.csv'.format(d, s)\n",
    "        targetpath = '{}/{}_sample.csv'.format(d, s)\n",
    "        df = pd.read_csv(filepath)\n",
    "        sample_df = df.sample(n=n_sample[s], random_state=1337)\n",
    "        sample_df.to_csv(targetpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence length is more than 512 characters so it needs to be expanded.\n",
    "import pandas as pd\n",
    "pos_ss_acc_balanced_dir = './dataset/splice-sites/pos_ss_acc_dataset'\n",
    "pos_ss_don_balanced_dir = './dataset/splice-sites/pos_ss_don_dataset'\n",
    "neg_ss_acc_balanced_dir = './dataset/splice-sites/neg_ss_acc_dataset'\n",
    "neg_ss_don_balanced_dir = './dataset/splice-sites/neg_ss_don_dataset'\n",
    "\n",
    "n_sample = {\n",
    "    'train': 80,\n",
    "    'validation': 10,\n",
    "    'test': 10\n",
    "}\n",
    "ss_dir = [pos_ss_acc_balanced_dir, pos_ss_don_balanced_dir, neg_ss_acc_balanced_dir, neg_ss_don_balanced_dir]\n",
    "ss_file = ['train', 'validation', 'test']\n",
    "\n",
    "for s in ss_dir:\n",
    "    for f in ss_file:\n",
    "        fpath = '{}/{}_sample.csv'.format(s, f)\n",
    "        gpath = '{}/{}_sample_expanded.csv'.format(s, f)\n",
    "        if os.path.exists(gpath):\n",
    "            os.remove(gpath)\n",
    "        g = open(gpath, 'x')\n",
    "        g.write('{},{}\\n'.format('sequence', 'label'))\n",
    "\n",
    "        df = pd.read_csv(fpath)\n",
    "        for i, r in df.iterrows():\n",
    "            seq = r['sequence']\n",
    "            label = r['label']\n",
    "            if len(seq)> 512:\n",
    "                kmers = [seq[i:i+512] for i in range(len(seq)+1-512)]\n",
    "                for mer in kmers:\n",
    "                    g.write('{},{}\\n'.format(mer, label))\n",
    "            else:\n",
    "                g.write('{},{}\\n'.format(seq, label))\n",
    "\n",
    "        g.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
