{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Text: Moby Dick by Herman Melville 1851>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The big black Kakatua flew onto a tree branch.\"\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'DT'),\n",
       "  ('big', 'JJ'),\n",
       "  ('black', 'JJ'),\n",
       "  ('Kakatua', 'NNP'),\n",
       "  ('flew', 'VBD'),\n",
       "  ('onto', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('tree', 'JJ'),\n",
       "  ('branch', 'NN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(sentence)\n",
    "sentences = [nltk.word_tokenize(s) for s in sentences]\n",
    "sentences = [nltk.pos_tag(s) for s in sentences]\n",
    "sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,472.0,168.0\" width=\"472px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"8.47458%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">The</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"4.23729%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"8.47458%\" x=\"8.47458%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">big</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.7119%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"11.8644%\" x=\"16.9492%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">black</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.8814%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"15.2542%\" x=\"28.8136%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Kakatua</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"36.4407%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"10.1695%\" x=\"44.0678%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">flew</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"49.1525%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"10.1695%\" x=\"54.2373%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">onto</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"59.322%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"30.5085%\" x=\"64.4068%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"22.2222%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.1111%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"33.3333%\" x=\"22.2222%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">tree</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"38.8889%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"44.4444%\" x=\"55.5556%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">branch</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"77.7778%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.661%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.08475%\" x=\"94.9153%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"97.4576%\" y1=\"1.2em\" y2=\"3em\" /></svg>",
      "text/plain": [
       "Tree('S', [('The', 'DT'), ('big', 'JJ'), ('black', 'JJ'), ('Kakatua', 'NNP'), ('flew', 'VBD'), ('onto', 'IN'), Tree('NP', [('a', 'DT'), ('tree', 'JJ'), ('branch', 'NN')]), ('.', '.')])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "cp.parse(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 2502, 2304, 100, 5520, 3031, 1037, 3392, 100]\n",
      "['[UNK]', 'big', 'black', '[UNK]', 'flew', 'onto', 'a', 'tree', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import os\n",
    "\n",
    "sentence = \"The big black Kakatua flew onto a tree branch.\"\n",
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(\"pretrained\", \"bert-base-uncased\"))\n",
    "token_ids = tokenizer.encode(sentence.split(\" \"), add_special_tokens=False, )\n",
    "print(token_ids)\n",
    "print([tokenizer.convert_ids_to_tokens(t) for t in token_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 35, 61, 40, 18, 57, 24, 19, 61, 38, 11, 32, 51, 64, 51, 61, 38, 12, 35, 61, 39, 14, 44, 33, 55, 14, 41, 24, 19]\n"
     ]
    }
   ],
   "source": [
    "sequence = ['ATG', 'TGC', 'GCA', 'CAG', 'AGT', 'GTA', 'TAG', 'AGC', 'GCA', 'CAT', 'ATC', 'TCG', 'CGC', 'GCG', 'CGC', 'GCA', 'CAT', 'ATG', 'TGC', 'GCA', 'CAC', 'ACT', 'CTG', 'TGA', 'GAC', 'ACT', 'CTA', 'TAG', 'AGC']\n",
    "from transformers import BertTokenizer\n",
    "import os\n",
    "gtokenizer = BertTokenizer.from_pretrained(os.path.join(\"pretrained\", \"3-new-12w-0\"))\n",
    "gtoken_ids = gtokenizer.encode(sequence, add_special_tokens=False)\n",
    "print(gtoken_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 208k/208k [00:01<00:00, 196kB/s]  \n",
      "Downloading: 100%|██████████| 60.0/60.0 [00:00<00:00, 30.0kB/s]\n",
      "Downloading: 100%|██████████| 998/998 [00:00<00:00, 499kB/s]\n",
      "Downloading: 100%|██████████| 1.24G/1.24G [35:48<00:00, 621kB/s]    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('pretrained\\\\bert-large-cased-finetuned-conll03-english\\\\tokenizer_config.json',\n",
       " 'pretrained\\\\bert-large-cased-finetuned-conll03-english\\\\special_tokens_map.json',\n",
       " 'pretrained\\\\bert-large-cased-finetuned-conll03-english\\\\vocab.txt',\n",
       " 'pretrained\\\\bert-large-cased-finetuned-conll03-english\\\\added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "model = BertForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "tokenizer.save_pretrained(os.path.join(\"pretrained\", \"bert-large-cased-finetuned-conll03-english\"))\n",
    "model.save_pretrained(os.path.join(\"pretrained\", \"bert-large-cased-finetuned-conll03-english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_special_tokens(tokenizer):\n",
    "    pad_tok = tokenizer.vocab[\"[PAD]\"]\n",
    "    sep_tok = tokenizer.vocab[\"[SEP]\"]\n",
    "    cls_tok = tokenizer.vocab[\"[CLS]\"]\n",
    "\n",
    "    return pad_tok, sep_tok, cls_tok\n",
    "\n",
    "get_special_tokens(BertTokenizer.from_pretrained(os.path.join(\"pretrained\", \"3-new-12w-0\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Split: ['learn', 'php', 'from', 'guru99', 'and', 'make', 'study', 'easy']\n",
      "After Token: [('learn', 'JJ'), ('php', 'NN'), ('from', 'IN'), ('guru99', 'NN'), ('and', 'CC'), ('make', 'VB'), ('study', 'NN'), ('easy', 'JJ')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk learn/JJ)\n",
      "  (mychunk php/NN)\n",
      "  from/IN\n",
      "  (mychunk guru99/NN and/CC)\n",
      "  make/VB\n",
      "  (mychunk study/NN easy/JJ))\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "text =\"learn php from guru99 and make study easy\".split()\n",
    "print(\"After Split:\",text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After Token:\",tokens_tag)\n",
    "patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex:\",chunker)\n",
    "output = chunker.parse(tokens_tag)\n",
    "print(\"After Chunking\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ATG',\n",
       " 'TGC',\n",
       " 'GCA',\n",
       " 'CAG',\n",
       " 'AGT',\n",
       " 'GTA',\n",
       " 'TAG',\n",
       " 'AGC',\n",
       " 'GCA',\n",
       " 'CAT',\n",
       " 'ATC',\n",
       " 'TCG',\n",
       " 'CGC',\n",
       " 'GCG',\n",
       " 'CGC',\n",
       " 'GCA',\n",
       " 'CAT',\n",
       " 'ATG',\n",
       " 'TGC',\n",
       " 'GCA',\n",
       " 'CAC',\n",
       " 'ACT',\n",
       " 'CTG',\n",
       " 'TGA',\n",
       " 'GAC',\n",
       " 'ACT',\n",
       " 'CTA',\n",
       " 'TAG',\n",
       " 'AGC']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_preparation import str_kmer, kmer\n",
    "genetic_sequence = \"ATGCAGTAGCATCGCGCATGCACTGACTAGC\"\n",
    "kmer = kmer(genetic_sequence, 3)\n",
    "kmer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('deep-learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb381ed8bacaf36aa3bfaca5a0502d4671ddf79cb6e63c342c2d7fda9a71fcc6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
