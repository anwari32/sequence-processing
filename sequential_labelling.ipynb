{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, BertModel\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "\n",
    "bertForTokenClassification = BertForTokenClassification.from_pretrained(pretrained_3kmer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/genome/grch38/exon/NC_000024.10.csv']\n",
      "['./data/chr/NC_000024.10.fasta']\n",
      "['./data/genome/labseq/chr24.csv']\n"
     ]
    }
   ],
   "source": [
    "from data_dir import chr24_index_csv, chr24_fasta, labseq_dir, labseq_names\n",
    "chr_indices = [chr24_index_csv]\n",
    "chr_fastas = [chr24_fasta]\n",
    "chr_labseq_path = [\"{}/{}\".format(labseq_dir, fname) for fname in [labseq_names[-1]]]\n",
    "print(chr_indices)\n",
    "print(chr_fastas)\n",
    "print(chr_labseq_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index ./data/genome/grch38/exon/NC_000024.10.csv, with fasta ./data/chr/NC_000024.10.fasta, to seq. labelling ./data/genome/labseq/chr24.csv, expanding [5431760/57226904]"
     ]
    }
   ],
   "source": [
    "from data_dir import chr24_index_csv, chr24_fasta, labseq_dir, labseq_names\n",
    "from data_preparation import generate_sequence_labelling\n",
    "chr_indices = [chr24_index_csv]\n",
    "chr_fastas = [chr24_fasta]\n",
    "chr_labseq_path = [\"{}/{}\".format(labseq_dir, fname) for fname in [labseq_names[-1]]]\n",
    "for src, fasta, target in zip(chr_indices, chr_fastas, chr_labseq_path):\n",
    "    print(\"Generating sequential labelling for index {}, from fasta {}, to {}: {}\".format(src, fasta, target, generate_sequence_labelling(src, fasta, target, do_expand=True, expand_size=512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "from sequential_labelling import preprocessing, initialize_seq2seq\n",
    "\n",
    "\"\"\"\n",
    "Initialize model and tokenizer.\n",
    "\"\"\"\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_3kmer_dir)\n",
    "in_out_dimensions = [768, 512, 512, 10]\n",
    "model = initialize_seq2seq(pretrained_3kmer_dir, in_out_dimensions)\n",
    "#print(model)\n",
    "\n",
    "\"\"\"\n",
    "Create sample data sequential labelling.\n",
    "\"\"\"\n",
    "from random import randint\n",
    "from data_preparation import kmer\n",
    "from sequential_labelling import process_sequence_and_label, create_dataloader\n",
    "sequences = ['ATGC' * 128, 'TGAC' * 128, 'GATC' * 128, \"AGCC\" * 128]\n",
    "labels = [['E' if randint(0, 255) % 2 == 0 else '.' for i in range(len(s))] for s in sequences]\n",
    "\n",
    "kmer_seq = [' '.join(kmer(sequence, 3)) for sequence in sequences]\n",
    "kmer_label = [' '.join(kmer(''.join(label), 3)) for label in labels]\n",
    "\n",
    "arr_input_ids = []\n",
    "arr_attn_mask = []\n",
    "arr_label_repr = []\n",
    "for seq, label in zip(kmer_seq, kmer_label):\n",
    "    input_ids, attn_mask, label_repr = process_sequence_and_label(seq, label, tokenizer)\n",
    "    arr_input_ids.append(input_ids)\n",
    "    arr_attn_mask.append(attn_mask)\n",
    "    arr_label_repr.append(label_repr)\n",
    "\n",
    "dataloader = create_dataloader(arr_input_ids, arr_attn_mask, arr_label_repr, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation torch.Size([2, 512, 10]) tensor([[[0.0335, 0.1366, 0.2869,  ..., 0.0582, 0.0439, 0.0373],\n",
      "         [0.0534, 0.0582, 0.1790,  ..., 0.0410, 0.0642, 0.1359],\n",
      "         [0.1073, 0.1038, 0.1294,  ..., 0.0689, 0.2670, 0.0596],\n",
      "         ...,\n",
      "         [0.0502, 0.0610, 0.1907,  ..., 0.0427, 0.0626, 0.1263],\n",
      "         [0.1091, 0.1058, 0.1277,  ..., 0.0690, 0.2639, 0.0587],\n",
      "         [0.0335, 0.1366, 0.2869,  ..., 0.0582, 0.0439, 0.0373]],\n",
      "\n",
      "        [[0.0247, 0.0701, 0.1186,  ..., 0.0540, 0.0300, 0.0743],\n",
      "         [0.0973, 0.0563, 0.1604,  ..., 0.1201, 0.1110, 0.0659],\n",
      "         [0.0378, 0.0617, 0.0935,  ..., 0.1416, 0.0607, 0.1822],\n",
      "         ...,\n",
      "         [0.0957, 0.0594, 0.1599,  ..., 0.1230, 0.1072, 0.0659],\n",
      "         [0.0368, 0.0624, 0.0940,  ..., 0.1403, 0.0591, 0.1861],\n",
      "         [0.0247, 0.0701, 0.1186,  ..., 0.0540, 0.0300, 0.0743]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "label repr torch.Size([2, 512]) tensor([[0, 1, 2,  ..., 7, 5, 9],\n",
      "        [0, 7, 5,  ..., 7, 5, 9]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [2, 10], got [2, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mw:\\Research\\sequence-processing\\sequential_labelling.ipynb Cell 5'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000004?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mactivation\u001b[39m\u001b[39m\"\u001b[39m, activation\u001b[39m.\u001b[39mshape, activation)\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000004?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlabel repr\u001b[39m\u001b[39m\"\u001b[39m, label_repr\u001b[39m.\u001b[39mshape, label_repr)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000004?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(activation, label_repr)\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\torch\\nn\\modules\\loss.py:211\u001b[0m, in \u001b[0;36mNLLLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/loss.py?line=209'>210</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/loss.py?line=210'>211</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnll_loss(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\torch\\nn\\functional.py:2532\u001b[0m, in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/functional.py?line=2529'>2530</a>\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/functional.py?line=2530'>2531</a>\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/functional.py?line=2531'>2532</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mnll_loss_nd(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected target size [2, 10], got [2, 512]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Play with result.\n",
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "loss_fn = nn.NLLLoss()\n",
    "activation_fn = nn.Softmax(dim=2)\n",
    "linear = nn.Linear(in_features=768, out_features=10)\n",
    "for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "    input_ids, attn_mask, label_repr = tuple(t for t in batch)\n",
    "    output = model(input_ids, attn_mask)\n",
    "    bert_output = model.bert(input_ids, attn_mask)\n",
    "    bert_output = bert_output[0] # tensor.Size([batch_size, seq_length, dim])\n",
    "    linear_output = linear(bert_output) # tensor.Size([batch_size, seq_length, dim])\n",
    "    activation = activation_fn(linear_output)\n",
    "    print(\"activation\", activation.shape, activation)\n",
    "    print(\"label repr\", label_repr.shape, label_repr)\n",
    "    loss = loss_fn(activation, label_repr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print('linear output {}'.format(linear_output.shape))\n",
    "    linear_output_permute = linear_output.permute(0, 2, 1) # tensor.Size([batch_size, dim, seq_length])\n",
    "    print('linear output permute {}'.format(linear_output_permute.shape))\n",
    "    print('label repr {}'.format(label_repr.shape))\n",
    "    activation = activation_fn(linear_output_permute)\n",
    "    print('activation {}'.format(activation.shape))\n",
    "    loss = loss_fn(linear_output_permute, label_repr)\n",
    "    print('loss {}'.format(linear_output_permute))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 10]) tensor([[[0.0761, 0.0309, 0.1782, 0.3150, 0.0041, 0.0667, 0.0346, 0.0324,\n",
      "          0.1923, 0.0697],\n",
      "         [0.0491, 0.0709, 0.1599, 0.1308, 0.0236, 0.0327, 0.2852, 0.0152,\n",
      "          0.0579, 0.1746],\n",
      "         [0.1077, 0.0813, 0.0841, 0.0973, 0.0452, 0.0425, 0.0752, 0.0313,\n",
      "          0.1997, 0.2358],\n",
      "         [0.1600, 0.1957, 0.1299, 0.1179, 0.0709, 0.0377, 0.1201, 0.0344,\n",
      "          0.0467, 0.0868],\n",
      "         [0.1917, 0.0216, 0.0058, 0.1390, 0.0640, 0.0325, 0.2813, 0.0407,\n",
      "          0.0276, 0.1961]],\n",
      "\n",
      "        [[0.0189, 0.2757, 0.0623, 0.0221, 0.0229, 0.1040, 0.0658, 0.0579,\n",
      "          0.0653, 0.3052],\n",
      "         [0.0463, 0.0238, 0.0840, 0.4257, 0.0258, 0.0374, 0.1712, 0.0945,\n",
      "          0.0349, 0.0564],\n",
      "         [0.0105, 0.0182, 0.2384, 0.0794, 0.0994, 0.2283, 0.1128, 0.1145,\n",
      "          0.0499, 0.0485],\n",
      "         [0.0164, 0.0236, 0.1261, 0.0135, 0.1104, 0.3142, 0.1054, 0.0131,\n",
      "          0.1640, 0.1134],\n",
      "         [0.0067, 0.0832, 0.1345, 0.1329, 0.1143, 0.0975, 0.0780, 0.1197,\n",
      "          0.1971, 0.0363]],\n",
      "\n",
      "        [[0.0477, 0.0160, 0.0389, 0.0668, 0.0833, 0.0298, 0.2603, 0.2925,\n",
      "          0.0597, 0.1051],\n",
      "         [0.0255, 0.2318, 0.0714, 0.1179, 0.0673, 0.0531, 0.2010, 0.0886,\n",
      "          0.0682, 0.0752],\n",
      "         [0.0394, 0.0680, 0.0966, 0.0636, 0.0079, 0.1252, 0.0206, 0.0448,\n",
      "          0.2099, 0.3239],\n",
      "         [0.4167, 0.0215, 0.0430, 0.0438, 0.0618, 0.0977, 0.0782, 0.1846,\n",
      "          0.0201, 0.0326],\n",
      "         [0.1338, 0.0446, 0.0423, 0.1575, 0.0861, 0.0782, 0.1435, 0.0288,\n",
      "          0.2614, 0.0239]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([3, 5, 10]) tensor([[[3., 3., 4., 0., 2., 2., 2., 3., 2., 3.],\n",
      "         [3., 4., 4., 0., 3., 1., 2., 0., 1., 0.],\n",
      "         [1., 1., 0., 3., 0., 3., 2., 3., 4., 4.],\n",
      "         [4., 4., 2., 3., 0., 1., 0., 1., 1., 1.],\n",
      "         [3., 4., 3., 0., 4., 2., 4., 2., 0., 3.]],\n",
      "\n",
      "        [[3., 3., 3., 4., 3., 4., 1., 0., 4., 3.],\n",
      "         [0., 2., 3., 0., 2., 3., 1., 4., 0., 1.],\n",
      "         [1., 1., 2., 1., 2., 4., 0., 0., 4., 2.],\n",
      "         [3., 1., 2., 3., 2., 2., 4., 2., 3., 1.],\n",
      "         [1., 0., 4., 1., 0., 1., 3., 4., 0., 3.]],\n",
      "\n",
      "        [[2., 2., 0., 4., 2., 3., 4., 1., 4., 1.],\n",
      "         [2., 4., 4., 1., 2., 0., 2., 0., 4., 1.],\n",
      "         [2., 2., 4., 4., 4., 0., 4., 3., 2., 4.],\n",
      "         [3., 2., 1., 0., 3., 2., 1., 0., 4., 4.],\n",
      "         [1., 1., 1., 4., 1., 3., 3., 2., 2., 4.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "loss = nn.CrossEntropyLoss()\n",
    "activate = nn.Softmax(dim=2)\n",
    "input = torch.randn(3, 5, 10, requires_grad=True, dtype=torch.float)\n",
    "input = activate(input)\n",
    "target = torch.empty(3, 5, 10, dtype=torch.float).random_(5)\n",
    "output = loss(input, target)\n",
    "print(input.shape, input)\n",
    "print(target.shape, target)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "473c7453bcb969eece5b07ef8b7f234e7c84010927f6bebce35f0aeb1f8c121e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('sequence-processing-py39': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
