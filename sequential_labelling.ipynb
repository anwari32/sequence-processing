{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [1, 11], got [1, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mw:\\Research\\sequence-processing\\sequential_labelling.ipynb Cell 1'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000000?line=11'>12</a>\u001b[0m pred \u001b[39m=\u001b[39m model(input_ids, attn_mask, token_type_ids)\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000000?line=12'>13</a>\u001b[0m \u001b[39m# print(pred.shape, label.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000000?line=13'>14</a>\u001b[0m \u001b[39m#for p, l in zip(pred, label):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000000?line=14'>15</a>\u001b[0m \u001b[39m#    print(p.shape, l.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000000?line=15'>16</a>\u001b[0m \u001b[39m#    loss = loss_fn(p, l)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000000?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, label)\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1150\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/loss.py?line=1148'>1149</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/loss.py?line=1149'>1150</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/loss.py?line=1150'>1151</a>\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/loss.py?line=1151'>1152</a>\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\torch\\nn\\functional.py:2846\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/functional.py?line=2843'>2844</a>\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/functional.py?line=2844'>2845</a>\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/functional.py?line=2845'>2846</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected target size [1, 11], got [1, 512]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils.data_generator import _data_generator_seq2seq\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "from sequential_labelling import init_seq2seq_model\n",
    "\n",
    "dataloader = _data_generator_seq2seq()\n",
    "model = init_seq2seq_model(pretrained_3kmer_dir)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# print(model)\n",
    "for step, batch in enumerate(dataloader):\n",
    "    input_ids, attn_mask, token_type_ids, label = tuple(t for t in batch)\n",
    "    pred = model(input_ids, attn_mask, token_type_ids)\n",
    "    # print(pred.shape, label.shape)\n",
    "    #for p, l in zip(pred, label):\n",
    "    #    print(p.shape, l.shape)\n",
    "    #    loss = loss_fn(p, l)\n",
    "    loss = loss_fn(pred, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3006, 0.3322, 0.3672],\n",
      "         [0.3006, 0.3322, 0.3672],\n",
      "         [0.3006, 0.3322, 0.3672],\n",
      "         [0.3006, 0.3322, 0.3672],\n",
      "         [0.3006, 0.3322, 0.3672]],\n",
      "\n",
      "        [[0.3420, 0.3780, 0.2800],\n",
      "         [0.3006, 0.3322, 0.3672],\n",
      "         [0.3006, 0.3322, 0.3672],\n",
      "         [0.3006, 0.3322, 0.3672],\n",
      "         [0.3907, 0.2894, 0.3199]]])\n",
      "torch.Size([2, 5, 3])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mw:\\Research\\sequence-processing\\sequential_labelling.ipynb Cell 2'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000001?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(pred\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000001?line=22'>23</a>\u001b[0m label \u001b[39m=\u001b[39m [\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000001?line=23'>24</a>\u001b[0m     [], \u001b[39m# Label Sentence 0\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000001?line=24'>25</a>\u001b[0m     \u001b[39m0\u001b[39m, \u001b[39m# Label Sentence 1\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000001?line=25'>26</a>\u001b[0m ]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000001?line=26'>27</a>\u001b[0m label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000001?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(label\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000001?line=29'>30</a>\u001b[0m fn \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[1;31mTypeError\u001b[0m: not a sequence"
     ]
    }
   ],
   "source": [
    "pred = [\n",
    "    [   # Sentence 0\n",
    "        [0.1, 0.2, 0.3, ], # Token 0\n",
    "        [0.1, 0.2, 0.3, ], # Token 1\n",
    "        [0.1, 0.2, 0.3, ], # Token 2\n",
    "        [0.1, 0.2, 0.3, ], # Token 3\n",
    "        [0.1, 0.2, 0.3, ], # Token 4\n",
    "    ],\n",
    "    [   # Sentence 1\n",
    "        [0.3, 0.4, 0.1, ], # Token 0\n",
    "        [0.2, 0.3, 0.4, ], # Token 1\n",
    "        [0.2, 0.3, 0.4, ], # Token 2\n",
    "        [0.1, 0.2, 0.3, ], # Token 3\n",
    "        [0.4, 0.1, 0.2, ], # Token 4\n",
    "    ],\n",
    "]\n",
    "import torch\n",
    "pred = torch.tensor(pred)\n",
    "pred = torch.nn.Softmax(dim=2)(pred)\n",
    "print(pred)\n",
    "print(pred.shape)\n",
    "\n",
    "label = [\n",
    "    [], # Label Sentence 0\n",
    "    0, # Label Sentence 1\n",
    "]\n",
    "label = torch.tensor(label)\n",
    "print(label.shape)\n",
    "\n",
    "fn = torch.nn.CrossEntropyLoss()\n",
    "loss = fn(pred, label)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, BertModel\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "\n",
    "bertForTokenClassification = BertForTokenClassification.from_pretrained(pretrained_3kmer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/genome/grch38/exon/NC_000024.10.csv']\n",
      "['./data/chr/NC_000024.10.fasta']\n",
      "['./data/genome/labseq/chr24.csv']\n"
     ]
    }
   ],
   "source": [
    "from data_dir import chr24_index_csv, chr24_fasta, labseq_dir, labseq_names\n",
    "chr_indices = [chr24_index_csv]\n",
    "chr_fastas = [chr24_fasta]\n",
    "chr_labseq_path = [\"{}/{}\".format(labseq_dir, fname) for fname in [labseq_names[-1]]]\n",
    "print(chr_indices)\n",
    "print(chr_fastas)\n",
    "print(chr_labseq_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index ./data/genome/grch38/exon/NC_000024.10.csv, with fasta ./data/chr/NC_000024.10.fasta, to seq. labelling ./data/genome/labseq/chr24.csv, expanding [5431760/57226904]"
     ]
    }
   ],
   "source": [
    "from data_dir import chr24_index_csv, chr24_fasta, labseq_dir, labseq_names\n",
    "from data_preparation import generate_sequence_labelling\n",
    "chr_indices = [chr24_index_csv]\n",
    "chr_fastas = [chr24_fasta]\n",
    "chr_labseq_path = [\"{}/{}\".format(labseq_dir, fname) for fname in [labseq_names[-1]]]\n",
    "for src, fasta, target in zip(chr_indices, chr_fastas, chr_labseq_path):\n",
    "    print(\"Generating sequential labelling for index {}, from fasta {}, to {}: {}\".format(src, fasta, target, generate_sequence_labelling(src, fasta, target, do_expand=True, expand_size=512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 11]) tensor([0.0415, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0046, 0.0000], grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 512, 11]) tensor([0.0944, 0.0905, 0.0905, 0.0905, 0.0905, 0.0905, 0.0905, 0.0905, 0.0905,\n",
      "        0.0909, 0.0905], grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 512, 11]) tensor([0.0000, 0.0000, 0.0045, 0.0002, 0.0000, 0.0000, -0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.1010], grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 512, 11]) tensor([0.0900, 0.0900, 0.0904, 0.0900, 0.0900, 0.0900, 0.0900, 0.0900, 0.0900,\n",
      "        0.0900, 0.0996], grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 512, 11]) tensor([0.0000, 0.0178, 0.0000, 0.0312, 0.0000, 0.0000, 0.0000, 0.0352, 0.1106,\n",
      "        0.0142, 0.0000], grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 512, 11]) tensor([0.0892, 0.0908, 0.0892, 0.0920, 0.0892, 0.0892, 0.0892, 0.0923, 0.0996,\n",
      "        0.0904, 0.0892], grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 512, 11]) tensor([-0.0000, 0.0000, 0.0000, 0.0000, 0.1084, -0.0000, 0.0000, 0.0000, 0.0193,\n",
      "        0.0000, -0.0000], grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 512, 11]) tensor([0.0898, 0.0898, 0.0898, 0.0898, 0.1001, 0.0898, 0.0898, 0.0898, 0.0916,\n",
      "        0.0898, 0.0898], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from utils.data_generator import _data_generator_seq2seq\n",
    "from models.seq2seq import DNABERTSeq2Seq\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "dataloader = _data_generator_seq2seq()\n",
    "model = DNABERTSeq2Seq(pretrained_3kmer_dir)\n",
    "\n",
    "for step, batch in enumerate(dataloader):\n",
    "    input_ids, attn_mask, token_type_ids, label = tuple(t for t in batch)\n",
    "    pred = model(input_ids, attn_mask, token_type_ids)\n",
    "    # print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998000000000001"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [0.0944, 0.0905, 0.0905, 0.0905, 0.0905, 0.0905, 0.0905, 0.0905, 0.0905,\n",
    "        0.0909, 0.0905]\n",
    "sum(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/4 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DNABERTSeq2Seq' object has no attribute 'loss_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mw:\\Research\\sequence-processing\\sequential_labelling.ipynb Cell 9'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000008?line=21'>22</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000008?line=22'>23</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000008?line=23'>24</a>\u001b[0m \u001b[39mPlay with result.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000008?line=24'>25</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/w%3A/Research/sequence-processing/sequential_labelling.ipynb#ch0000008?line=25'>26</a>\u001b[0m trained_model \u001b[39m=\u001b[39m train(model, optimizer, optim_scheduler, dataloader, \u001b[39m10\u001b[39;49m, \u001b[39m2\u001b[39;49m, log_path, save_path, device)\n",
      "File \u001b[1;32mw:\\Research\\sequence-processing\\sequential_labelling.py:205\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, scheduler, train_dataloader, epoch_size, batch_size, log_path, save_model_path, device, remove_old_model, training_counter, resume_from_checkpoint, resume_from_optimizer, grad_accumulation_step, loss_function)\u001b[0m\n\u001b[0;32m    <a href='file:///w%3A/Research/sequence-processing/sequential_labelling.py?line=202'>203</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///w%3A/Research/sequence-processing/sequential_labelling.py?line=203'>204</a>\u001b[0m         loss_batch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m--> <a href='file:///w%3A/Research/sequence-processing/sequential_labelling.py?line=204'>205</a>\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39;49mloss_strategy \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39maverage\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    <a href='file:///w%3A/Research/sequence-processing/sequential_labelling.py?line=205'>206</a>\u001b[0m     loss_batch \u001b[39m=\u001b[39m loss_batch \u001b[39m/\u001b[39m batch_size\n\u001b[0;32m    <a href='file:///w%3A/Research/sequence-processing/sequential_labelling.py?line=206'>207</a>\u001b[0m lr \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\.virtualenv\\sequence-processing-py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1177\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/module.py?line=1174'>1175</a>\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/module.py?line=1175'>1176</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/module.py?line=1176'>1177</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   <a href='file:///c%3A/.virtualenv/sequence-processing-py39/lib/site-packages/torch/nn/modules/module.py?line=1177'>1178</a>\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DNABERTSeq2Seq' object has no attribute 'loss_strategy'"
     ]
    }
   ],
   "source": [
    "from sequential_labelling import DNABERTSeq2Seq, train, init_adamw_optimizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "import os\n",
    "from utils.seq2seq import init_seq2seq_model\n",
    "import json\n",
    "from utils.data_generator import _data_generator_seq2seq\n",
    "\n",
    "dataloader = _data_generator_seq2seq()\n",
    "\n",
    "epoch_size = 10\n",
    "warmup = 10\n",
    "device = \"cuda\"\n",
    "seq2seq_config = json.load(open(os.path.join(\"models\", \"config\", \"config_seq2seq.json\"), \"r\"))\n",
    "model = init_seq2seq_model(pretrained_3kmer_dir, seq2seq_config)\n",
    "model.to(device)\n",
    "optimizer = init_adamw_optimizer(model.parameters())\n",
    "training_steps = len(dataloader) * epoch_size\n",
    "optim_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup, num_training_steps=training_steps)\n",
    "log_path = os.path.join(\"logs\", \"seq2seq\", \"19082022\", \"log.t-sample.csv\")\n",
    "save_path = os.path.join(\"result\", \"seq2seq\", \"19082022\", \"t-sample\")\n",
    "model.train()\n",
    "\"\"\"\n",
    "Play with result.\n",
    "\"\"\"\n",
    "trained_model = train(model, optimizer, optim_scheduler, dataloader, 10, 2, log_path, save_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "473c7453bcb969eece5b07ef8b7f234e7c84010927f6bebce35f0aeb1f8c121e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('sequence-processing-py39': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
