{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3006, 0.3322, 0.3672],\n",
      "         [0.3006, 0.3322, 0.3672],\n",
      "         [0.3006, 0.3322, 0.3672],\n",
      "         [0.3006, 0.3322, 0.3672],\n",
      "         [0.3006, 0.3322, 0.3672]],\n",
      "\n",
      "        [[0.3420, 0.3780, 0.2800],\n",
      "         [0.3006, 0.3322, 0.3672],\n",
      "         [0.3006, 0.3322, 0.3672],\n",
      "         [0.3006, 0.3322, 0.3672],\n",
      "         [0.3907, 0.2894, 0.3199]]])\n",
      "torch.Size([2, 5, 3])\n",
      "torch.Size([2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [2, 3], got [2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1792/1310331249.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\.virtualenv\\sequence-processing\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\.virtualenv\\sequence-processing\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\.virtualenv\\sequence-processing\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   2844\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2846\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected target size [2, 3], got [2]"
     ]
    }
   ],
   "source": [
    "pred = [\n",
    "    [   # Sentence 0\n",
    "        [0.1, 0.2, 0.3, ], # Token 0\n",
    "        [0.1, 0.2, 0.3, ], # Token 1\n",
    "        [0.1, 0.2, 0.3, ], # Token 2\n",
    "        [0.1, 0.2, 0.3, ], # Token 3\n",
    "        [0.1, 0.2, 0.3, ], # Token 4\n",
    "    ],\n",
    "    [   # Sentence 1\n",
    "        [0.3, 0.4, 0.1, ], # Token 0\n",
    "        [0.2, 0.3, 0.4, ], # Token 1\n",
    "        [0.2, 0.3, 0.4, ], # Token 2\n",
    "        [0.1, 0.2, 0.3, ], # Token 3\n",
    "        [0.4, 0.1, 0.2, ], # Token 4\n",
    "    ],\n",
    "]\n",
    "import torch\n",
    "pred = torch.tensor(pred)\n",
    "pred = torch.nn.Softmax(dim=2)(pred)\n",
    "print(pred)\n",
    "print(pred.shape)\n",
    "\n",
    "label = [\n",
    "    [], # Label Sentence 0\n",
    "    0, # Label Sentence 1\n",
    "]\n",
    "label = torch.tensor(label)\n",
    "print(label.shape)\n",
    "\n",
    "fn = torch.nn.CrossEntropyLoss()\n",
    "loss = fn(pred, label)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, BertModel\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "\n",
    "bertForTokenClassification = BertForTokenClassification.from_pretrained(pretrained_3kmer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/genome/grch38/exon/NC_000024.10.csv']\n",
      "['./data/chr/NC_000024.10.fasta']\n",
      "['./data/genome/labseq/chr24.csv']\n"
     ]
    }
   ],
   "source": [
    "from data_dir import chr24_index_csv, chr24_fasta, labseq_dir, labseq_names\n",
    "chr_indices = [chr24_index_csv]\n",
    "chr_fastas = [chr24_fasta]\n",
    "chr_labseq_path = [\"{}/{}\".format(labseq_dir, fname) for fname in [labseq_names[-1]]]\n",
    "print(chr_indices)\n",
    "print(chr_fastas)\n",
    "print(chr_labseq_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index ./data/genome/grch38/exon/NC_000024.10.csv, with fasta ./data/chr/NC_000024.10.fasta, to seq. labelling ./data/genome/labseq/chr24.csv, expanding [5431760/57226904]"
     ]
    }
   ],
   "source": [
    "from data_dir import chr24_index_csv, chr24_fasta, labseq_dir, labseq_names\n",
    "from data_preparation import generate_sequence_labelling\n",
    "chr_indices = [chr24_index_csv]\n",
    "chr_fastas = [chr24_fasta]\n",
    "chr_labseq_path = [\"{}/{}\".format(labseq_dir, fname) for fname in [labseq_names[-1]]]\n",
    "for src, fasta, target in zip(chr_indices, chr_fastas, chr_labseq_path):\n",
    "    print(\"Generating sequential labelling for index {}, from fasta {}, to {}: {}\".format(src, fasta, target, generate_sequence_labelling(src, fasta, target, do_expand=True, expand_size=512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1085, 0.0951, 0.1004,  ..., 0.0995, 0.0978, 0.0987],\n",
      "         [0.0949, 0.1173, 0.0994,  ..., 0.0985, 0.0937, 0.1008],\n",
      "         [0.0960, 0.1087, 0.1006,  ..., 0.0997, 0.1011, 0.0977],\n",
      "         ...,\n",
      "         [0.0946, 0.1058, 0.0991,  ..., 0.0982, 0.0970, 0.1002],\n",
      "         [0.1048, 0.1024, 0.0989,  ..., 0.0980, 0.1029, 0.0960],\n",
      "         [0.1107, 0.0953, 0.1005,  ..., 0.0997, 0.0956, 0.1036]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0971, 0.0910, 0.0996,  ..., 0.0995, 0.0960, 0.1047],\n",
      "         [0.0995, 0.1060, 0.0975,  ..., 0.0974, 0.0976, 0.0919],\n",
      "         [0.0982, 0.0913, 0.0992,  ..., 0.0990, 0.1081, 0.1080],\n",
      "         ...,\n",
      "         [0.1033, 0.1042, 0.0998,  ..., 0.0997, 0.0929, 0.0941],\n",
      "         [0.1038, 0.0909, 0.0992,  ..., 0.0991, 0.1053, 0.1085],\n",
      "         [0.0984, 0.1025, 0.1020,  ..., 0.1000, 0.1015, 0.1075]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0976, 0.0976, 0.1002,  ..., 0.1002, 0.0983, 0.0984],\n",
      "         [0.0924, 0.1063, 0.0992,  ..., 0.0992, 0.1032, 0.1061],\n",
      "         [0.0996, 0.0974, 0.0993,  ..., 0.0993, 0.1052, 0.0985],\n",
      "         ...,\n",
      "         [0.1031, 0.0988, 0.0977,  ..., 0.0977, 0.1049, 0.1043],\n",
      "         [0.1003, 0.0940, 0.1004,  ..., 0.1004, 0.1012, 0.0979],\n",
      "         [0.1038, 0.0950, 0.1014,  ..., 0.1014, 0.0995, 0.0988]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.1066, 0.1016, 0.0996,  ..., 0.1040, 0.1021, 0.0959],\n",
      "         [0.0938, 0.1057, 0.1011,  ..., 0.0996, 0.0938, 0.1035],\n",
      "         [0.1010, 0.0972, 0.1009,  ..., 0.0993, 0.1007, 0.1019],\n",
      "         ...,\n",
      "         [0.0927, 0.1062, 0.0999,  ..., 0.0984, 0.0918, 0.1062],\n",
      "         [0.1015, 0.0963, 0.1012,  ..., 0.0996, 0.0998, 0.1000],\n",
      "         [0.1080, 0.0983, 0.0985,  ..., 0.1051, 0.1083, 0.0962]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from utils.data_generator import _data_generator_seq2seq\n",
    "from models.seq2seq import DNABERTSeq2Seq\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "dataloader = _data_generator_seq2seq()\n",
    "model = DNABERTSeq2Seq(pretrained_3kmer_dir)\n",
    "\n",
    "for step, batch in enumerate(dataloader):\n",
    "    input_ids, attn_mask, token_type_ids, label = tuple(t for t in batch)\n",
    "    pred = model(input_ids, attn_mask, token_type_ids)\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0957, 0.1316, 0.1041,  ..., 0.0761, 0.1100, 0.1061],\n",
      "        [0.0928, 0.0975, 0.0976,  ..., 0.0714, 0.1303, 0.0872],\n",
      "        [0.0953, 0.1054, 0.1036,  ..., 0.0807, 0.1185, 0.0926],\n",
      "        ...,\n",
      "        [0.0862, 0.0936, 0.0937,  ..., 0.1239, 0.1213, 0.0837],\n",
      "        [0.0955, 0.1037, 0.1038,  ..., 0.0809, 0.1166, 0.0928],\n",
      "        [0.0995, 0.1367, 0.1081,  ..., 0.0790, 0.0755, 0.1102]],\n",
      "       device='cuda:0', grad_fn=<UnbindBackward0>) tensor([0, 3, 4, 1, 1, 1, 1, 1, 2, 3, 4, 2, 3, 4, 1, 1, 2, 3, 7, 3, 4, 1, 1, 1,\n",
      "        2, 5, 6, 4, 2, 3, 7, 3, 4, 1, 2, 5, 6, 4, 2, 5, 6, 4, 1, 2, 5, 8, 8, 8,\n",
      "        6, 7, 3, 4, 1, 2, 3, 4, 2, 5, 6, 4, 2, 5, 6, 7, 3, 4, 1, 1, 2, 3, 4, 2,\n",
      "        3, 4, 1, 2, 5, 6, 4, 2, 5, 8, 6, 4, 1, 1, 1, 1, 1, 2, 5, 8, 8, 8, 8, 8,\n",
      "        6, 7, 3, 4, 2, 3, 4, 1, 2, 5, 6, 7, 5, 8, 8, 8, 6, 4, 2, 3, 4, 1, 1, 1,\n",
      "        1, 1, 2, 5, 8, 8, 8, 6, 7, 3, 7, 5, 6, 4, 1, 1, 2, 3, 7, 5, 8, 6, 7, 3,\n",
      "        7, 3, 4, 1, 1, 1, 2, 3, 7, 3, 7, 5, 6, 7, 3, 4, 1, 2, 3, 7, 3, 7, 3, 4,\n",
      "        2, 5, 6, 7, 5, 6, 4, 1, 2, 3, 4, 2, 5, 8, 6, 7, 5, 8, 6, 4, 2, 5, 8, 6,\n",
      "        4, 1, 1, 1, 1, 1, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1, 1, 2, 5, 6,\n",
      "        7, 5, 8, 8, 6, 4, 1, 2, 5, 8, 8, 6, 7, 5, 6, 7, 3, 4, 2, 5, 6, 4, 2, 3,\n",
      "        4, 2, 5, 8, 8, 8, 6, 4, 1, 1, 1, 1, 2, 5, 6, 4, 1, 1, 2, 3, 4, 2, 3, 7,\n",
      "        3, 4, 2, 5, 8, 8, 8, 8, 6, 4, 1, 2, 3, 7, 3, 4, 1, 2, 3, 4, 1, 2, 5, 8,\n",
      "        6, 4, 1, 2, 5, 8, 8, 8, 6, 4, 2, 5, 6, 7, 3, 7, 3, 7, 3, 4, 2, 5, 6, 7,\n",
      "        5, 8, 8, 6, 7, 5, 6, 7, 3, 4, 1, 1, 2, 5, 6, 4, 1, 1, 1, 2, 3, 4, 2, 5,\n",
      "        8, 6, 4, 2, 3, 4, 2, 5, 6, 7, 5, 6, 7, 3, 4, 1, 1, 2, 3, 7, 3, 4, 2, 3,\n",
      "        7, 3, 7, 5, 6, 4, 1, 2, 3, 7, 5, 8, 8, 6, 7, 5, 8, 6, 4, 2, 3, 4, 2, 3,\n",
      "        4, 2, 3, 7, 5, 8, 8, 8, 8, 8, 8, 8, 8, 6, 4, 2, 5, 8, 8, 6, 4, 1, 2, 3,\n",
      "        7, 5, 6, 4, 2, 5, 6, 7, 3, 7, 3, 7, 5, 8, 8, 8, 6, 7, 3, 4, 1, 2, 5, 8,\n",
      "        8, 8, 8, 6, 7, 3, 4, 1, 2, 5, 8, 6, 4, 1, 1, 2, 5, 6, 4, 2, 3, 4, 2, 3,\n",
      "        7, 5, 6, 4, 2, 5, 6, 7, 3, 4, 1, 2, 5, 8, 8, 6, 7, 5, 8, 8, 6, 4, 1, 2,\n",
      "        5, 8, 8, 6, 4, 1, 1, 2, 5, 8, 8, 8, 8, 8, 8, 6, 7, 5, 6, 4, 1, 1, 1, 1,\n",
      "        1, 1, 2, 3, 7, 3, 4, 9], device='cuda:0')\n",
      "tensor(2.3015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[0.0917, 0.1077, 0.1194,  ..., 0.1009, 0.0609, 0.1135],\n",
      "        [0.0880, 0.0980, 0.0982,  ..., 0.0859, 0.0712, 0.1089],\n",
      "        [0.0758, 0.1548, 0.0847,  ..., 0.0740, 0.1653, 0.0939],\n",
      "        ...,\n",
      "        [0.1334, 0.0866, 0.0860,  ..., 0.1404, 0.0612, 0.0954],\n",
      "        [0.0762, 0.1539, 0.0851,  ..., 0.0744, 0.1633, 0.0944],\n",
      "        [0.0935, 0.1098, 0.1044,  ..., 0.1029, 0.0621, 0.1157]],\n",
      "       device='cuda:0', grad_fn=<UnbindBackward0>) tensor([0, 5, 8, 6, 4, 2, 3, 7, 5, 8, 8, 8, 6, 7, 3, 4, 1, 2, 3, 7, 5, 6, 7, 3,\n",
      "        4, 2, 3, 4, 2, 5, 8, 6, 7, 5, 8, 6, 7, 3, 4, 2, 3, 4, 1, 2, 5, 6, 4, 1,\n",
      "        1, 2, 3, 7, 5, 8, 8, 8, 8, 6, 7, 5, 8, 6, 4, 1, 1, 1, 1, 1, 2, 5, 8, 6,\n",
      "        4, 1, 1, 1, 1, 1, 2, 3, 7, 5, 8, 6, 7, 3, 4, 2, 3, 7, 5, 8, 6, 7, 5, 8,\n",
      "        6, 7, 5, 8, 6, 4, 2, 5, 8, 6, 7, 3, 4, 2, 3, 4, 1, 1, 1, 2, 3, 7, 3, 4,\n",
      "        2, 5, 8, 8, 8, 6, 7, 3, 4, 1, 1, 1, 1, 2, 5, 8, 8, 8, 6, 4, 2, 5, 8, 8,\n",
      "        8, 8, 8, 8, 8, 6, 4, 1, 2, 3, 4, 1, 1, 1, 2, 3, 4, 2, 5, 6, 7, 5, 8, 8,\n",
      "        6, 7, 5, 6, 7, 5, 8, 8, 6, 7, 3, 7, 3, 7, 3, 7, 3, 4, 2, 3, 4, 1, 2, 5,\n",
      "        6, 4, 2, 3, 7, 3, 4, 1, 2, 3, 4, 1, 1, 2, 3, 7, 3, 7, 3, 7, 3, 7, 3, 4,\n",
      "        2, 3, 7, 3, 7, 3, 4, 1, 1, 2, 5, 6, 7, 3, 4, 1, 1, 1, 2, 3, 4, 1, 2, 5,\n",
      "        8, 8, 8, 8, 8, 8, 8, 6, 7, 3, 7, 5, 6, 7, 5, 6, 7, 3, 4, 2, 5, 8, 6, 4,\n",
      "        1, 1, 1, 2, 3, 7, 5, 6, 7, 3, 7, 3, 7, 5, 8, 6, 7, 3, 4, 1, 1, 1, 1, 1,\n",
      "        2, 5, 6, 7, 5, 8, 6, 7, 5, 8, 8, 6, 4, 2, 3, 7, 5, 6, 4, 1, 2, 5, 6, 4,\n",
      "        2, 5, 6, 4, 2, 3, 4, 2, 3, 7, 5, 6, 4, 1, 1, 1, 1, 2, 3, 7, 5, 8, 6, 7,\n",
      "        5, 6, 7, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 7, 5, 8, 8, 6, 7, 5, 6, 4, 1,\n",
      "        1, 1, 1, 1, 2, 5, 6, 7, 3, 4, 2, 3, 7, 5, 6, 7, 5, 6, 7, 3, 7, 5, 8, 6,\n",
      "        4, 1, 1, 2, 5, 6, 7, 5, 6, 4, 2, 5, 6, 7, 3, 4, 1, 2, 5, 6, 4, 1, 1, 1,\n",
      "        1, 1, 2, 3, 4, 2, 3, 4, 2, 5, 6, 4, 1, 2, 5, 8, 8, 8, 6, 7, 3, 4, 1, 1,\n",
      "        2, 3, 7, 3, 4, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 5, 8, 8, 8, 6, 7, 5, 8, 8,\n",
      "        6, 7, 3, 4, 1, 2, 5, 8, 8, 6, 4, 2, 3, 4, 1, 1, 1, 2, 5, 6, 4, 2, 5, 6,\n",
      "        7, 5, 6, 7, 5, 8, 8, 6, 7, 5, 6, 4, 1, 1, 2, 3, 4, 2, 5, 6, 4, 2, 5, 6,\n",
      "        7, 3, 4, 1, 2, 5, 8, 9], device='cuda:0')\n",
      "tensor(2.3048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[0.0888, 0.0710, 0.1067,  ..., 0.1115, 0.1106, 0.0982],\n",
      "        [0.0840, 0.0672, 0.1009,  ..., 0.1372, 0.0895, 0.0929],\n",
      "        [0.0817, 0.0979, 0.0981,  ..., 0.0753, 0.1579, 0.0903],\n",
      "        ...,\n",
      "        [0.0756, 0.1311, 0.0908,  ..., 0.1232, 0.0786, 0.1134],\n",
      "        [0.0812, 0.0994, 0.0975,  ..., 0.0749, 0.1597, 0.0898],\n",
      "        [0.0915, 0.0732, 0.1099,  ..., 0.0844, 0.1139, 0.1012]],\n",
      "       device='cuda:0', grad_fn=<UnbindBackward0>) tensor([0, 3, 4, 2, 5, 6, 4, 1, 1, 2, 5, 6, 7, 3, 4, 2, 5, 6, 7, 3, 4, 2, 5, 6,\n",
      "        4, 2, 3, 4, 1, 1, 1, 2, 3, 7, 3, 4, 2, 3, 4, 2, 5, 6, 4, 2, 3, 4, 1, 2,\n",
      "        5, 6, 7, 3, 7, 5, 6, 4, 2, 5, 8, 8, 8, 6, 4, 1, 1, 2, 3, 7, 5, 6, 7, 3,\n",
      "        4, 2, 3, 4, 2, 3, 4, 2, 3, 7, 5, 8, 8, 8, 6, 4, 2, 3, 4, 2, 3, 7, 3, 7,\n",
      "        3, 4, 2, 3, 7, 3, 7, 5, 8, 6, 4, 1, 1, 1, 1, 2, 5, 6, 7, 5, 6, 7, 5, 6,\n",
      "        4, 1, 2, 3, 7, 5, 6, 4, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 5, 8, 6, 7, 3,\n",
      "        4, 1, 2, 5, 8, 8, 8, 8, 6, 7, 5, 8, 6, 7, 3, 4, 2, 3, 4, 2, 5, 8, 8, 8,\n",
      "        8, 8, 8, 6, 7, 5, 8, 6, 7, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 7, 3, 4,\n",
      "        1, 1, 1, 2, 5, 8, 8, 6, 7, 3, 4, 1, 2, 5, 6, 4, 1, 2, 3, 4, 2, 5, 8, 8,\n",
      "        6, 4, 2, 3, 4, 1, 1, 2, 5, 8, 6, 7, 3, 7, 5, 6, 7, 5, 6, 7, 5, 8, 6, 4,\n",
      "        1, 1, 2, 3, 7, 3, 7, 3, 4, 1, 1, 1, 1, 2, 5, 8, 6, 4, 1, 2, 5, 8, 6, 7,\n",
      "        3, 4, 1, 2, 5, 6, 4, 2, 3, 7, 5, 6, 4, 2, 5, 6, 7, 5, 8, 8, 8, 8, 6, 4,\n",
      "        1, 1, 2, 5, 8, 6, 4, 1, 1, 2, 5, 6, 7, 3, 4, 2, 3, 4, 2, 3, 4, 1, 1, 2,\n",
      "        3, 7, 3, 7, 3, 4, 1, 1, 2, 3, 7, 3, 7, 3, 7, 3, 4, 2, 5, 6, 4, 2, 5, 8,\n",
      "        6, 7, 5, 6, 4, 1, 1, 1, 2, 5, 6, 7, 3, 7, 3, 7, 3, 7, 5, 6, 7, 3, 7, 3,\n",
      "        4, 2, 5, 6, 7, 5, 8, 6, 7, 3, 4, 1, 1, 1, 2, 5, 6, 4, 2, 3, 4, 2, 5, 8,\n",
      "        8, 6, 4, 1, 2, 5, 8, 6, 4, 1, 1, 1, 2, 3, 7, 5, 8, 6, 7, 3, 7, 5, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 6, 7, 3, 4, 1, 2, 5, 8, 8, 6, 4, 1, 2, 3, 7, 5, 6, 4,\n",
      "        1, 2, 5, 8, 6, 4, 1, 1, 2, 5, 8, 8, 8, 6, 7, 3, 4, 2, 5, 8, 6, 4, 1, 2,\n",
      "        3, 4, 2, 5, 8, 6, 7, 3, 7, 5, 6, 4, 1, 1, 2, 5, 8, 6, 4, 2, 5, 6, 4, 1,\n",
      "        1, 1, 2, 3, 4, 2, 3, 7, 5, 8, 8, 6, 7, 5, 8, 8, 8, 8, 8, 6, 7, 5, 8, 6,\n",
      "        7, 5, 8, 8, 6, 4, 1, 9], device='cuda:0')\n",
      "tensor(2.3019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[0.1507, 0.0922, 0.0842,  ..., 0.0922, 0.0831, 0.1139],\n",
      "        [0.0758, 0.1011, 0.0923,  ..., 0.1011, 0.0921, 0.0929],\n",
      "        [0.0788, 0.1050, 0.1311,  ..., 0.1050, 0.0946, 0.0966],\n",
      "        ...,\n",
      "        [0.0781, 0.1040, 0.0950,  ..., 0.1040, 0.0944, 0.0956],\n",
      "        [0.0773, 0.1030, 0.1324,  ..., 0.1030, 0.0928, 0.1098],\n",
      "        [0.1582, 0.0968, 0.0884,  ..., 0.0968, 0.0872, 0.1195]],\n",
      "       device='cuda:0', grad_fn=<UnbindBackward0>) tensor([0, 7, 5, 6, 7, 3, 7, 3, 4, 2, 3, 7, 5, 6, 4, 2, 5, 8, 6, 4, 1, 1, 1, 1,\n",
      "        2, 5, 8, 6, 7, 5, 8, 8, 8, 6, 7, 3, 7, 3, 4, 2, 5, 8, 6, 4, 2, 5, 8, 8,\n",
      "        6, 4, 1, 1, 1, 1, 1, 2, 3, 4, 2, 5, 6, 7, 3, 7, 3, 7, 5, 8, 8, 6, 7, 3,\n",
      "        4, 1, 1, 1, 2, 3, 7, 3, 4, 2, 3, 4, 2, 3, 7, 3, 7, 5, 6, 7, 3, 7, 5, 8,\n",
      "        8, 6, 4, 2, 3, 4, 1, 2, 3, 4, 1, 1, 1, 2, 3, 4, 2, 3, 7, 5, 8, 6, 7, 3,\n",
      "        7, 5, 6, 7, 3, 7, 3, 4, 1, 1, 1, 1, 2, 3, 7, 3, 4, 1, 2, 3, 4, 1, 1, 1,\n",
      "        2, 3, 4, 2, 5, 8, 6, 7, 5, 8, 8, 8, 8, 6, 7, 3, 7, 5, 6, 4, 2, 5, 8, 6,\n",
      "        4, 2, 5, 8, 6, 4, 1, 2, 5, 8, 8, 6, 4, 1, 2, 3, 4, 1, 1, 1, 2, 5, 8, 8,\n",
      "        6, 4, 1, 1, 2, 5, 6, 7, 5, 8, 6, 4, 2, 3, 4, 2, 3, 4, 2, 3, 7, 3, 4, 1,\n",
      "        2, 5, 8, 8, 6, 7, 5, 6, 4, 2, 3, 4, 1, 1, 1, 1, 2, 3, 7, 5, 6, 4, 2, 5,\n",
      "        8, 8, 6, 4, 1, 1, 2, 5, 6, 4, 1, 2, 5, 6, 4, 2, 3, 4, 1, 1, 1, 1, 2, 3,\n",
      "        7, 3, 7, 5, 6, 4, 2, 3, 7, 3, 7, 5, 6, 4, 2, 3, 4, 2, 3, 7, 5, 6, 7, 5,\n",
      "        8, 8, 8, 8, 6, 7, 3, 4, 1, 2, 3, 7, 3, 7, 3, 7, 5, 8, 8, 6, 4, 2, 3, 7,\n",
      "        5, 6, 7, 5, 8, 8, 8, 8, 6, 4, 1, 2, 3, 7, 3, 7, 5, 8, 8, 6, 7, 5, 8, 8,\n",
      "        6, 4, 2, 3, 7, 3, 4, 1, 2, 3, 4, 1, 1, 1, 1, 2, 5, 8, 8, 6, 7, 5, 6, 4,\n",
      "        2, 5, 6, 7, 5, 8, 8, 8, 6, 7, 5, 8, 6, 7, 3, 4, 2, 5, 8, 6, 7, 5, 8, 6,\n",
      "        4, 1, 2, 3, 7, 5, 6, 4, 2, 3, 7, 3, 4, 2, 3, 7, 3, 7, 3, 4, 1, 2, 3, 4,\n",
      "        2, 5, 6, 4, 1, 2, 5, 8, 8, 8, 6, 4, 2, 3, 4, 1, 2, 5, 8, 6, 7, 3, 4, 2,\n",
      "        5, 6, 7, 3, 7, 5, 8, 6, 7, 5, 8, 6, 7, 5, 6, 4, 2, 3, 7, 3, 7, 3, 4, 2,\n",
      "        5, 8, 8, 8, 8, 6, 4, 2, 3, 7, 5, 6, 7, 3, 7, 3, 7, 3, 4, 2, 3, 4, 1, 2,\n",
      "        5, 8, 8, 6, 4, 1, 2, 3, 4, 1, 1, 2, 3, 7, 5, 6, 4, 1, 1, 2, 5, 6, 7, 3,\n",
      "        4, 1, 2, 5, 8, 6, 4, 9], device='cuda:0')\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:15<00:00,  3.83s/it]\n",
      "100%|██████████| 4/4 [00:15<00:00,  3.76s/it]\n",
      "100%|██████████| 4/4 [00:12<00:00,  3.24s/it]\n",
      "100%|██████████| 4/4 [00:13<00:00,  3.33s/it]\n",
      "100%|██████████| 4/4 [00:13<00:00,  3.39s/it]\n",
      "100%|██████████| 4/4 [00:13<00:00,  3.38s/it]\n",
      "100%|██████████| 4/4 [00:13<00:00,  3.40s/it]\n",
      "100%|██████████| 4/4 [00:12<00:00,  3.10s/it]\n",
      "100%|██████████| 4/4 [00:12<00:00,  3.12s/it]\n",
      "100%|██████████| 4/4 [00:12<00:00,  3.10s/it]\n"
     ]
    }
   ],
   "source": [
    "from sequential_labelling import DNABERTSeq2Seq, train, init_adamw_optimizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "import os\n",
    "epoch_size = 10\n",
    "warmup = 10\n",
    "model = DNABERTSeq2Seq(pretrained_3kmer_dir)\n",
    "optimizer = init_adamw_optimizer(model.parameters())\n",
    "training_steps = len(dataloader) * epoch_size\n",
    "optim_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup, num_training_steps=training_steps)\n",
    "device = \"cpu\"\n",
    "log_path = os.path.join(\"logs\", \"sample_log\", \"seq2seq\", \"log.2022-03-03.txt\")\n",
    "save_path = os.path.join(\"result\", \"samples\", \"seq2seq\", \"2022-03-03\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\"\"\"\n",
    "Play with result.\n",
    "\"\"\"\n",
    "trained_model = train(model, optimizer, optim_scheduler, dataloader, 10, 2, log_path, save_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pretrained\\3-new-12w-0 were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at pretrained\\3-new-12w-0 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForTokenClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(69, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "473c7453bcb969eece5b07ef8b7f234e7c84010927f6bebce35f0aeb1f8c121e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('sequence-processing-py39': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
