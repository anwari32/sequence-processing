{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, BertModel\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "\n",
    "bertForTokenClassification = BertForTokenClassification.from_pretrained(pretrained_3kmer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/genome/grch38/exon/NC_000024.10.csv']\n",
      "['./data/chr/NC_000024.10.fasta']\n",
      "['./data/genome/labseq/chr24.csv']\n"
     ]
    }
   ],
   "source": [
    "from data_dir import chr24_index_csv, chr24_fasta, labseq_dir, labseq_names\n",
    "chr_indices = [chr24_index_csv]\n",
    "chr_fastas = [chr24_fasta]\n",
    "chr_labseq_path = [\"{}/{}\".format(labseq_dir, fname) for fname in [labseq_names[-1]]]\n",
    "print(chr_indices)\n",
    "print(chr_fastas)\n",
    "print(chr_labseq_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index ./data/genome/grch38/exon/NC_000024.10.csv, with fasta ./data/chr/NC_000024.10.fasta, to seq. labelling ./data/genome/labseq/chr24.csv, expanding [5431760/57226904]"
     ]
    }
   ],
   "source": [
    "from data_dir import chr24_index_csv, chr24_fasta, labseq_dir, labseq_names\n",
    "from data_preparation import generate_sequence_labelling\n",
    "chr_indices = [chr24_index_csv]\n",
    "chr_fastas = [chr24_fasta]\n",
    "chr_labseq_path = [\"{}/{}\".format(labseq_dir, fname) for fname in [labseq_names[-1]]]\n",
    "for src, fasta, target in zip(chr_indices, chr_fastas, chr_labseq_path):\n",
    "    print(\"Generating sequential labelling for index {}, from fasta {}, to {}: {}\".format(src, fasta, target, generate_sequence_labelling(src, fasta, target, do_expand=True, expand_size=512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "from sequential_labelling import preprocessing, initialize_seq2seq\n",
    "\n",
    "\"\"\"\n",
    "Initialize model and tokenizer.\n",
    "\"\"\"\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_3kmer_dir)\n",
    "in_out_dimensions = [768, 512, 512, 512]\n",
    "model = initialize_seq2seq(pretrained_3kmer_dir, in_out_dimensions)\n",
    "#print(model)\n",
    "\n",
    "\"\"\"\n",
    "Create sample data sequential labelling.\n",
    "\"\"\"\n",
    "from random import randint\n",
    "from data_preparation import kmer\n",
    "from sequential_labelling import process_sequence_and_label, create_dataloader\n",
    "sequences = ['ATGC' * 128, 'TGAC' * 128, 'GATC' * 128]\n",
    "labels = [['E' if randint(0, 255) % 2 == 0 else '.' for i in range(len(s))] for s in sequences]\n",
    "\n",
    "kmer_seq = [' '.join(kmer(sequence, 3)) for sequence in sequences]\n",
    "kmer_label = [' '.join(kmer(''.join(label), 3)) for label in labels]\n",
    "\n",
    "arr_input_ids = []\n",
    "arr_attn_mask = []\n",
    "arr_label_repr = []\n",
    "for seq, label in zip(kmer_seq, kmer_label):\n",
    "    input_ids, attn_mask, label_repr = process_sequence_and_label(seq, label, tokenizer)\n",
    "    arr_input_ids.append(input_ids)\n",
    "    arr_attn_mask.append(attn_mask)\n",
    "    arr_label_repr.append(label_repr)\n",
    "\n",
    "dataloader = create_dataloader(arr_input_ids, arr_attn_mask, arr_label_repr, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 230.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 7, 5, 6, 7, 3, 7, 3, 4, 2, 5, 6, 4, 1, 2, 5, 8, 6, 4, 1, 1, 2, 5, 6,\n",
      "         7, 5, 6, 4, 1, 1, 1, 1, 1, 1, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 1, 2, 3, 4,\n",
      "         2, 5, 8, 8, 6, 7, 5, 6, 7, 5, 8, 8, 8, 6, 7, 5, 6, 4, 2, 3, 4, 1, 1, 1,\n",
      "         2, 3, 7, 5, 8, 6, 4, 1, 1, 1, 1, 2, 5, 8, 8, 6, 4, 2, 3, 7, 3, 4, 2, 3,\n",
      "         4, 1, 1, 2, 5, 8, 6, 7, 3, 4, 1, 2, 5, 8, 6, 4, 1, 1, 1, 2, 5, 8, 6, 7,\n",
      "         5, 6, 7, 5, 8, 8, 6, 4, 1, 1, 1, 2, 3, 4, 2, 5, 8, 6, 4, 1, 2, 3, 7, 5,\n",
      "         6, 7, 5, 6, 7, 5, 6, 4, 1, 2, 3, 4, 1, 1, 2, 3, 7, 5, 6, 4, 2, 5, 8, 8,\n",
      "         6, 4, 2, 5, 8, 6, 4, 2, 3, 7, 3, 7, 5, 8, 8, 6, 7, 5, 8, 8, 6, 7, 5, 6,\n",
      "         7, 5, 6, 4, 1, 1, 1, 2, 3, 7, 5, 8, 8, 8, 8, 6, 4, 1, 2, 5, 8, 6, 7, 5,\n",
      "         6, 4, 1, 2, 3, 4, 2, 5, 6, 4, 2, 3, 4, 1, 1, 1, 1, 2, 5, 6, 4, 2, 3, 4,\n",
      "         2, 3, 7, 5, 8, 8, 8, 8, 6, 7, 3, 7, 3, 7, 3, 7, 3, 7, 5, 6, 4, 2, 5, 6,\n",
      "         7, 3, 4, 2, 3, 7, 3, 4, 2, 3, 4, 1, 2, 3, 4, 2, 5, 6, 7, 3, 7, 5, 8, 8,\n",
      "         8, 8, 6, 4, 2, 3, 7, 3, 4, 1, 1, 1, 2, 3, 7, 5, 6, 4, 2, 5, 6, 4, 1, 1,\n",
      "         1, 2, 3, 4, 1, 2, 5, 8, 8, 8, 8, 6, 4, 1, 1, 1, 2, 5, 6, 7, 3, 4, 2, 3,\n",
      "         4, 1, 2, 5, 8, 6, 7, 5, 6, 4, 1, 2, 5, 8, 6, 7, 3, 4, 2, 5, 6, 4, 2, 5,\n",
      "         6, 4, 2, 3, 4, 2, 3, 7, 5, 8, 8, 6, 4, 1, 1, 1, 1, 2, 3, 4, 1, 2, 3, 4,\n",
      "         1, 2, 3, 4, 1, 1, 2, 3, 7, 5, 8, 6, 4, 1, 2, 3, 4, 1, 2, 5, 6, 7, 3, 4,\n",
      "         2, 5, 8, 6, 4, 2, 5, 8, 6, 4, 1, 2, 3, 7, 5, 6, 7, 3, 4, 2, 3, 4, 1, 1,\n",
      "         2, 5, 6, 7, 3, 4, 2, 3, 4, 2, 3, 4, 1, 1, 2, 3, 7, 5, 8, 8, 8, 6, 7, 5,\n",
      "         6, 4, 1, 1, 1, 2, 3, 4, 2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 2, 3, 7, 3, 4, 2,\n",
      "         3, 7, 5, 6, 7, 5, 8, 6, 7, 5, 6, 7, 3, 4, 1, 1, 1, 1, 1, 2, 5, 8, 8, 6,\n",
      "         4, 1, 1, 2, 3, 4, 2, 9]])\n",
      "tensor([[0, 4, 2, 5, 8, 6, 7, 3, 4, 1, 2, 3, 7, 3, 4, 2, 3, 7, 3, 4, 2, 5, 8, 8,\n",
      "         6, 7, 3, 7, 3, 7, 5, 6, 4, 1, 2, 3, 4, 1, 1, 1, 1, 1, 2, 5, 6, 4, 1, 2,\n",
      "         5, 6, 7, 3, 7, 5, 6, 4, 1, 2, 5, 6, 7, 5, 6, 7, 5, 8, 6, 7, 3, 4, 2, 3,\n",
      "         7, 5, 6, 7, 5, 8, 8, 6, 7, 3, 7, 3, 4, 2, 5, 8, 8, 6, 7, 3, 7, 3, 4, 1,\n",
      "         2, 3, 7, 3, 4, 1, 1, 2, 5, 6, 7, 5, 6, 7, 5, 6, 7, 5, 8, 6, 4, 1, 1, 1,\n",
      "         1, 1, 1, 2, 5, 6, 7, 5, 8, 6, 4, 2, 3, 7, 5, 8, 8, 8, 8, 6, 7, 3, 7, 5,\n",
      "         8, 6, 7, 3, 4, 1, 2, 5, 8, 6, 7, 3, 7, 5, 8, 6, 4, 1, 1, 2, 3, 7, 3, 4,\n",
      "         1, 2, 5, 8, 8, 6, 4, 2, 3, 4, 1, 2, 3, 4, 1, 2, 5, 8, 8, 8, 8, 6, 7, 5,\n",
      "         6, 7, 5, 6, 7, 5, 8, 6, 7, 5, 6, 4, 1, 1, 1, 2, 3, 7, 3, 7, 3, 4, 1, 2,\n",
      "         5, 6, 7, 5, 8, 8, 8, 8, 8, 8, 6, 4, 1, 1, 1, 1, 1, 2, 3, 4, 1, 2, 3, 4,\n",
      "         1, 1, 1, 1, 1, 2, 3, 7, 5, 8, 6, 7, 3, 4, 2, 5, 8, 6, 4, 2, 5, 8, 6, 7,\n",
      "         3, 7, 3, 4, 1, 2, 5, 6, 4, 1, 1, 1, 2, 3, 7, 3, 7, 3, 7, 5, 8, 8, 8, 8,\n",
      "         8, 8, 6, 7, 5, 6, 7, 3, 7, 3, 4, 2, 3, 7, 3, 7, 3, 7, 5, 8, 8, 6, 4, 1,\n",
      "         1, 2, 5, 6, 7, 5, 6, 7, 3, 4, 2, 5, 6, 7, 5, 6, 7, 5, 8, 6, 4, 2, 5, 8,\n",
      "         8, 8, 8, 8, 8, 6, 4, 2, 3, 7, 5, 8, 8, 8, 8, 8, 8, 6, 4, 2, 3, 4, 2, 3,\n",
      "         7, 3, 7, 3, 7, 5, 8, 6, 4, 1, 1, 2, 5, 6, 7, 5, 8, 8, 8, 6, 4, 1, 1, 2,\n",
      "         5, 8, 6, 7, 3, 4, 1, 2, 3, 7, 5, 8, 8, 8, 8, 6, 7, 3, 7, 3, 7, 5, 8, 6,\n",
      "         7, 3, 4, 1, 2, 5, 8, 8, 8, 8, 8, 6, 7, 5, 8, 8, 6, 4, 2, 3, 4, 1, 1, 2,\n",
      "         3, 7, 3, 7, 5, 6, 7, 5, 6, 4, 2, 3, 4, 1, 1, 2, 3, 7, 3, 7, 3, 4, 2, 5,\n",
      "         8, 6, 4, 1, 1, 1, 1, 1, 1, 1, 2, 3, 4, 2, 3, 7, 5, 8, 8, 6, 4, 2, 5, 8,\n",
      "         8, 6, 4, 2, 3, 7, 3, 7, 3, 7, 3, 4, 1, 2, 5, 6, 7, 5, 8, 6, 7, 5, 8, 6,\n",
      "         4, 2, 3, 4, 2, 3, 4, 9]])\n",
      "tensor([[0, 7, 5, 8, 8, 8, 8, 8, 6, 7, 3, 4, 1, 1, 2, 5, 8, 8, 8, 8, 6, 7, 3, 7,\n",
      "         3, 7, 5, 8, 6, 7, 3, 4, 1, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 4, 1, 2, 3, 4,\n",
      "         2, 5, 8, 8, 6, 7, 5, 6, 4, 2, 5, 8, 8, 8, 6, 7, 5, 6, 4, 2, 5, 6, 4, 2,\n",
      "         3, 7, 5, 6, 7, 5, 8, 6, 4, 2, 3, 4, 2, 5, 6, 7, 5, 8, 8, 8, 6, 4, 1, 2,\n",
      "         3, 7, 5, 8, 8, 6, 4, 1, 2, 3, 4, 1, 2, 5, 8, 8, 8, 8, 6, 7, 3, 4, 1, 1,\n",
      "         1, 1, 2, 5, 6, 4, 2, 3, 4, 2, 5, 8, 6, 4, 1, 2, 5, 8, 6, 4, 2, 3, 7, 3,\n",
      "         4, 2, 5, 6, 4, 1, 1, 1, 1, 1, 2, 5, 8, 6, 7, 3, 7, 5, 6, 4, 2, 5, 6, 7,\n",
      "         5, 8, 6, 4, 2, 5, 6, 4, 2, 5, 8, 8, 6, 4, 2, 5, 8, 6, 4, 1, 1, 1, 1, 1,\n",
      "         2, 3, 7, 3, 7, 3, 4, 1, 2, 3, 7, 5, 8, 6, 7, 5, 6, 4, 2, 3, 7, 3, 4, 2,\n",
      "         3, 7, 5, 6, 4, 2, 3, 4, 2, 5, 8, 6, 7, 3, 7, 3, 7, 5, 8, 8, 8, 8, 6, 7,\n",
      "         5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 4, 2, 5, 6, 4, 1, 1, 1, 1, 2, 5, 6, 7,\n",
      "         3, 4, 1, 2, 3, 7, 5, 6, 4, 2, 3, 4, 1, 2, 5, 8, 8, 6, 4, 1, 2, 3, 7, 5,\n",
      "         6, 7, 5, 8, 6, 4, 1, 2, 3, 4, 2, 5, 6, 7, 5, 8, 6, 7, 5, 6, 7, 3, 4, 1,\n",
      "         1, 2, 5, 6, 4, 2, 5, 6, 4, 1, 2, 5, 6, 7, 3, 7, 3, 4, 2, 3, 7, 3, 7, 3,\n",
      "         4, 1, 1, 2, 3, 4, 2, 5, 6, 4, 2, 3, 7, 3, 7, 3, 4, 1, 2, 5, 6, 7, 5, 8,\n",
      "         6, 7, 3, 7, 5, 6, 7, 5, 8, 6, 4, 2, 3, 4, 2, 5, 6, 7, 3, 4, 2, 3, 7, 5,\n",
      "         8, 6, 7, 3, 4, 1, 2, 5, 6, 4, 2, 3, 4, 2, 3, 7, 5, 8, 8, 8, 6, 4, 2, 3,\n",
      "         7, 3, 7, 3, 7, 3, 4, 1, 1, 1, 1, 1, 1, 1, 2, 3, 4, 2, 5, 6, 4, 2, 3, 4,\n",
      "         2, 3, 7, 5, 8, 6, 7, 5, 6, 4, 1, 2, 3, 7, 5, 8, 8, 8, 6, 4, 1, 1, 2, 5,\n",
      "         8, 6, 7, 5, 6, 7, 5, 8, 6, 7, 5, 8, 6, 4, 2, 3, 7, 5, 6, 4, 1, 2, 3, 4,\n",
      "         1, 2, 3, 7, 3, 7, 3, 4, 1, 2, 3, 7, 3, 7, 5, 6, 7, 5, 6, 7, 5, 8, 8, 8,\n",
      "         6, 4, 2, 5, 6, 7, 5, 9]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Play with result.\n",
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "    input_ids, attn_mask, label_repr = tuple(t for t in batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "tensor([3., 7., 3., 7.], dtype=torch.float64)\n",
      "tensor(14461.8037, grad_fn=<DivBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:03<00:06,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0110, 0.0611, 0.0000], grad_fn=<SliceBackward0>)\n",
      "tensor([7., 3., 7., 5.], dtype=torch.float64)\n",
      "tensor(13898.5840, grad_fn=<DivBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:05<00:02,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0648, 0.0000, 0.0000], grad_fn=<SliceBackward0>)\n",
      "tensor([5., 8., 6., 7.], dtype=torch.float64)\n",
      "tensor(15300.6055, grad_fn=<DivBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:09<00:00,  3.00s/it]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "    input_ids, attention_mask, label = tuple(t for t in batch)\n",
    "    output = model(input_ids, attention_mask)\n",
    "    print(output[0][1:5])\n",
    "    print(label[0][1:5])\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    loss = loss_func(output, label.float())\n",
    "    print(loss)\n",
    "    loss.backward()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "473c7453bcb969eece5b07ef8b7f234e7c84010927f6bebce35f0aeb1f8c121e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('sequence-processing-py39': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
