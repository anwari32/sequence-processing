{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, BertModel\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "\n",
    "bertForTokenClassification = BertForTokenClassification.from_pretrained(pretrained_3kmer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/genome/grch38/exon/NC_000024.10.csv']\n",
      "['./data/chr/NC_000024.10.fasta']\n",
      "['./data/genome/labseq/chr24.csv']\n"
     ]
    }
   ],
   "source": [
    "from data_dir import chr24_index_csv, chr24_fasta, labseq_dir, labseq_names\n",
    "chr_indices = [chr24_index_csv]\n",
    "chr_fastas = [chr24_fasta]\n",
    "chr_labseq_path = [\"{}/{}\".format(labseq_dir, fname) for fname in [labseq_names[-1]]]\n",
    "print(chr_indices)\n",
    "print(chr_fastas)\n",
    "print(chr_labseq_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index ./data/genome/grch38/exon/NC_000024.10.csv, with fasta ./data/chr/NC_000024.10.fasta, to seq. labelling ./data/genome/labseq/chr24.csv, expanding [5431760/57226904]"
     ]
    }
   ],
   "source": [
    "from data_dir import chr24_index_csv, chr24_fasta, labseq_dir, labseq_names\n",
    "from data_preparation import generate_sequence_labelling\n",
    "chr_indices = [chr24_index_csv]\n",
    "chr_fastas = [chr24_fasta]\n",
    "chr_labseq_path = [\"{}/{}\".format(labseq_dir, fname) for fname in [labseq_names[-1]]]\n",
    "for src, fasta, target in zip(chr_indices, chr_fastas, chr_labseq_path):\n",
    "    print(\"Generating sequential labelling for index {}, from fasta {}, to {}: {}\".format(src, fasta, target, generate_sequence_labelling(src, fasta, target, do_expand=True, expand_size=512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "from sequential_labelling import preprocessing\n",
    "\n",
    "\"\"\"\n",
    "Initialize tokenizer.\n",
    "\"\"\"\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_3kmer_dir)\n",
    "\n",
    "\"\"\"\n",
    "Create sample data sequential labelling.\n",
    "\"\"\"\n",
    "from random import randint\n",
    "from data_preparation import kmer\n",
    "from sequential_labelling import process_sequence_and_label, create_dataloader\n",
    "sequences = ['ATGC' * 128, 'TGAC' * 128, 'GATC' * 128, \"AGCC\" * 128]\n",
    "labels = [['E' if randint(0, 255) % 2 == 0 else '.' for i in range(len(s))] for s in sequences]\n",
    "\n",
    "kmer_seq = [' '.join(kmer(sequence, 3)) for sequence in sequences]\n",
    "kmer_label = [' '.join(kmer(''.join(label), 3)) for label in labels]\n",
    "\n",
    "arr_input_ids = []\n",
    "arr_attn_mask = []  \n",
    "arr_label_repr = []\n",
    "arr_token_type_ids = []\n",
    "for seq, label in zip(kmer_seq, kmer_label):\n",
    "    input_ids, attn_mask, token_type_ids, label_repr = process_sequence_and_label(seq, label, tokenizer)\n",
    "    arr_input_ids.append(input_ids)\n",
    "    arr_attn_mask.append(attn_mask)\n",
    "    arr_token_type_ids.append(token_type_ids)\n",
    "    arr_label_repr.append(label_repr)\n",
    "\n",
    "dataloader = create_dataloader(arr_input_ids, arr_attn_mask, arr_token_type_ids, arr_label_repr, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0957, 0.1316, 0.1041,  ..., 0.0761, 0.1100, 0.1061],\n",
      "        [0.0928, 0.0975, 0.0976,  ..., 0.0714, 0.1303, 0.0872],\n",
      "        [0.0953, 0.1054, 0.1036,  ..., 0.0807, 0.1185, 0.0926],\n",
      "        ...,\n",
      "        [0.0862, 0.0936, 0.0937,  ..., 0.1239, 0.1213, 0.0837],\n",
      "        [0.0955, 0.1037, 0.1038,  ..., 0.0809, 0.1166, 0.0928],\n",
      "        [0.0995, 0.1367, 0.1081,  ..., 0.0790, 0.0755, 0.1102]],\n",
      "       device='cuda:0', grad_fn=<UnbindBackward0>) tensor([0, 3, 4, 1, 1, 1, 1, 1, 2, 3, 4, 2, 3, 4, 1, 1, 2, 3, 7, 3, 4, 1, 1, 1,\n",
      "        2, 5, 6, 4, 2, 3, 7, 3, 4, 1, 2, 5, 6, 4, 2, 5, 6, 4, 1, 2, 5, 8, 8, 8,\n",
      "        6, 7, 3, 4, 1, 2, 3, 4, 2, 5, 6, 4, 2, 5, 6, 7, 3, 4, 1, 1, 2, 3, 4, 2,\n",
      "        3, 4, 1, 2, 5, 6, 4, 2, 5, 8, 6, 4, 1, 1, 1, 1, 1, 2, 5, 8, 8, 8, 8, 8,\n",
      "        6, 7, 3, 4, 2, 3, 4, 1, 2, 5, 6, 7, 5, 8, 8, 8, 6, 4, 2, 3, 4, 1, 1, 1,\n",
      "        1, 1, 2, 5, 8, 8, 8, 6, 7, 3, 7, 5, 6, 4, 1, 1, 2, 3, 7, 5, 8, 6, 7, 3,\n",
      "        7, 3, 4, 1, 1, 1, 2, 3, 7, 3, 7, 5, 6, 7, 3, 4, 1, 2, 3, 7, 3, 7, 3, 4,\n",
      "        2, 5, 6, 7, 5, 6, 4, 1, 2, 3, 4, 2, 5, 8, 6, 7, 5, 8, 6, 4, 2, 5, 8, 6,\n",
      "        4, 1, 1, 1, 1, 1, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1, 1, 2, 5, 6,\n",
      "        7, 5, 8, 8, 6, 4, 1, 2, 5, 8, 8, 6, 7, 5, 6, 7, 3, 4, 2, 5, 6, 4, 2, 3,\n",
      "        4, 2, 5, 8, 8, 8, 6, 4, 1, 1, 1, 1, 2, 5, 6, 4, 1, 1, 2, 3, 4, 2, 3, 7,\n",
      "        3, 4, 2, 5, 8, 8, 8, 8, 6, 4, 1, 2, 3, 7, 3, 4, 1, 2, 3, 4, 1, 2, 5, 8,\n",
      "        6, 4, 1, 2, 5, 8, 8, 8, 6, 4, 2, 5, 6, 7, 3, 7, 3, 7, 3, 4, 2, 5, 6, 7,\n",
      "        5, 8, 8, 6, 7, 5, 6, 7, 3, 4, 1, 1, 2, 5, 6, 4, 1, 1, 1, 2, 3, 4, 2, 5,\n",
      "        8, 6, 4, 2, 3, 4, 2, 5, 6, 7, 5, 6, 7, 3, 4, 1, 1, 2, 3, 7, 3, 4, 2, 3,\n",
      "        7, 3, 7, 5, 6, 4, 1, 2, 3, 7, 5, 8, 8, 6, 7, 5, 8, 6, 4, 2, 3, 4, 2, 3,\n",
      "        4, 2, 3, 7, 5, 8, 8, 8, 8, 8, 8, 8, 8, 6, 4, 2, 5, 8, 8, 6, 4, 1, 2, 3,\n",
      "        7, 5, 6, 4, 2, 5, 6, 7, 3, 7, 3, 7, 5, 8, 8, 8, 6, 7, 3, 4, 1, 2, 5, 8,\n",
      "        8, 8, 8, 6, 7, 3, 4, 1, 2, 5, 8, 6, 4, 1, 1, 2, 5, 6, 4, 2, 3, 4, 2, 3,\n",
      "        7, 5, 6, 4, 2, 5, 6, 7, 3, 4, 1, 2, 5, 8, 8, 6, 7, 5, 8, 8, 6, 4, 1, 2,\n",
      "        5, 8, 8, 6, 4, 1, 1, 2, 5, 8, 8, 8, 8, 8, 8, 6, 7, 5, 6, 4, 1, 1, 1, 1,\n",
      "        1, 1, 2, 3, 7, 3, 4, 9], device='cuda:0')\n",
      "tensor(2.3015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[0.0917, 0.1077, 0.1194,  ..., 0.1009, 0.0609, 0.1135],\n",
      "        [0.0880, 0.0980, 0.0982,  ..., 0.0859, 0.0712, 0.1089],\n",
      "        [0.0758, 0.1548, 0.0847,  ..., 0.0740, 0.1653, 0.0939],\n",
      "        ...,\n",
      "        [0.1334, 0.0866, 0.0860,  ..., 0.1404, 0.0612, 0.0954],\n",
      "        [0.0762, 0.1539, 0.0851,  ..., 0.0744, 0.1633, 0.0944],\n",
      "        [0.0935, 0.1098, 0.1044,  ..., 0.1029, 0.0621, 0.1157]],\n",
      "       device='cuda:0', grad_fn=<UnbindBackward0>) tensor([0, 5, 8, 6, 4, 2, 3, 7, 5, 8, 8, 8, 6, 7, 3, 4, 1, 2, 3, 7, 5, 6, 7, 3,\n",
      "        4, 2, 3, 4, 2, 5, 8, 6, 7, 5, 8, 6, 7, 3, 4, 2, 3, 4, 1, 2, 5, 6, 4, 1,\n",
      "        1, 2, 3, 7, 5, 8, 8, 8, 8, 6, 7, 5, 8, 6, 4, 1, 1, 1, 1, 1, 2, 5, 8, 6,\n",
      "        4, 1, 1, 1, 1, 1, 2, 3, 7, 5, 8, 6, 7, 3, 4, 2, 3, 7, 5, 8, 6, 7, 5, 8,\n",
      "        6, 7, 5, 8, 6, 4, 2, 5, 8, 6, 7, 3, 4, 2, 3, 4, 1, 1, 1, 2, 3, 7, 3, 4,\n",
      "        2, 5, 8, 8, 8, 6, 7, 3, 4, 1, 1, 1, 1, 2, 5, 8, 8, 8, 6, 4, 2, 5, 8, 8,\n",
      "        8, 8, 8, 8, 8, 6, 4, 1, 2, 3, 4, 1, 1, 1, 2, 3, 4, 2, 5, 6, 7, 5, 8, 8,\n",
      "        6, 7, 5, 6, 7, 5, 8, 8, 6, 7, 3, 7, 3, 7, 3, 7, 3, 4, 2, 3, 4, 1, 2, 5,\n",
      "        6, 4, 2, 3, 7, 3, 4, 1, 2, 3, 4, 1, 1, 2, 3, 7, 3, 7, 3, 7, 3, 7, 3, 4,\n",
      "        2, 3, 7, 3, 7, 3, 4, 1, 1, 2, 5, 6, 7, 3, 4, 1, 1, 1, 2, 3, 4, 1, 2, 5,\n",
      "        8, 8, 8, 8, 8, 8, 8, 6, 7, 3, 7, 5, 6, 7, 5, 6, 7, 3, 4, 2, 5, 8, 6, 4,\n",
      "        1, 1, 1, 2, 3, 7, 5, 6, 7, 3, 7, 3, 7, 5, 8, 6, 7, 3, 4, 1, 1, 1, 1, 1,\n",
      "        2, 5, 6, 7, 5, 8, 6, 7, 5, 8, 8, 6, 4, 2, 3, 7, 5, 6, 4, 1, 2, 5, 6, 4,\n",
      "        2, 5, 6, 4, 2, 3, 4, 2, 3, 7, 5, 6, 4, 1, 1, 1, 1, 2, 3, 7, 5, 8, 6, 7,\n",
      "        5, 6, 7, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 7, 5, 8, 8, 6, 7, 5, 6, 4, 1,\n",
      "        1, 1, 1, 1, 2, 5, 6, 7, 3, 4, 2, 3, 7, 5, 6, 7, 5, 6, 7, 3, 7, 5, 8, 6,\n",
      "        4, 1, 1, 2, 5, 6, 7, 5, 6, 4, 2, 5, 6, 7, 3, 4, 1, 2, 5, 6, 4, 1, 1, 1,\n",
      "        1, 1, 2, 3, 4, 2, 3, 4, 2, 5, 6, 4, 1, 2, 5, 8, 8, 8, 6, 7, 3, 4, 1, 1,\n",
      "        2, 3, 7, 3, 4, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 5, 8, 8, 8, 6, 7, 5, 8, 8,\n",
      "        6, 7, 3, 4, 1, 2, 5, 8, 8, 6, 4, 2, 3, 4, 1, 1, 1, 2, 5, 6, 4, 2, 5, 6,\n",
      "        7, 5, 6, 7, 5, 8, 8, 6, 7, 5, 6, 4, 1, 1, 2, 3, 4, 2, 5, 6, 4, 2, 5, 6,\n",
      "        7, 3, 4, 1, 2, 5, 8, 9], device='cuda:0')\n",
      "tensor(2.3048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[0.0888, 0.0710, 0.1067,  ..., 0.1115, 0.1106, 0.0982],\n",
      "        [0.0840, 0.0672, 0.1009,  ..., 0.1372, 0.0895, 0.0929],\n",
      "        [0.0817, 0.0979, 0.0981,  ..., 0.0753, 0.1579, 0.0903],\n",
      "        ...,\n",
      "        [0.0756, 0.1311, 0.0908,  ..., 0.1232, 0.0786, 0.1134],\n",
      "        [0.0812, 0.0994, 0.0975,  ..., 0.0749, 0.1597, 0.0898],\n",
      "        [0.0915, 0.0732, 0.1099,  ..., 0.0844, 0.1139, 0.1012]],\n",
      "       device='cuda:0', grad_fn=<UnbindBackward0>) tensor([0, 3, 4, 2, 5, 6, 4, 1, 1, 2, 5, 6, 7, 3, 4, 2, 5, 6, 7, 3, 4, 2, 5, 6,\n",
      "        4, 2, 3, 4, 1, 1, 1, 2, 3, 7, 3, 4, 2, 3, 4, 2, 5, 6, 4, 2, 3, 4, 1, 2,\n",
      "        5, 6, 7, 3, 7, 5, 6, 4, 2, 5, 8, 8, 8, 6, 4, 1, 1, 2, 3, 7, 5, 6, 7, 3,\n",
      "        4, 2, 3, 4, 2, 3, 4, 2, 3, 7, 5, 8, 8, 8, 6, 4, 2, 3, 4, 2, 3, 7, 3, 7,\n",
      "        3, 4, 2, 3, 7, 3, 7, 5, 8, 6, 4, 1, 1, 1, 1, 2, 5, 6, 7, 5, 6, 7, 5, 6,\n",
      "        4, 1, 2, 3, 7, 5, 6, 4, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 5, 8, 6, 7, 3,\n",
      "        4, 1, 2, 5, 8, 8, 8, 8, 6, 7, 5, 8, 6, 7, 3, 4, 2, 3, 4, 2, 5, 8, 8, 8,\n",
      "        8, 8, 8, 6, 7, 5, 8, 6, 7, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 7, 3, 4,\n",
      "        1, 1, 1, 2, 5, 8, 8, 6, 7, 3, 4, 1, 2, 5, 6, 4, 1, 2, 3, 4, 2, 5, 8, 8,\n",
      "        6, 4, 2, 3, 4, 1, 1, 2, 5, 8, 6, 7, 3, 7, 5, 6, 7, 5, 6, 7, 5, 8, 6, 4,\n",
      "        1, 1, 2, 3, 7, 3, 7, 3, 4, 1, 1, 1, 1, 2, 5, 8, 6, 4, 1, 2, 5, 8, 6, 7,\n",
      "        3, 4, 1, 2, 5, 6, 4, 2, 3, 7, 5, 6, 4, 2, 5, 6, 7, 5, 8, 8, 8, 8, 6, 4,\n",
      "        1, 1, 2, 5, 8, 6, 4, 1, 1, 2, 5, 6, 7, 3, 4, 2, 3, 4, 2, 3, 4, 1, 1, 2,\n",
      "        3, 7, 3, 7, 3, 4, 1, 1, 2, 3, 7, 3, 7, 3, 7, 3, 4, 2, 5, 6, 4, 2, 5, 8,\n",
      "        6, 7, 5, 6, 4, 1, 1, 1, 2, 5, 6, 7, 3, 7, 3, 7, 3, 7, 5, 6, 7, 3, 7, 3,\n",
      "        4, 2, 5, 6, 7, 5, 8, 6, 7, 3, 4, 1, 1, 1, 2, 5, 6, 4, 2, 3, 4, 2, 5, 8,\n",
      "        8, 6, 4, 1, 2, 5, 8, 6, 4, 1, 1, 1, 2, 3, 7, 5, 8, 6, 7, 3, 7, 5, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 6, 7, 3, 4, 1, 2, 5, 8, 8, 6, 4, 1, 2, 3, 7, 5, 6, 4,\n",
      "        1, 2, 5, 8, 6, 4, 1, 1, 2, 5, 8, 8, 8, 6, 7, 3, 4, 2, 5, 8, 6, 4, 1, 2,\n",
      "        3, 4, 2, 5, 8, 6, 7, 3, 7, 5, 6, 4, 1, 1, 2, 5, 8, 6, 4, 2, 5, 6, 4, 1,\n",
      "        1, 1, 2, 3, 4, 2, 3, 7, 5, 8, 8, 6, 7, 5, 8, 8, 8, 8, 8, 6, 7, 5, 8, 6,\n",
      "        7, 5, 8, 8, 6, 4, 1, 9], device='cuda:0')\n",
      "tensor(2.3019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[0.1507, 0.0922, 0.0842,  ..., 0.0922, 0.0831, 0.1139],\n",
      "        [0.0758, 0.1011, 0.0923,  ..., 0.1011, 0.0921, 0.0929],\n",
      "        [0.0788, 0.1050, 0.1311,  ..., 0.1050, 0.0946, 0.0966],\n",
      "        ...,\n",
      "        [0.0781, 0.1040, 0.0950,  ..., 0.1040, 0.0944, 0.0956],\n",
      "        [0.0773, 0.1030, 0.1324,  ..., 0.1030, 0.0928, 0.1098],\n",
      "        [0.1582, 0.0968, 0.0884,  ..., 0.0968, 0.0872, 0.1195]],\n",
      "       device='cuda:0', grad_fn=<UnbindBackward0>) tensor([0, 7, 5, 6, 7, 3, 7, 3, 4, 2, 3, 7, 5, 6, 4, 2, 5, 8, 6, 4, 1, 1, 1, 1,\n",
      "        2, 5, 8, 6, 7, 5, 8, 8, 8, 6, 7, 3, 7, 3, 4, 2, 5, 8, 6, 4, 2, 5, 8, 8,\n",
      "        6, 4, 1, 1, 1, 1, 1, 2, 3, 4, 2, 5, 6, 7, 3, 7, 3, 7, 5, 8, 8, 6, 7, 3,\n",
      "        4, 1, 1, 1, 2, 3, 7, 3, 4, 2, 3, 4, 2, 3, 7, 3, 7, 5, 6, 7, 3, 7, 5, 8,\n",
      "        8, 6, 4, 2, 3, 4, 1, 2, 3, 4, 1, 1, 1, 2, 3, 4, 2, 3, 7, 5, 8, 6, 7, 3,\n",
      "        7, 5, 6, 7, 3, 7, 3, 4, 1, 1, 1, 1, 2, 3, 7, 3, 4, 1, 2, 3, 4, 1, 1, 1,\n",
      "        2, 3, 4, 2, 5, 8, 6, 7, 5, 8, 8, 8, 8, 6, 7, 3, 7, 5, 6, 4, 2, 5, 8, 6,\n",
      "        4, 2, 5, 8, 6, 4, 1, 2, 5, 8, 8, 6, 4, 1, 2, 3, 4, 1, 1, 1, 2, 5, 8, 8,\n",
      "        6, 4, 1, 1, 2, 5, 6, 7, 5, 8, 6, 4, 2, 3, 4, 2, 3, 4, 2, 3, 7, 3, 4, 1,\n",
      "        2, 5, 8, 8, 6, 7, 5, 6, 4, 2, 3, 4, 1, 1, 1, 1, 2, 3, 7, 5, 6, 4, 2, 5,\n",
      "        8, 8, 6, 4, 1, 1, 2, 5, 6, 4, 1, 2, 5, 6, 4, 2, 3, 4, 1, 1, 1, 1, 2, 3,\n",
      "        7, 3, 7, 5, 6, 4, 2, 3, 7, 3, 7, 5, 6, 4, 2, 3, 4, 2, 3, 7, 5, 6, 7, 5,\n",
      "        8, 8, 8, 8, 6, 7, 3, 4, 1, 2, 3, 7, 3, 7, 3, 7, 5, 8, 8, 6, 4, 2, 3, 7,\n",
      "        5, 6, 7, 5, 8, 8, 8, 8, 6, 4, 1, 2, 3, 7, 3, 7, 5, 8, 8, 6, 7, 5, 8, 8,\n",
      "        6, 4, 2, 3, 7, 3, 4, 1, 2, 3, 4, 1, 1, 1, 1, 2, 5, 8, 8, 6, 7, 5, 6, 4,\n",
      "        2, 5, 6, 7, 5, 8, 8, 8, 6, 7, 5, 8, 6, 7, 3, 4, 2, 5, 8, 6, 7, 5, 8, 6,\n",
      "        4, 1, 2, 3, 7, 5, 6, 4, 2, 3, 7, 3, 4, 2, 3, 7, 3, 7, 3, 4, 1, 2, 3, 4,\n",
      "        2, 5, 6, 4, 1, 2, 5, 8, 8, 8, 6, 4, 2, 3, 4, 1, 2, 5, 8, 6, 7, 3, 4, 2,\n",
      "        5, 6, 7, 3, 7, 5, 8, 6, 7, 5, 8, 6, 7, 5, 6, 4, 2, 3, 7, 3, 7, 3, 4, 2,\n",
      "        5, 8, 8, 8, 8, 6, 4, 2, 3, 7, 5, 6, 7, 3, 7, 3, 7, 3, 4, 2, 3, 4, 1, 2,\n",
      "        5, 8, 8, 6, 4, 1, 2, 3, 4, 1, 1, 2, 3, 7, 5, 6, 4, 1, 1, 2, 5, 6, 7, 3,\n",
      "        4, 1, 2, 5, 8, 6, 4, 9], device='cuda:0')\n",
      "tensor(2.3027, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from data_dir import pretrained_3kmer_dir\n",
    "from transformers import BertForMaskedLM\n",
    "from models.seq2seq import Seq2SeqHead\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Softmax, CrossEntropyLoss, NLLLoss\n",
    "from torch.cuda import empty_cache\n",
    "model = BertForMaskedLM.from_pretrained(pretrained_3kmer_dir)\n",
    "bert = model.bert\n",
    "bert = bert.to('cuda')\n",
    "seq2seq = Seq2SeqHead([768, 10])\n",
    "seq2seq.to('cuda')\n",
    "softmax = Softmax(dim=2)\n",
    "loss_func = CrossEntropyLoss()\n",
    "for step, batch in enumerate(dataloader):\n",
    "    input_ids, attn_mask, token_type_ids, label = tuple(t.to('cuda') for t in batch)\n",
    "    output_bert = bert(input_ids, attention_mask=attn_mask, token_type_ids=token_type_ids)\n",
    "    output_seq2seq = seq2seq(output_bert[0])\n",
    "    output = softmax(output_seq2seq)\n",
    "\n",
    "    #print(output, output.shape)\n",
    "    #print(output.view(-1, 10), output.view(-1, 10).shape)\n",
    "    #print(label, label.shape)\n",
    "    #print(label.reshape([label.shape[0] * label.shape[1]]), label.shape)\n",
    "    loss = 0\n",
    "    for pred, target in zip(output, label):\n",
    "        print(pred, target)\n",
    "        loss += loss_func(pred, target)\n",
    "    print(loss)\n",
    "\n",
    "    empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:15<00:00,  3.83s/it]\n",
      "100%|██████████| 4/4 [00:15<00:00,  3.76s/it]\n",
      "100%|██████████| 4/4 [00:12<00:00,  3.24s/it]\n",
      "100%|██████████| 4/4 [00:13<00:00,  3.33s/it]\n",
      "100%|██████████| 4/4 [00:13<00:00,  3.39s/it]\n",
      "100%|██████████| 4/4 [00:13<00:00,  3.38s/it]\n",
      "100%|██████████| 4/4 [00:13<00:00,  3.40s/it]\n",
      "100%|██████████| 4/4 [00:12<00:00,  3.10s/it]\n",
      "100%|██████████| 4/4 [00:12<00:00,  3.12s/it]\n",
      "100%|██████████| 4/4 [00:12<00:00,  3.10s/it]\n"
     ]
    }
   ],
   "source": [
    "from sequential_labelling import DNABERTSeq2Seq, train, init_adamw_optimizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "import os\n",
    "epoch_size = 10\n",
    "warmup = 10\n",
    "model = DNABERTSeq2Seq(pretrained_3kmer_dir)\n",
    "optimizer = init_adamw_optimizer(model.parameters())\n",
    "training_steps = len(dataloader) * epoch_size\n",
    "optim_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup, num_training_steps=training_steps)\n",
    "device = \"cpu\"\n",
    "log_path = os.path.join(\"logs\", \"sample_log\", \"seq2seq\", \"log.2022-03-03.txt\")\n",
    "save_path = os.path.join(\"result\", \"samples\", \"seq2seq\", \"2022-03-03\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\"\"\"\n",
    "Play with result.\n",
    "\"\"\"\n",
    "trained_model = train(model, optimizer, optim_scheduler, dataloader, 10, 2, log_path, save_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pretrained\\3-new-12w-0 were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at pretrained\\3-new-12w-0 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForTokenClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(69, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "473c7453bcb969eece5b07ef8b7f234e7c84010927f6bebce35f0aeb1f8c121e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('sequence-processing-py39': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
