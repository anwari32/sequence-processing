{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from visualisation import visualize\n",
    "\n",
    "log_file = os.path.join(\"logs\", \"mtl\", \"2022-03-22\", \"log.t-sample.csv\")\n",
    "visualize(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualise_mtl_average_loss(log_path):\n",
    "    log_df = pd.read_csv(log_path)\n",
    "    epochs = log_df[\"epoch\"].unique()\n",
    "    average_prom = []\n",
    "    average_ss = []\n",
    "    average_polya = []\n",
    "    average_lr = []\n",
    "    for e in epochs:\n",
    "        df = log_df[log_df[\"epoch\"] == e]\n",
    "        average_prom.append(df[\"loss_prom\"].mean())\n",
    "        average_ss.append(df[\"loss_ss\"].mean())\n",
    "        average_polya.append(df[\"loss_polya\"].mean())\n",
    "        average_lr.append(df[\"lr\"].mean())\n",
    "    \n",
    "    figs, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "    \n",
    "    axes[0][0].plot(epochs, average_prom)\n",
    "    axes[0][0].set_title(\"average prom loss\")\n",
    "    axes[0][0].set_xticks(epochs)\n",
    "    axes[0][1].plot(epochs, average_ss)\n",
    "    axes[0][1].set_title(\"average ss loss\")\n",
    "    axes[0][1].set_xticks(epochs)\n",
    "    axes[1][0].plot(epochs, average_polya)\n",
    "    axes[1][0].set_title(\"average polya loss\")\n",
    "    axes[1][0].set_xticks(epochs)\n",
    "    axes[1][1].plot(epochs, average_lr)\n",
    "    axes[1][1].set_title(\"average learning rate\")\n",
    "    axes[1][1].set_xticks(epochs)\n",
    "\n",
    "    plt.subplots_adjust(top=2, bottom=1, left=5, right=6, hspace=0.2, wspace=0.25)\n",
    "    plt.legend()\n",
    "    \n",
    "visualise_mtl_average_loss(os.path.join(\"logs\", \"mtl\", \"2022-03-22\", \"log.t-sample.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_file = os.path.join(\"logs\", \"mtl\", \"tambora\", \"2022-03-18\", \"log1.t1300.e10.b1.g64.csv\")\n",
    "visualise_mtl_average_loss(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "log_file1 = os.path.join(\"logs\", \"mtl\", \"tambora\", \"2022-03-18\", \"log1.t1300.e10.b1.g64.1.csv\")\n",
    "log_file2 = os.path.join(\"logs\", \"mtl\", \"tambora\", \"2022-03-18\", \"log1.t1300.e10.b1.g64.2.csv\")\n",
    "\n",
    "df1 = pd.read_csv(log_file1)\n",
    "df2 = pd.read_csv(log_file2)\n",
    "df3 = pd.concat([df1, df2])\n",
    "df3.to_csv(os.path.join(\"logs\", \"mtl\", \"tambora\", \"2022-03-18\", \"log1.t1300.e10.b1.g64.csv\"), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = os.path.join(\"logs\", \"mtl\", \"tambora\", \"2022-03-18\", \"log1.t1300.e10.b1.g64.csv\")\n",
    "visualise_mtl_average_loss(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "def visualize_avg_loss_per_epoch(log_path):\n",
    "    if not os.path.exists(log_path):\n",
    "        raise FileNotFoundError(\"File {} not found.\".format(log_path))\n",
    "\n",
    "    df = pd.read_csv(log_path)\n",
    "    epochs = list(df['epoch'].unique())\n",
    "    avg_loss_prom_by_epoch = []\n",
    "    avg_loss_ss_by_epoch = []\n",
    "    avg_loss_polya_by_epoch = []\n",
    "    for e in epochs:\n",
    "        edf = df[df['epoch'] == e]\n",
    "        avg_loss_prom_by_epoch.append(edf['loss_prom'].mean())\n",
    "        avg_loss_ss_by_epoch.append(edf['loss_ss'].mean())\n",
    "        avg_loss_polya_by_epoch.append(edf['loss_polya'].mean())\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(epochs, avg_loss_prom_by_epoch, label=\"avg loss prom\")\n",
    "    ax.plot(epochs, avg_loss_ss_by_epoch, label=\"avg loss ss\")\n",
    "    ax.plot(epochs, avg_loss_polya_by_epoch, label=\"avg loss polya\")   \n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "log_dir = os.path.join('logs', '2022-03-05')\n",
    "log_file = os.path.join(log_dir, 'log_2022-03-05-07-20-19_train.sample.csv_e5_b1_sum.csv')\n",
    "visualize_avg_loss_per_epoch(log_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create confusion matrix for MTL.\n",
    "\"\"\"\n",
    "from multitask_learning import init_model_mtl\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "from utils.mtl import create_dataloader_from_csv\n",
    "from transformers import BertTokenizer\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "model = init_model_mtl(pretrained_3kmer_dir)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_3kmer_dir)\n",
    "eval_log = os.path.join(\"logs\", \"mtl\", \"eval\", \"2022-03-17\", \"log.csv\")\n",
    "if os.path.exists(eval_log):\n",
    "    os.remove(eval_log)\n",
    "os.makedirs(os.path.dirname(eval_log), exist_ok=True)\n",
    "log = open(eval_log, 'x')\n",
    "log.write(\"pred_prom,label_prom,pred_ss,label_ss,pred_polya,label_polya\\n\")\n",
    "\n",
    "val_csv = os.path.join(\"workspace\", \"mtl\", \"validation.sample.csv\")\n",
    "dataloader = create_dataloader_from_csv(val_csv, tokenizer, batch_size=1)\n",
    "for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "    input_ids, attn_mask, token_type_ids, label_prom, label_ss, label_polya = tuple(t for t in batch)\n",
    "    preds = model(input_ids, attn_mask)\n",
    "    pred_prom = preds[\"prom\"]\n",
    "    pred_ss = preds[\"ss\"]\n",
    "    pred_polya = preds[\"polya\"]\n",
    "\n",
    "    pred_prom_val, pred_prom_index = torch.max(pred_prom, dim=1)\n",
    "    actual_prom = label_prom.float().item()\n",
    "    predicted_prom = torch.round(pred_prom_val).item()\n",
    "\n",
    "    pred_ss_val, pred_ss_index = torch.max(pred_ss, dim=1)\n",
    "    pred_ss_index = pred_ss_index.item()\n",
    "    label_ss = label_ss.item()\n",
    "\n",
    "    pred_polya_val, pred_polya_index = torch.max(pred_polya, dim=1)\n",
    "    pred_polya_index = pred_polya_index.item()\n",
    "    label_polya = label_polya.item()\n",
    "    \n",
    "    log.write(f\"{predicted_prom},{actual_prom},{pred_ss_index},{label_ss},{pred_polya_index},{label_polya}\\n\")\n",
    "\n",
    "    #print(pred_prom, pred_prom_index, label_prom)\n",
    "    #print(pred_ss, pred_ss_index, label_ss)\n",
    "    #print(pred_polya, pred_polya_index, label_polya)\n",
    "\n",
    "log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multitask_learning import evaluate, init_model_mtl\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "import torch\n",
    "import os\n",
    "from utils.mtl import create_dataloader_from_csv\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "summary = {}\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_3kmer_dir)\n",
    "dataloader = create_dataloader_from_csv(os.path.join(\"workspace\", \"mtl\", \"validation.sample.csv\"), tokenizer)\n",
    "for model_epoch in range(10):\n",
    "    model = init_model_mtl(pretrained_3kmer_dir)\n",
    "    model.load_state_dict(torch.load(os.path.join(\"result\", \"mtl\", \"2022-03-17\", f\"epoch-{model_epoch}.pth\")), strict=False)\n",
    "    model.eval()\n",
    "    pred_acc, ss_acc, polya_acc = evaluate(model, dataloader, os.path.join(\"logs\", \"mtl\", \"2022-03-17\", f\"validation.epoch.{model_epoch}.csv\"))\n",
    "    t = (pred_acc, ss_acc, polya_acc)\n",
    "    summary[f\"epoch-{model_epoch}\"] = t\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (1, 2, 3)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create confusion matrix.\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "eval_log = os.path.join(\"logs\", \"mtl\", \"eval\", \"2022-03-17\", \"log.csv\")\n",
    "df = pd.read_csv(eval_log)\n",
    "pred_prom = list(df['pred_prom'])\n",
    "label_prom = list(df['label_prom'])\n",
    "cf_matrix_prom = confusion_matrix(pred_prom, label_prom)\n",
    "print(cf_matrix_prom)\n",
    "pred_ss = list(df['pred_ss'])\n",
    "label_ss = list(df['label_ss'])\n",
    "cf_matrix_ss = confusion_matrix(pred_ss, label_ss)\n",
    "print(cf_matrix_ss)\n",
    "pred_polya = list(df['pred_polya'])\n",
    "label_polya = list(df['label_polya'])\n",
    "cf_matrix_polya = confusion_matrix(pred_polya, label_polya)\n",
    "print(cf_matrix_polya)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = sns.heatmap(cf_matrix_prom, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Confusion Matrix Promoter Prediction\\n\\n')\n",
    "ax.set_xlabel('Predicted Promoter \\n')\n",
    "ax.set_ylabel('Actual Promoter ')\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = sns.heatmap(cf_matrix_ss, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Confusion Matrix Splice Sites (SS) Prediction\\n\\n')\n",
    "ax.set_xlabel('Predicted SS \\n')\n",
    "ax.set_ylabel('Actual SS ')\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = sns.heatmap(cf_matrix_polya, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Confusion Matrix PolyA Prediction\\n\\n')\n",
    "ax.set_xlabel('Predicted PolyA \\n')\n",
    "ax.set_ylabel('Actual PolyA ')\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def filter_df_by_epoch(csv_file, epochs):\n",
    "    results = []\n",
    "    df = pd.read_csv(csv_file)\n",
    "    for e in epochs:\n",
    "        results.append(df[df[\"epoch\"] == e])\n",
    "    return results\n",
    "\n",
    "def create_cf_matrix(df):\n",
    "    predictions = np.array([], int)\n",
    "    targets = np.array([], int)\n",
    "    for i, r in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing : \"):\n",
    "        target = r[\"target\"].split(' ')\n",
    "        target = [int(a) for a in target]\n",
    "        target = np.array(target, int)\n",
    "        targets = np.concatenate((targets, target))\n",
    "        prediction = r[\"prediction\"].split(' ')\n",
    "        prediction = [int(a) for a in prediction]\n",
    "        prediction = np.array(prediction, int)\n",
    "        predictions = np.concatenate((predictions, prediction))\n",
    "\n",
    "    return predictions, targets\n",
    "\n",
    "def create_confusion_matrix(csv_file, epoch=0):\n",
    "    # raise NotImplementedError(\"Function is not implemented.\")\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df[df[\"epoch\"] == epoch]\n",
    "    predictions = np.array([], int)\n",
    "    targets = np.array([], int)\n",
    "    for i, r in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing : \"):\n",
    "        target = r[\"target\"].split(' ')\n",
    "        target = [int(a) for a in target]\n",
    "        target = np.array(target, int)\n",
    "        targets = np.concatenate((targets, target))\n",
    "        prediction = r[\"prediction\"].split(' ')\n",
    "        prediction = [int(a) for a in prediction]\n",
    "        prediction = np.array(prediction, int)\n",
    "        predictions = np.concatenate((predictions, prediction))\n",
    "\n",
    "    return predictions, targets\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlab_10_prediction, seqlab_10_target = create_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "preds, targets = create_confusion_matrix(os.path.join(\"run\", \"seqlab-tiny-base-b64-e50-20220704-215933\", \"eval_log.csv\"))\n",
    "print(preds.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(constrained_layout=True, figsize=(8,4))\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    confusion_matrix(preds, targets)\n",
    "    , annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Confusion Matrix Token Prediction\\n\\n')\n",
    "ax.set_xlabel('Predicted Label \\n')\n",
    "ax.set_ylabel('Target Label ')\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.confusion_matrix import create_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "preds, targets = create_confusion_matrix(os.path.join(\"run\", \"seqlab-ss-base-b32-e20-20220725-180122\", \"eval_log.csv\"))\n",
    "fig, ax = plt.subplots(constrained_layout=True, figsize=(8,4))\n",
    "ax = sns.heatmap(confusion_matrix(targets, preds), annot=True, cmap='Blues')\n",
    "ax.set_title('seqlab-ss-base-b32-e20-20220725-180122\\n\\n')\n",
    "ax.set_xlabel('Predicted Label \\n')\n",
    "ax.set_ylabel('Target Label ')\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.confusion_matrix import create_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "preds, targets = create_confusion_matrix(os.path.join(\"run\", \"seqlab-ss-freeze.base-b32-e20-20220725-180122\", \"eval_log.csv\"))\n",
    "fig, ax = plt.subplots(constrained_layout=True, figsize=(8,4))\n",
    "ax = sns.heatmap(confusion_matrix(targets, preds), annot=True, cmap='Blues')\n",
    "ax.set_title('seqlab-ss-freeze.base-b32-e20-20220725-180122\\n\\n')\n",
    "ax.set_xlabel('Predicted Label \\n')\n",
    "ax.set_ylabel('Target Label ')\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.confusion_matrix import create_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "preds, targets = create_confusion_matrix(os.path.join(\"run\", \"seqlab-tiny-freeze.base-b32-e20-20220726-094427\", \"eval_log.csv\"))\n",
    "fig, ax = plt.subplots(constrained_layout=True, figsize=(8,4))\n",
    "ax = sns.heatmap(confusion_matrix(targets, preds), annot=True, cmap='Blues')\n",
    "ax.set_title('seqlab-tiny-freeze.base-b32-e20-20220726-094427\\n\\n')\n",
    "ax.set_xlabel('Predicted Label \\n')\n",
    "ax.set_ylabel('Target Label ')\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.confusion_matrix import create_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "preds, targets = create_confusion_matrix(os.path.join(\"run\", \"seqlab-tiny-base-b32-e20-20220726-094427\", \"eval_log.csv\"))\n",
    "fig, ax = plt.subplots(constrained_layout=True, figsize=(8,4))\n",
    "ax = sns.heatmap(confusion_matrix(targets, preds), annot=True, cmap='Blues')\n",
    "ax.set_title('seqlab-tiny-base-b32-e20-20220726-094427\\n\\n')\n",
    "ax.set_xlabel('Predicted Label \\n')\n",
    "ax.set_ylabel('Target Label ')\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.confusion_matrix import create_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "preds, targets = create_confusion_matrix(os.path.join(\"run\", \"genlab-10-tambora-freeze.lstm-1udr7gb9\", \"validation_log.csv\"))\n",
    "fig, ax = plt.subplots(constrained_layout=True, figsize=(8,4))\n",
    "ax = sns.heatmap(confusion_matrix(targets, preds), annot=True, cmap='Blues')\n",
    "ax.set_title('genlab-10-tambora-freeze.lstm-1udr7gb9\\n\\n')\n",
    "ax.set_xlabel('Predicted Label \\n')\n",
    "ax.set_ylabel('Target Label ')\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.confusion_matrix import create_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "preds, targets = create_confusion_matrix(os.path.join(\"run\", \"genlab-10-tambora-lstm-2mdnh784\", \"validation_log.csv\"))\n",
    "fig, ax = plt.subplots(constrained_layout=True, figsize=(8,4))\n",
    "ax = sns.heatmap(confusion_matrix(preds, targets), annot=True, cmap='Blues')\n",
    "ax.set_title('genlab-10-tambora-lstm-2mdnh784\\n\\n')\n",
    "ax.set_xlabel('Predicted Label \\n')\n",
    "ax.set_ylabel('Target Label ')\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "path = os.path.join(\"run\", \"genlab-10-tambora-lstm-2mdnh784\", \"validation_log.csv\")\n",
    "def count_label(csv):\n",
    "    pred, target = {}, {}\n",
    "    for i in range(8):\n",
    "        pred[str(i)] = 0\n",
    "        target[str(i)] = 0\n",
    "    \n",
    "    pred[\"-100\"] = 0\n",
    "    target[\"-100\"] = 0\n",
    "    \n",
    "    df = pd.read_csv(csv)\n",
    "    for i, r in df.iterrows():\n",
    "        target_label = r[\"target\"].split(' ')\n",
    "        prediction = r[\"prediction\"].split(' ')\n",
    "        for j in range(512):\n",
    "            pred[prediction[j]] += 1\n",
    "            target[target_label[j]] += 1\n",
    "    return pred, target\n",
    "\n",
    "pred, target = count_label(path)\n",
    "pred, target\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.seqlab import Index_Dictionary\n",
    "pred_tokens = {}\n",
    "target_tokens = {}\n",
    "for k in pred.keys():\n",
    "    pred_tokens[Index_Dictionary[int(k)]] = pred[k]\n",
    "    target_tokens[Index_Dictionary[int(k)]] = target[k]\n",
    "\n",
    "pred_tokens, target_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_tokens, target_tokens)\n",
    "s = 0\n",
    "for k in target_tokens.keys():\n",
    "    s += target_tokens[k]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = filter_df_by_epoch(os.path.join(\"run\", \"seqlab-tiny-base-b64-e50-20220704-215933\", \"eval_log.csv\"),\n",
    "    [0, 9, 19, 29, 39, 49]\n",
    "    )\n",
    "\n",
    "preds0, targets0 = create_cf_matrix(dfs[0]) # After one epoch.\n",
    "preds9, targets9 = create_cf_matrix(dfs[1]) # After 10 epochs.\n",
    "preds19, targets19 = create_cf_matrix(dfs[2]) # After 20 epochs.\n",
    "preds29, targets29 = create_cf_matrix(dfs[3]) # After 30 epochs.\n",
    "preds39, targets39 = create_cf_matrix(dfs[4]) # After 40 epochs.\n",
    "preds49, targets49 = create_cf_matrix(dfs[5]) # After 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figs, axes = plt.subplots(nrows=6, ncols=1, constrained_layout=True)\n",
    "\n",
    "axes[0] = sns.heatmap(\n",
    "    confusion_matrix(preds0, targets0)\n",
    "    , annot=True, cmap='Blues')\n",
    "\n",
    "axes[0].set_title('Confusion Matrix Token Prediction After First Epoch\\n\\n')\n",
    "axes[0].set_xlabel('Predicted Label \\n')\n",
    "axes[0].set_ylabel('Target Label ')\n",
    "\n",
    "axes[1] = sns.heatmap(\n",
    "    confusion_matrix(preds9, targets9)\n",
    "    , annot=True, cmap='Blues')\n",
    "\n",
    "axes[1].set_title('Confusion Matrix Token Prediction After Epoch 10\\n\\n')\n",
    "axes[1].set_xlabel('Predicted Label \\n')\n",
    "axes[1].set_ylabel('Target Label ')\n",
    "\n",
    "axes[2] = sns.heatmap(\n",
    "    confusion_matrix(preds19, targets19)\n",
    "    , annot=True, cmap='Blues')\n",
    "\n",
    "axes[2].set_title('Confusion Matrix Token Prediction After Epoch 20\\n\\n')\n",
    "axes[2].set_xlabel('Predicted Label \\n')\n",
    "axes[2].set_ylabel('Target Label ')\n",
    "\n",
    "axes[3] = sns.heatmap(\n",
    "    confusion_matrix(preds29, targets29)\n",
    "    , annot=True, cmap='Blues')\n",
    "\n",
    "axes[3].set_title('Confusion Matrix Token Prediction After Epoch 30\\n\\n')\n",
    "axes[3].set_xlabel('Predicted Label \\n')\n",
    "axes[3].set_ylabel('Target Label ')\n",
    "\n",
    "axes[4] = sns.heatmap(\n",
    "    confusion_matrix(preds39, targets39)\n",
    "    , annot=True, cmap='Blues')\n",
    "\n",
    "axes[4].set_title('Confusion Matrix Token Prediction After Epoch 40\\n\\n')\n",
    "axes[4].set_xlabel('Predicted Label \\n')\n",
    "axes[4].set_ylabel('Target Label ')\n",
    "\n",
    "\n",
    "axes[5] = sns.heatmap(\n",
    "    confusion_matrix(preds49, targets49)\n",
    "    , annot=True, cmap='Blues')\n",
    "\n",
    "axes[5].set_title('Confusion Matrix Token Prediction After Epoch 50\\n\\n')\n",
    "axes[5].set_xlabel('Predicted Label \\n')\n",
    "axes[5].set_ylabel('Target Label ')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = [\n",
    "    [[1], [2], [3]],\n",
    "    [[4], [5], [6]]\n",
    "]\n",
    "x = np.asarray(a)\n",
    "print(x.shape)\n",
    "x = np.squeeze(x, axis=2)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "z = np.squeeze(x[0])\n",
    "z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def visualize_log_seq2seq(log_path):\n",
    "    log_df = pd.read_csv(log_path)\n",
    "    epoch = list(log_df['epoch'].unique())\n",
    "    epochs = [i for i in range(len(epoch))]\n",
    "    subplot_id = 320\n",
    "    fig = plt.figure()\n",
    "    fig.set_figheight(10)\n",
    "    fig.set_figwidth(10)\n",
    "\n",
    "    for e in epochs:\n",
    "        e_df = log_df[log_df['epoch'] == e]\n",
    "        len_e = len(e_df)\n",
    "        loss = list(e_df['loss'])\n",
    "        steps = [(k+1) for k in range(0, len_e)]\n",
    "        subplot_id += 1\n",
    "        ax = fig.add_subplot(subplot_id)\n",
    "        ax.plot(steps, loss, label='loss ')\n",
    "        ax.set_title('epoch {}'.format(e+1))\n",
    "        ax.set_xlabel('steps')\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.legend()\n",
    "        \n",
    "    plt.subplots_adjust(top=1.5, bottom=1, left=0.10, right=0.95, hspace=0.5, wspace=0.35)\n",
    "    plt.show()\n",
    "\n",
    "def visualize_average_epoch_loss(log_path):\n",
    "    log_df = pd.read_csv(log_path)\n",
    "    epochs = log_df[\"epoch\"].unique()\n",
    "    averages = []\n",
    "    for e in epochs:\n",
    "        loss_by_epoch_df = log_df[log_df[\"epoch\"] == e]\n",
    "        average = loss_by_epoch_df[\"loss\"].mean()\n",
    "        averages.append(average)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    subplot_id = 110\n",
    "    ax = fig.add_subplot(subplot_id + 1)\n",
    "    ax.plot(epochs, averages)\n",
    "    ax.set_title(\"average loss per epoch\")\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(\"loss\")\n",
    "    ax.legend()\n",
    "    plt.xticks(np.arange(0, len(epochs), step=1))\n",
    "    plt.subplots_adjust(top=1.5, bottom=1, left=0.10, right=0.95, hspace=0.5, wspace=0.35)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "seq2seq_log = os.path.join(\"logs\", \"seq2seq\", \"bundle.sample\", \"log_e5_b1_g1.csv\")\n",
    "visualize_log_seq2seq(seq2seq_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "seq2seq_log = os.path.join(\"logs\", \"seq2seq\", \"bundle.sample\", \"log_e5_b1_g1.csv\")\n",
    "visualize_average_epoch_loss(seq2seq_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert Visualization\n",
    "\n",
    "from bertviz import head_view, model_view\n",
    "import os\n",
    "from models.seqlab import DNABERT_SL\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "from utils.seqlab import preprocessing_kmer, _process_label, _process_sequence\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "bert_path = os.path.join(\"pretrained\", \"3-new-12w-0\")\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
    "bert = BertForMaskedLM.from_pretrained(bert_path).bert\n",
    "model = DNABERT_SL(bert, None)\n",
    "test_data = os.path.join(\"workspace\", \"seqlab\", \"seqlab.strand-positive.kmer.stride-510.from-index\", \"sample.csv\")\n",
    "test_dataloader = preprocessing_kmer(test_data, tokenizer, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df = pd.read_csv(test_data)\n",
    "sequence = test_data_df.iloc[0][\"sequence\"]\n",
    "label = test_data_df.iloc[0][\"label\"]\n",
    "input_ids, attention_mask, token_type_ids = _process_sequence(sequence, tokenizer)\n",
    "arr_input_ids = []\n",
    "arr_input_ids.append(input_ids)\n",
    "arr_input_ids = torch.tensor(arr_input_ids)\n",
    "arr_attention_mask = []\n",
    "arr_attention_mask.append(attention_mask)\n",
    "arr_attention_mask = torch.tensor(arr_attention_mask)\n",
    "label_ids = _process_label(label)\n",
    "pred, bert_output = model(arr_input_ids, arr_attention_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = bert_output[-1]\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "print(tokens)\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_view(attention, tokens, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "model_version = os.path.join('pretrained', 'bert-base-uncased')\n",
    "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version)\n",
    "sentence_a = \"The cat sat on the mat\"\n",
    "sentence_b = \"The cat lay on the rug\"\n",
    "inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "token_type_ids = inputs['token_type_ids']\n",
    "attention = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
    "sentence_b_start = token_type_ids[0].tolist().index(1)\n",
    "input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_id_list) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_b_start\n",
    "attention\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_view(attention, tokens, sentence_b_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.seqlab\n",
    "\n",
    "utils.seqlab.NUM_LABELS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14ae8cb2141f3f34f4e0523006ff2d6cb0f7956c0f094e5497e312072e4d0d3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
