

Epoch [1/1]:   1%|▉                                                                                                                                                    | 1/150 [00:03<07:51,  3.17s/it]
Traceback (most recent call last):
  File "W:\Research\sequence-processing\multitask_learning.py", line 177, in train
    loss_prom, loss_ss, loss_polya = __train__(model, in_ids, attn_mask, label_prom, label_ss, label_polya, loss_fn_prom=loss_fn["prom"], loss_fn_ss=loss_fn["ss"], loss_fn_polya=loss_fn["polya"])
  File "W:\Research\sequence-processing\multitask_learning.py", line 28, in __train__
    output = model(input_ids, attention_mask)
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "W:\Research\sequence-processing\models\mtl.py", line 106, in forward
    x = self.shared_layer(input_ids=input_ids, attention_mask=attention_masks)
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\transformers\models\bert\modeling_bert.py", line 996, in forward
    encoder_outputs = self.encoder(
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\transformers\models\bert\modeling_bert.py", line 585, in forward
    layer_outputs = layer_module(
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\transformers\models\bert\modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\transformers\models\bert\modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\transformers\models\bert\modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 6.00 GiB total capacity; 4.58 GiB already allocated; 640.00 KiB free; 4.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 6.00 GiB total capacity; 4.58 GiB already allocated; 640.00 KiB free; 4.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Epoch [1/1]:   3%|███▉                                                                                                                                                 | 4/150 [00:05<03:08,  1.29s/it]