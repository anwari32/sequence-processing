

Epoch [1/1]:   1%|██▍                                                                                                                                                                                      | 2/150 [00:04<04:36,  1.87s/it]
Traceback (most recent call last):
  File "W:\Research\sequence-processing\multitask_learning.py", line 176, in train
    loss_prom, loss_ss, loss_polya = __train__(model, in_ids, attn_mask, label_prom, label_ss, label_polya, loss_fn_prom=loss_fn["prom"], loss_fn_ss=loss_fn["ss"], loss_fn_polya=loss_fn["polya"])
  File "W:\Research\sequence-processing\multitask_learning.py", line 28, in __train__
    output = model(input_ids, attention_mask)
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "W:\Research\sequence-processing\models\mtl.py", line 106, in forward
    x = self.shared_layer(input_ids=input_ids, attention_mask=attention_masks)
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\transformers\models\bert\modeling_bert.py", line 996, in forward
    encoder_outputs = self.encoder(
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\transformers\models\bert\modeling_bert.py", line 585, in forward
    layer_outputs = layer_module(
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\transformers\models\bert\modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\transformers\models\bert\modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\transformers\models\bert\modeling_bert.py", line 291, in forward
    value_layer = self.transpose_for_scores(self.value(hidden_states))
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\torch\nn\modules\linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "C:\.virtualenv\sequence-processing-py39\lib\site-packages\torch\nn\functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 4.00 GiB already allocated; 7.02 MiB free; 4.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 4.00 GiB already allocated; 7.02 MiB free; 4.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Epoch [1/1]:   2%|███▋                                                                                                                                                                                     | 3/150 [00:06<05:37,  2.30s/it]