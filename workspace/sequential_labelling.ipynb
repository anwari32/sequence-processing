{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.data_generator import _data_generator_seq2seq\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "from sequential_labelling import init_seq2seq_model\n",
    "\n",
    "dataloader = _data_generator_seq2seq()\n",
    "model = init_seq2seq_model(pretrained_3kmer_dir)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# print(model)\n",
    "for step, batch in enumerate(dataloader):\n",
    "    input_ids, attn_mask, token_type_ids, label = tuple(t for t in batch)\n",
    "    pred = model(input_ids, attn_mask, token_type_ids)\n",
    "    # print(pred.shape, label.shape)\n",
    "    #for p, l in zip(pred, label):\n",
    "    #    print(p.shape, l.shape)\n",
    "    #    loss = loss_fn(p, l)\n",
    "    loss = loss_fn(pred, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [\n",
    "    [   # Sentence 0\n",
    "        [0.1, 0.2, 0.3, ], # Token 0\n",
    "        [0.1, 0.2, 0.3, ], # Token 1\n",
    "        [0.1, 0.2, 0.3, ], # Token 2\n",
    "        [0.1, 0.2, 0.3, ], # Token 3\n",
    "        [0.1, 0.2, 0.3, ], # Token 4\n",
    "    ],\n",
    "    [   # Sentence 1\n",
    "        [0.3, 0.4, 0.1, ], # Token 0\n",
    "        [0.2, 0.3, 0.4, ], # Token 1\n",
    "        [0.2, 0.3, 0.4, ], # Token 2\n",
    "        [0.1, 0.2, 0.3, ], # Token 3\n",
    "        [0.4, 0.1, 0.2, ], # Token 4\n",
    "    ],\n",
    "]\n",
    "import torch\n",
    "pred = torch.tensor(pred)\n",
    "pred = torch.nn.Softmax(dim=2)(pred)\n",
    "print(pred)\n",
    "print(pred.shape)\n",
    "\n",
    "label = [\n",
    "    [1, 1, 1, 1, 1], # Label Sentence 0\n",
    "    [0, 0, 0, 0, 0], # Label Sentence 1\n",
    "]\n",
    "label = torch.tensor(label)\n",
    "print(label.shape)\n",
    "\n",
    "fn = torch.nn.CrossEntropyLoss()\n",
    "loss = fn(pred, label)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "pred = torch.tensor([[0.8]])\n",
    "label = torch.tensor([[1.0]])\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "loss_fn(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, BertModel\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "\n",
    "bertForTokenClassification = BertForTokenClassification.from_pretrained(pretrained_3kmer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_dir import chr24_index_csv, chr24_fasta, labseq_dir, labseq_names\n",
    "chr_indices = [chr24_index_csv]\n",
    "chr_fastas = [chr24_fasta]\n",
    "chr_labseq_path = [\"{}/{}\".format(labseq_dir, fname) for fname in [labseq_names[-1]]]\n",
    "print(chr_indices)\n",
    "print(chr_fastas)\n",
    "print(chr_labseq_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_dir import chr24_index_csv, chr24_fasta, labseq_dir, labseq_names\n",
    "from data_preparation import generate_sequence_labelling\n",
    "chr_indices = [chr24_index_csv]\n",
    "chr_fastas = [chr24_fasta]\n",
    "chr_labseq_path = [\"{}/{}\".format(labseq_dir, fname) for fname in [labseq_names[-1]]]\n",
    "for src, fasta, target in zip(chr_indices, chr_fastas, chr_labseq_path):\n",
    "    print(\"Generating sequential labelling for index {}, from fasta {}, to {}: {}\".format(src, fasta, target, generate_sequence_labelling(src, fasta, target, do_expand=True, expand_size=512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_generator import _data_generator_seq2seq\n",
    "from models.seq2seq import DNABERTSeq2Seq\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "dataloader = _data_generator_seq2seq()\n",
    "model = DNABERTSeq2Seq(pretrained_3kmer_dir)\n",
    "\n",
    "for step, batch in enumerate(dataloader):\n",
    "    input_ids, attn_mask, token_type_ids, label = tuple(t for t in batch)\n",
    "    pred = model(input_ids, attn_mask, token_type_ids)\n",
    "    # print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [0.0944, 0.0905, 0.0905, 0.0905, 0.0905, 0.0905, 0.0905, 0.0905, 0.0905,\n",
    "        0.0909, 0.0905]\n",
    "sum(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequential_labelling import DNABERTSeq2Seq, train, init_adamw_optimizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "import os\n",
    "from utils.seq2seq import init_seq2seq_model\n",
    "import json\n",
    "from utils.data_generator import _data_generator_seq2seq\n",
    "\n",
    "dataloader = _data_generator_seq2seq()\n",
    "\n",
    "num_epoch = 10\n",
    "batch_size = 2\n",
    "warmup = 10\n",
    "device = \"cuda\"\n",
    "seq2seq_config = json.load(open(os.path.join(\"models\", \"config\", \"config_seq2seq.json\"), \"r\"))\n",
    "model = init_seq2seq_model(seq2seq_config)\n",
    "model.to(device)\n",
    "optimizer = init_adamw_optimizer(model.parameters())\n",
    "training_steps = len(dataloader) * num_epoch\n",
    "optim_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup, num_training_steps=training_steps)\n",
    "log_path = os.path.join(\"logs\", \"seq2seq\", \"19082022\", \"log.t-sample.csv\")\n",
    "save_path = os.path.join(\"result\", \"seq2seq\", \"19082022\", \"t-sample\")\n",
    "model.train()\n",
    "\"\"\"\n",
    "Play with result.\n",
    "\"\"\"\n",
    "trained_model = train(model, optimizer, optim_scheduler, dataloader, num_epoch, batch_size, log_path, save_path, device, remove_old_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "activation_function = torch.nn.Softmax(dim=2)\n",
    "pred = torch.tensor([[[0.5, 0.2, 0.3], [0.1, 0.2, 0.7]], [[0.5, 0.2, 0.3], [0.1, 0.2, 0.7]]], requires_grad=True)\n",
    "pred_activated = activation_function(pred)\n",
    "print(pred, pred_activated)\n",
    "labels = torch.tensor([[1, 2], [0, 1]])\n",
    "print(pred.shape, labels.shape)\n",
    "for p, l in zip(pred, labels):\n",
    "    loss = loss_function(p, l)\n",
    "    print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.data_generator import _data_generator_mtl\n",
    "from multitask_learning import init_model_mtl\n",
    "from data_dir import pretrained_3kmer_dir\n",
    "model = init_model_mtl(pretrained_path=pretrained_3kmer_dir)\n",
    "dataloader = _data_generator_mtl(batch_size=2)\n",
    "loss_function = torch.nn.Softmax(dim=1)\n",
    "for step, batch in enumerate(dataloader):\n",
    "    b_input_ids, b_attn_mask, b_label_prom, b_label_ss, b_label_polya = tuple(t for t in batch)\n",
    "    output = model(b_input_ids, b_attn_mask)\n",
    "    #print(output[\"prom\"], b_label_prom)\n",
    "    print(output[\"ss\"].shape, b_label_ss.shape)\n",
    "    print(output[\"ss\"], b_label_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.seq2seq import init_seq2seq_model\n",
    "import json\n",
    "import os\n",
    "model = init_seq2seq_model(json.load(open(os.path.join(\"models\", \"config\", \"config_seq2seq.json\"), 'r')))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.seq2seq import init_seq2seq_model\n",
    "import json\n",
    "import os\n",
    "model = init_seq2seq_model(json.load(open(os.path.join(\"models\", \"config\", \"config_seq2seq_norm.json\"), 'r')))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.seqlab import init_seqlab_model\n",
    "import json\n",
    "import os\n",
    "model = init_seqlab_model(json.load(open(os.path.join(\"models\", \"config\", \"config_seq2seq_multiple.json\"), 'r')))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.seqlab import init_seqlab_model\n",
    "import json\n",
    "import os\n",
    "model = init_seqlab_model(json.load(open(os.path.join(\"models\", \"config\", \"config_seq2seq_norm_multiple.json\"), 'r')))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.seqlab import init_seqlab_model\n",
    "import json\n",
    "import os\n",
    "model = init_seqlab_model(json.load(open(os.path.join(\"models\", \"config\", \"config_seq2seq_bilstm.json\"), 'r')))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What about merging genes from each chromosome?\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "_cols = [\"chr\",\"id\",\"sequence\",\"label\"]\n",
    "whole_df = pd.DataFrame(columns=_cols)\n",
    "for i in range(24):\n",
    "    chr = f\"chr{i + 1}\"\n",
    "    path = os.path.join(\"data\", \"genome\", \"seqlab.positive.strand\", chr)\n",
    "    files = os.listdir(path)\n",
    "    for f in tqdm(files, total=len(files), desc=f\"Chr{i + 1}\"):\n",
    "        fpath = os.path.join(path, f)\n",
    "        fdf = pd.read_csv(fpath)\n",
    "        # for index, row in tqdm(fdf.iterrows(), total=fdf.shape[0], desc=f\"{chr}, {f}\"):\n",
    "        for index, row in fdf.iterrows():\n",
    "            id = f\"{f.split('.')[0]}\"\n",
    "            sequence = row[\"sequence\"]\n",
    "            label = row[\"label\"]\n",
    "            frame = pd.DataFrame([[chr, id, sequence, label]], columns=_cols)\n",
    "            whole_df = pd.concat([whole_df, frame])\n",
    "        #endfor\n",
    "    #endfor\n",
    "#endfor\n",
    "whole_df.to_csv(os.path.join(\"data\", \"genome\", \"seqlab.positive.strand\", \"all_genes.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Randomized all genes.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "all_genes_df = pd.read_csv(os.path.join(\"data\", \"genome\", \"seqlab.positive.strand\", \"all_genes.csv\"))\n",
    "all_genes_df = all_genes_df.sample(frac=1).reset_index(drop=True)\n",
    "all_genes_df.to_csv(os.path.join(\"data\", \"genome\", \"seqlab.positive.strand\", \"all_genes_randomized.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split all genes into three parts.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "all_genes_df = pd.read_csv(os.path.join(\"data\", \"genome\", \"seqlab.positive.strand\", \"all_genes_randomized.csv\"))\n",
    "all_genes_train_df = all_genes_df.sample(frac=0.8)\n",
    "all_genes_val_test_df = all_genes_df.drop(all_genes_train_df.index)\n",
    "all_genes_val_df = all_genes_val_test_df.sample(frac=0.5)\n",
    "all_genes_test_df = all_genes_val_test_df.drop(all_genes_val_df.index)\n",
    "\n",
    "all_genes_train_df.to_csv(os.path.join(\"data\", \"genome\", \"seqlab.positive.strand\", \"all_genes_train.csv\"), index=False)\n",
    "all_genes_val_df.to_csv(os.path.join(\"data\", \"genome\", \"seqlab.positive.strand\", \"all_genes_validation.csv\"), index=False)\n",
    "all_genes_test_df.to_csv(os.path.join(\"data\", \"genome\", \"seqlab.positive.strand\", \"all_genes_test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create small sample for local training.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "all_genes_df = pd.read_csv(os.path.join(\"data\", \"genome\", \"seqlab.positive.strand\", \"all_genes_randomized.csv\"))\n",
    "all_genes_sample_df = all_genes_df.sample(n=100)\n",
    "all_genes_sample_df.to_csv(os.path.join(\"data\", \"genome\", \"seqlab.positive.strand\", \"all_genes.sample.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's profile the data.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "all_genes_df = pd.read_csv(os.path.join(\"data\", \"genome\", \"seqlab.positive.strand\", \"all_genes.csv\"))\n",
    "all_genes_df[\"length\"] = all_genes_df[\"sequence\"].str.len()\n",
    "chrs = list(all_genes_df[\"chr\"].unique())\n",
    "chr_axis = [(i+1) for i in range(24)]\n",
    "min_axis = []\n",
    "max_axis = []\n",
    "for c in chrs:\n",
    "    df = all_genes_df[all_genes_df[\"chr\"] == c]\n",
    "    max_length = df[\"length\"].max()\n",
    "    max_axis.append(max_length)\n",
    "    min_length = df[\"length\"].min()\n",
    "    min_axis.append(min_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "labels = chrs\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, min_axis, width, label='min')\n",
    "rects2 = ax.bar(x + width/2, max_axis, width, label='max')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Sequence Length')\n",
    "ax.set_title('Sequence Length for Each Chromosome')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.set_size_inches(15, 5)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('sequence-processing-py39': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1bcc05615a70b396b2914747c544672e77157727153b1cb6572b3ac9e1c1c348"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
